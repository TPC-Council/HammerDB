<?xml version="1.0" encoding="UTF-8"?>
<book version="5.1" xmlns="http://docbook.org/ns/docbook"
      xmlns:xlink="http://www.w3.org/1999/xlink"
      xmlns:xila="http://www.w3.org/2001/XInclude/local-attributes"
      xmlns:xi="http://www.w3.org/2001/XInclude"
      xmlns:trans="http://docbook.org/ns/transclusion"
      xmlns:svg="http://www.w3.org/2000/svg"
      xmlns:m="http://www.w3.org/1998/Math/MathML"
      xmlns:html="http://www.w3.org/1999/xhtml"
      xmlns:db="http://docbook.org/ns/docbook">
  <info>
    <title><inlinemediaobject>
        <imageobject>
          <imagedata fileref="docs/images/hammerDB-H-Logo-FB.png"/>
        </imageobject>
      </inlinemediaobject> HammerDB Documentation</title>
  </info>

  <chapter>
    <title>Installation Guide</title>

    <section>
      <title>HammerDB v3.2 New Features</title>

      <para><emphasis role="bold">Bug fixes</emphasis></para>

      <para>[#132] Running buildschema in CLI for TPC-H has incorrect VU
      count</para>

      <para>[#133] Use all warehouses omits warehouse 1 and fails if more
      VUsers than warehouses</para>

      <para>[#134] Buildschema in CLI errors if 1 virtual user and more than 1
      warehouse</para>

      <para>[GH#58] Bug when building TPC-C and TPC-H on Azure</para>

      <para>Updated time profiler to report percentile values at 10 second
      intervals</para>

      <para>Updated PostgreSQL Oracle SLEV Stored Procedure to return stock
      count</para>

      <para>Updated hammerdbcli to enable autostart with script</para>

      <para>Added PostgreSQL v11+ compatible Stored Procedures to use instead
      of Functions</para>

      <para>Added HTTP Web Service interface</para>
    </section>

    <section>
      <title>Test Matrix</title>

      <para>HammerDB has been built and tested on the following x86 64-bit
      Linux and Windows releases.</para>

      <para><table>
          <title>OS Test Matrix</title>

          <tgroup cols="2">
            <thead>
              <row>
                <entry align="center">Operating System</entry>

                <entry align="center">Release</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>Linux</entry>

                <entry>Ubuntu 17.04 - 17.10 - 18.04 / RHEL 7.3 - RHEL 7.4 -
                RHEL 7.5 - RHEL 7.6</entry>
              </row>

              <row>
                <entry>Windows</entry>

                <entry>Windows 10</entry>
              </row>
            </tbody>
          </tgroup>
        </table></para>

      <para>On Linux HammerDB requires the Xft FreeType-based font drawing
      library for X installed as follows:</para>

      <para>Ubuntu:</para>

      <para><programlisting>$ sudo apt-get install libxft-dev</programlisting>Red
      Hat:</para>

      <programlisting>$ yum install libXft</programlisting>

      <para>HammerDB has been built and testing on the following x86 64-bit
      Databases.</para>

      <table>
        <title>Database Test Matrix</title>

        <tgroup cols="2">
          <thead>
            <row>
              <entry align="center">Database (Compatible)</entry>

              <entry align="center">Release</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>Oracle (TimesTen)</entry>

              <entry>12c / 18c</entry>
            </row>

            <row>
              <entry>SQL Server</entry>

              <entry>2017</entry>
            </row>

            <row>
              <entry>Db2</entry>

              <entry>11.1</entry>
            </row>

            <row>
              <entry>MySQL (MariaDB) (Amazon Aurora)</entry>

              <entry>5.7 / 8.0 / 10.2 / 10.3 / 10.4</entry>
            </row>

            <row>
              <entry>PostgreSQL (EnterpriseDB) (Amazon Redshift)
              (Greenplum)</entry>

              <entry>10.2 / 10.3</entry>
            </row>

            <row>
              <entry>Redis</entry>

              <entry>4.0.6</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>

    <section>
      <title>Installing and Starting HammerDB on Windows</title>

      <para>To install HammerDB on Windows you have the option of using the
      self-extracting installer or zipfile. The self-extracting installer will
      create an uninstall executable for you. A zipfile installation can be
      deleted manually. In both cases the install is entirely self-contained
      within the installation directory.</para>

      <section>
        <title>Self Extracting Installer</title>

        <para>Double click on the Setup file and the language choice is
        shown.</para>

        <para><figure>
            <title>Language Choice</title>

            <mediaobject>
              <imageobject>
                <imagedata fileref="docs/images/ch1-2.png"/>
              </imageobject>
            </mediaobject>
          </figure>Click continue to begin the installation.</para>

        <para><figure>
            <title>Install Continue</title>

            <mediaobject>
              <imageobject>
                <imagedata fileref="docs/images/ch1-3.PNG"/>
              </imageobject>
            </mediaobject>
          </figure></para>

        <para>Click next to acknowledge the version</para>

        <figure>
          <title>HammerDB Version</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch1-4.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>Accept or modify the destination location.</para>

        <figure>
          <title>Destination</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch1-5.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>Review the settings before starting copying files.</para>

        <figure>
          <title>Review</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch1-6.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>The files will be copied and the uninstall built</para>

        <figure>
          <title>Files copying</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch1-7.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>Click Finish and launch HammerDB</para>

        <figure>
          <title>Complete</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch1-8.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>HammerDB will start ready for you to use</para>

        <figure>
          <title>HammerDB Started</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch1-9.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>

      <section>
        <title>Zip File</title>

        <para>As an alternative to the self-extracting installer you can
        download and extract the zipfile into a directory of your
        choice.</para>

        <figure>
          <title>Zip File</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch1-10.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para/>
      </section>

      <section>
        <title>Starting HammerDB</title>

        <para>After installation double-click on the "Windows Batch File"
        hammerdb to start hammerdb.</para>

        <figure>
          <title>hammerdb batch file</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch1-11.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>

      <section>
        <title>Uninstalling HammerDB</title>

        <para>For a zipfile installation, delete the hammerDB directory. For
        an installer based installation doubl-click on uninstall and follow
        the on-screen prompts.</para>

        <figure>
          <title>Uninstall</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch1-12.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>
    </section>

    <section>
      <title>Installing and Starting HammerDB on Linux</title>

      <para>To install HammerDB on Linux you have the option of using the
      self-extracting installer or a tar.gz file. The self-extracting
      installer will create an uninstall executable for you. A tar.gz
      installation can be deleted manually. In both cases the install is
      entirely self-contained within the installation directory.</para>

      <section>
        <title>Self Extracting Installer</title>

        <para>To install from the self-extracting installer refer to the
        previous section on the self-extracting installer for Windows, the
        installation method is the same.</para>
      </section>

      <section>
        <title>Tar.gz File</title>

        <para>To install from the tar.gz run the command</para>

        <programlisting>tar -zxvf HammerDB-3.0.tar.gz </programlisting>

        <para>This will extract HammerDB into a directory named
        HammerDB-3.0.</para>
      </section>

      <section>
        <title>Starting HammerDB</title>

        <para>To start HammerDB change to the HammerDB directory and run
        locally as follows.</para>

        <programlisting>./hammerdb</programlisting>
      </section>

      <section>
        <title>Uninstalling HammerDB</title>

        <para>To uninstall HammerDB on Linux run the uninstall executable for
        the self-extracting installer or remove the directory for the tar.gz
        install.</para>
      </section>
    </section>

    <section>
      <title>Verifying Client Libraries</title>

      <para>For all of the databases that HammerDB supports it is necessary to
      have a 3rd party client library installed that HammerDB can use to
      connect and interact with the database. This client library will also be
      installed with database server software. The HammerDB command line tool
      can be used to check the status of library availability for all
      databases.</para>

      <para>To run this utility run the following command</para>

      <programlisting>./hammerdbcli</programlisting>

      <para>and type librarycheck.</para>

      <programlisting>HammerDB CLI v3.0
Copyright (C) 2003-2018 Steve Shaw
Type "help" for a list of commands
The xml is well-formed, applying configuration
hammerdb&gt;librarycheck
Checking database library for Oracle
Error: failed to load Oratcl - can't read "env(ORACLE_HOME)": no such variable
Ensure that Oracle client libraries are installed and the location in the LD_LIBRARY_PATH environment variable
Checking database library for MSSQLServer
Success ... loaded library tclodbc for MSSQLServer
Checking database library for Db2
Success ... loaded library db2tcl for Db2
Checking database library for MySQL
Success ... loaded library mysqltcl for MySQL
Checking database library for PostgreSQL
Success ... loaded library Pgtcl for PostgreSQL
Checking database library for Redis
Success ... loaded library redis for Redis

hammerdb
</programlisting>

      <para>in the example it can be seen that the environment is not set for
      Oracle however all of the other libraries were found and correctly
      loaded. The following table illustrates the first level library that
      HammerDB requires however there may be additional dependencies. Refer to
      the Test Matrix to determine which database versions HammerDB was built
      against. On Linux the command ldd and on Windows the Dependency Walker
      Utility to determine additional dependencies. On Linux the
      LD_LIBRARY_PATH environment variable can be set to the location of
      installed libraries and PATH on Windows.</para>

      <table>
        <title>3rd party libraries</title>

        <tgroup cols="2">
          <thead>
            <row>
              <entry align="center">Database / OS</entry>

              <entry align="center">Library</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>Oracle Linux</entry>

              <entry>libclntsh.so.</entry>
            </row>

            <row>
              <entry>Oracle Windows</entry>

              <entry>OCI.DLL</entry>
            </row>

            <row>
              <entry>SQL Server Linux</entry>

              <entry>libodbc.so.</entry>
            </row>

            <row>
              <entry>SQL Server Windows</entry>

              <entry>ODBC32.DLL</entry>
            </row>

            <row>
              <entry>Db2 Linux</entry>

              <entry>libdb2.so.</entry>
            </row>

            <row>
              <entry>Db2 Windows</entry>

              <entry>DB2CLI64.DLL</entry>
            </row>

            <row>
              <entry>MySQL Linux</entry>

              <entry>libmysqlclient.so</entry>
            </row>

            <row>
              <entry>MySQL Windows</entry>

              <entry>LIBMYSQL.DLL</entry>
            </row>

            <row>
              <entry>PostgreSQL Linux</entry>

              <entry>libpq.so</entry>
            </row>

            <row>
              <entry>PostgreSQL Windows</entry>

              <entry>LIBPQ.DLL</entry>
            </row>

            <row>
              <entry>Redis</entry>

              <entry>Built in library</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <section>
        <title>Oracle Client</title>

        <para>When using the Oracle instant client Oratcl uses the additional
        environment variable ORACLE_LIBRARY to identify the Oracle client
        library. On the Windows the Oracle client library is called oci.dll in
        a location such as: C:\oraclexe\app\oracle\product\11.2.0\server\bin
        On Linux the library is called libclntsh.so where this is typically a
        symbolic link to a product specific name such as libclntsh.so.12.1 for
        Oracle 12c. An example .bash_profile file is shown for a typical
        Oracle environment.</para>

        <programlisting>oracle@server1  oracle]$ cat ~/.bash_profile
# .bash_profile

if [ -t 0 ]; then
stty intr ^C
fi

# Get the aliases and functions
if [ -f ~/.bashrc ]; then
        . ~/.bashrc
fi
# User specific environment and startup programs
umask 022
export ORACLE_BASE=/u01/app/oracle
export ORACLE_HOME=$ORACLE_BASE/product/12.1.0/dbhome_1
export LD_LIBRARY_PATH=$ORACLE_HOME/lib
export ORACLE_LIBRARY=$ORACLE_HOME/lib/libclntsh.so
export ORACLE_SID=PROD1
export PATH=$ORACLE_HOME/bin:$PATH
</programlisting>
      </section>

      <section>
        <title>SQL Server</title>

        <para>On SQL Server on Windows the client libraries and necessary
        environment variables are set automatically during the SQL Server
        installation. Note that on 64-bit Windows the 64-bit ODBC client
        library is named ODBC32.DLL in the following location.
        C:\Windows\System32\odbc32.dll. On Linux follow the SQL Server on
        Linux installation guide to install 'mssql-tools' with the unixODBC
        developer package. The command database drivers will show the
        installed ODBC Driver.</para>

        <programlisting>hammerdb&gt;database drivers
{{ODBC Driver 17 for SQL Server} {{Description=Microsoft ODBC Driver 17 for SQL Server} 
Driver=/opt/microsoft/msodbcsql/lib64/libmsodbcsql-17.0.so.1.1 UsageCount=1}}</programlisting>
      </section>

      <section>
        <title>Db2</title>

        <para>For DB2 on Linux the client library libdb2.so.1 is required
        either in the lib64 directory for 32. Similarly on Windows the
        db2cli64.dll is required. These libraries are included with a standard
        DB2 installation or also with a standalone DB2 client install.</para>
      </section>

      <section>
        <title>MySQL</title>

        <para>HammerDB version 3.0 has been built and tested against a MySQL
        5.7 client installation. On Linux this means that HammerDB will
        require a MySQL client library called libmysqlclient.so.20. This
        client library needs to be referenced in the LD_LIBRARY_PATH in the
        same way described for Oracle previously in this section.</para>
      </section>

      <section>
        <title>PostgreSQL</title>

        <para>For PostgreSQL the client library is called libpq.dll on Windows
        and libpq.so on Linux however note that additional libraries are also
        required. For Windows this means setting your PATH environment
        variable such as the following: D:\PostgreSQL\pgsql\bin; On Linux it
        is required to set the LD_LIBRARY_PATH environment variable in the
        same way described for Oracle previously in this section to the
        location of the PostgreSQL lib directory. Alternatively for
        installations of EnterpriseDB the client directory also contains the
        necessary files for a HammerDB installation.</para>
      </section>

      <section>
        <title>Redis</title>

        <para>Redis The Redis client package is included with HammerDB for all
        installations and requires no further configuration.</para>
      </section>
    </section>
  </chapter>

  <chapter>
    <title>Quick Start</title>

    <para>Before proceeding you should have your database software installed
    and running and be familiar with basic functionality of connecting to your
    database. After starting HammerDB, firstly you will need to select which
    benchmark and database you wish to use by choosing Benchmark from under
    the Options menu or double on your chosen database in the tree-view. The
    initial settings are determined by the values in your xml configuration
    file. Select your chosen database and TPC-C and press OK. This Quick Start
    guide uses Microsoft SQL Server, however the process is the same for all
    other supported databases.</para>

    <section>
      <title>Building the Schema</title>

      <para><figure>
          <title>Benchmark Options</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch2-1.PNG"/>
            </imageobject>
          </mediaobject>
        </figure></para>

      <para>Click on the Benchmark tree view and under TPC-C and select TPC-C
      Schema Build Options to display the TPC-C Schema Options window. Within
      this window enter the connection details of your database software.
      These options will vary depending on the database chosen. Select a
      number of warehouses, 10 is good for a first test and set the Virtual
      Users to build schema to the number of CPU cores on your system. Click
      OK.</para>

      <figure>
        <title>Build Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch2-2.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>.</para>

      <para>Double-click on Build in the tree view and you will receive a
      prompt on the settings chosen to build the schema. Click Yes.</para>

      <para><figure>
          <title>Create Schema</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch2-3.PNG"/>
            </imageobject>
          </mediaobject>
        </figure></para>

      <para>Observe that HammerDB begins to build the schema with multiple
      users.</para>

      <figure>
        <title>Building Schema</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch2-4.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Wait until the schema build completes and then click on the red
      button, tagged with Destroy Virtual Users.</para>

      <figure>
        <title>Schema build complete</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch2-5.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Within the tree-view Select Driver Script Options. Leave the
      settings at Test Driver Script and click OK</para>
    </section>

    <section>
      <title>Run a Test Workload</title>

      <para><figure>
          <title>Driver Options</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch2-6.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>Observe that the Driver Script is Loaded. Clicking on Load
      will reload the script.</para>

      <figure>
        <title>Driver Script Loaded</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch2-7.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Further down the tree-view under Virtual User select Options and
      select "2" for the number of Virtual Users, click OK.</para>

      <figure>
        <title>Virtual User Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch2-8.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Click on Create and the Virtual Users will be created and waiting
      to run in an idle status. (Clicking run first will run both Create and
      Run Immediately).</para>

      <para><figure>
          <title>Virtual Users Created</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch2-9.PNG"/>
            </imageobject>
          </mediaobject>
        </figure></para>

      <para>Double-click on Run - you can now observe the Virtual Users
      running a workload against the database. When ready press the red stop
      button to stop the workload.</para>

      <para><figure>
          <title>Virtual Users Running</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch2-10.PNG"/>
            </imageobject>
          </mediaobject>
        </figure></para>
    </section>

    <section>
      <title>Run a Timed Workload</title>

      <para>the Test script is to check connectivity and diagnose performance
      and configuration errors. It is the Timed Workload that should be used
      to conduct performance tests. Under Driver Options select Timed Driver
      Script and click OK, the Timed Driver Script is now loaded.</para>

      <para/>

      <figure>
        <title>Driver Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch2-11.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Verify the Virtual User Options.</para>

      <figure>
        <title>Virtual Users</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch2-12.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Click create Virtual User and observe that for Timed workloads an
      additional Virtual User has been created. This Virtual User does not run
      the workload but provides the timing and monitoring
      functionality.</para>

      <figure>
        <title>Virtual User and Monitor Created</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch2-13.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Click Run, the workload will begin but this time without the
      Virtual User output being written to the screen. The Monitor Virtual
      User will provide information on Timing as the workload
      progresses.</para>

      <figure>
        <title>Timed Workload Running</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch2-14.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>On completion observe that the Monitor Virtual User reports a
      value for TPM and a value for NOPM. TPM is the transactions per minute
      and is a unique value to how each database processes transactions. NOPM
      stands for New Orders per Minute and is database independent
      Consequently TPM cannot be used to compare tests running the same
      database software however NOPM can.</para>

      <figure>
        <title>Test Result</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch2-15.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>

    <section>
      <title>Using the Transaction Counter</title>

      <para>During a workload press the Transaction Counter button. This will
      report the TPM value at a timed interval (default 10 seconds) to the
      screen.</para>

      <figure>
        <title>Transaction Counter</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch2-16.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>
  </chapter>

  <chapter>
    <title>Introduction to OLTP Testing (TPC-C)</title>

    <section>
      <title>What is a Transactional Workload</title>

      <para>A transactional or OLTP (online transaction processing) workload
      is a workload typically identified by a database receiving both requests
      for data and multiple changes to this data from a number of users over
      time where these modifications are called transactions. Each database
      transaction has a defined beginning point, manipulates and modifies the
      data within the database and either commits the changes or rollbacks the
      changes to the starting point. A database must adhere to the ACID
      (Atomicity, Consistency, Isolation, Durability) properties to ensure
      that the database remains consistent whilst processing transactions.
      Database systems that process transactional workloads are inherently
      complex in order to manage the user’s sessions access to same data at
      the same time, processing the transactions in isolation whilst keeping
      the database consistent and recoverable. People will typically interact
      with OLTP systems on a regular basis with examples such as an online
      grocery ordering and delivery system or an airline reservation system.
      Performance and scalability are essential properties of systems designed
      to process transactional workloads and the TPC-C benchmark is a
      benchmark designed by the TPC to measure the performance of the software
      and hardware of a relational database system to process these
      workloads.</para>
    </section>

    <section>
      <title>What is the TPC and TPC-C?</title>

      <para>Designing and implementing a database benchmark is a significant
      challenge. Many performance tests and tools experience difficulties in
      comparing system performance especially in the area of scalability, the
      ability of a test conducted on a certain system and schema size to be
      comparable with a test on a larger scale system. When system vendors
      wish to publish benchmark information about database performance they
      have long had to access to such sophisticated test specifications to do
      so and the TPC is the industry body most widely recognized for defining
      benchmarks in the database industry recognized by all of the leading
      database vendors. TPC-C is the benchmark published by the TPC for Online
      Transaction Processing and you can view the published TPC-C results at
      the TPC website.</para>

      <para>As defined by the TPC "TPC benchmarks are industry standards. The
      TPC, at no charge, distributes its benchmark specifications to the
      public." For this reason HammerDB includes an implementation of the
      specification of the TPC-C benchmark that can be run in supported
      database environments. Implementing the TPC-C specification has the
      significant advantage that you know that the test is reliable, scalable
      and tested to produce accurate, repeatable and consistent
      results.</para>
    </section>

    <section>
      <title>HammerDB Transactional TPC-C based workloads</title>

      <para>The HammerDB workload is based on and intended to be as close as
      possible to the example published in the TPC-C specification. As such
      the implementation is intentionally non-optimized or biased towards any
      particular database implementation or system hardware (being open source
      you are free to inspect all of the HammerDB source code). The crucial
      element is to reiterate the point made in the previous section that the
      HammerDB workloads are designed to be reliable, scalable and tested to
      produce accurate, repeatable and consistent results. In other words
      HammerDB is designed to measure relative as opposed to absolute database
      performance between systems. What this means is if you run a test
      against one particular configuration of hardware and software and re-run
      the same test against exactly the same configuration you will get
      exactly the same result within the bounds of the random selection of
      transactions which will typically be within 1%. Results Any differences
      between results are directly as a result of changes you have made to the
      configuration (or management overhead of your system such as database
      checkpoints or user/administrator error). Testing has proven that
      HammerDB tests re-run multiple times unattended (see the autopilot
      feature) on the same reliable configuration produce performance profiles
      that will overlay each other almost identically. The Figure below
      illustrates an example of this consistency and shows the actual results
      of 2 sequences of tests run unattended one after another against one of
      the supported databases with the autopilot feature from 1 to 144 virtual
      users to test modifications to a WAL (Write Ahead Log File). In other
      words HammerDB will give you the same results each time, if your results
      vary you need to focus entirely on your database, OS and hardware
      configuration.</para>

      <para/>

      <figure>
        <title>WAL Test</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch3-1.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Consequently when you begin to make changes you are able to
      quantify the impact of these changes. Examples include modifying
      database parameters, changing database software or operating system or
      upgrading or modifying hardware. Taking a baseline of a database system
      with a HammerDB workload is an ideal method to ensure that optimal
      database configurations are engineered put into production.</para>
    </section>

    <section>
      <title>Comparing HammerDB results</title>

      <para>HammerDB implements a workload based on the TPC-C specification
      however does NOT implement a full specification TPC-C benchmark and the
      transaction results from HammerDB cannot be compared with the official
      published TPC-C benchmarks in any manner. Official Audited TPC-C
      benchmarks are extremely costly, time consuming and complex to establish
      and maintain. The HammerDB implementation based on the specification of
      the TPC-C benchmark is designed to capture the essence of TPC-C in a
      form that can be run at low cost on any system bringing professional,
      reliable and predictable load testing to all database environments. For
      this reason HammerDB results cannot and should NOT be compared or used
      with the term tpmC in any circumstance. You are permitted however to
      observe for your own benefit whether a correlation exists between the
      ratios of HammerDB results conducted on different systems and
      officially, audited and published results. HammerDB workloads produce 2
      statistics to compare systems called TPM and NOPM respectively. TPM is
      the specific database transactional measurement typically defined as the
      number of user commits plus the number of user rollbacks. Being database
      specific TPM values cannot be compared between different database types.
      On the other hand the NOPM value is based on a metric captured from
      within the test schema itself. As such NOPM (New Orders per minute) is a
      performance metric independent of any particular database implementation
      and is the recommended primary metric to use.</para>
    </section>

    <section>
      <title>Understanding the TPC-C workload</title>

      <para>TPC-C implements a computer system to fulfil orders from customers
      to supply products from a company. The company sells 100,000 items and
      keeps its stock in warehouses. Each warehouse has 10 sales districts and
      each district serves 3000 customers. The customers call the company
      whose operators take the order, each order containing a number of items.
      Orders are usually satisfied from the local warehouse however a small
      number of items are not in stock at a particular point in time and are
      supplied by an alternative warehouse. It is important to note that the
      size of the company is not fixed and can add Warehouses and sales
      districts as the company grows. For this reason your test schema can be
      as small or large as you wish with a larger schema requiring a more
      powerful computer system to process the increased level of transactions.
      The TPC-C schema is shown below, in particular note how the number of
      rows in all of the tables apart from the ITEM table which is fixed is
      dependent upon the number of warehouses you choose to create your
      schema.</para>

      <para/>

      <para><figure>
          <title>TPC-C Schema</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch3-2.png"/>
            </imageobject>
          </mediaobject>
        </figure>For additional clarity please note that the term Warehouse in
      the context of TPC-C bears no relation to a Data Warehousing workload,
      as you have seen TPC-C defines a transactional based system and not a
      decision support (DSS) one. In addition to the computer system being
      used to place orders it also enables payment and delivery of orders and
      the ability to query the stock levels of warehouses. Consequently the
      workload is defined by a mix of 5 transactions selected at random
      according to the balance of the percentage value shown as
      follows:</para>

      <itemizedlist>
        <listitem>
          <para>New-order: receive a new order from a customer: 45%</para>
        </listitem>

        <listitem>
          <para>Payment: update the customers balance to record a payment:
          43%</para>
        </listitem>

        <listitem>
          <para>Delivery: deliver orders asynchronously: 4%</para>
        </listitem>

        <listitem>
          <para>Order-status: retrieve the status of customers most recent
          order: 4%</para>
        </listitem>

        <listitem>
          <para>Stock-level: return the status of the warehouses inventory:
          4%</para>
        </listitem>
      </itemizedlist>
    </section>

    <section>
      <title>Generating Performance Profiles</title>

      <para>For an official audited TPC-C benchmark the result of the tests is
      detailed as tpmC which represents the number of New Orders processed.
      This measurement is published as a single data point representing peak
      performance. A particular advantage of HammerDB is is the ability to
      generate a performance profile as the load increases on your system (see
      the Autopilot feature for doing this in an unattended manner). Therefore
      whereas an official TPC-C benchmark gives you a single data-point and a
      typical single-threaded test, consider the graph shown.</para>

      <figure>
        <title>Performance Profile</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch3-3.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>This graph shows the relative performance of real tests on
      different database configurations, this example in fact shows the same
      database on the same server with the same configuration, however with
      source code modifications to improve scalability. It is evident that the
      performance improvement only becomes visible once the system reaches
      beyond 16 Virtual Users. It is also clear that in both cases adding
      Virtual Users beyond peak performance results in lower throughput. It
      should therefore be clear that your testing goal for transactional
      systems should be to measure the performance profile of your system
      across all levels of utilisation starting with 1 virtual user through to
      peak utilisation.</para>
    </section>

    <section>
      <title>Generating Time Profiles</title>

      <para>In addition to performance profiles based on throughput you should
      also take note of transaction response times. Whereas performance
      profiles show the cumulative performance of all of the virtual users
      running on the system, response times show performance based on the
      experience of the individual user. When comparing systems both
      throughput and response time are important comparative measurements.
      HammerDB includes a time profiling package called etprof that enables
      you to select an individual user and measure the response times. This
      functionality is enabled by selecting Time Profile checkbox in the
      driver options. When enabled the time profile will report response time
      percentile values at 10 second intervals as well as cumulative values
      for all of the test at the end of the test run.</para>

      <programlisting>Hammerdb Log @ Fri Jul 05 09:55:26 BST 2019
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-
Vuser 1:Beginning rampup time of 1 minutes
Vuser 2:Processing 1000000 transactions with output suppressed...
Vuser 3:Processing 1000000 transactions with output suppressed...
Vuser 4:Processing 1000000 transactions with output suppressed...
Vuser 5:Processing 1000000 transactions with output suppressed...
Vuser 2:|PERCENTILES 2019-07-05 09:55:46 to 2019-07-05 09:55:56
Vuser 2:|neword|MIN-391|P50%-685|P95%-1286|P99%-3298|MAX-246555|SAMPLES-3603
Vuser 2:|payment|MIN-314|P50%-574|P95%-1211|P99%-2253|MAX-89367|SAMPLES-3564
Vuser 2:|delivery|MIN-1128|P50%-1784|P95%-2784|P99%-6960|MAX-267012|SAMPLES-356
Vuser 2:|slev|MIN-723|P50%-884|P95%-1363|P99%-3766|MAX-120687|SAMPLES-343
Vuser 2:|ostat|MIN-233|P50%-568|P95%-1325|P99%-2387|MAX-82538|SAMPLES-365
Vuser 2:|gettimestamp|MIN-2|P50%-4|P95%-7|P99%-14|MAX-39|SAMPLES-7521
Vuser 2:|prep_statement|MIN-188|P50%-209|P95%-1067|P99%-1067|MAX-1067|SAMPLES-6
Vuser 2:+-----------------+--------------+------+--------+--------------+--------------+
...
Vuser 2:+-----------------+--------------+------+--------+--------------+--------------+
Vuser 2:|PERCENTILES 2019-07-05 09:59:26 to 2019-07-05 09:59:36
Vuser 2:|neword|MIN-410|P50%-678|P95%-1314|P99%-4370|MAX-32030|SAMPLES-4084
Vuser 2:|payment|MIN-331|P50%-583|P95%-1271|P99%-3152|MAX-43996|SAMPLES-4142
Vuser 2:|delivery|MIN-1177|P50%-2132|P95%-3346|P99%-4040|MAX-8492|SAMPLES-416
Vuser 2:|slev|MIN-684|P50%-880|P95%-1375|P99%-1950|MAX-230733|SAMPLES-364
Vuser 2:|ostat|MIN-266|P50%-688.5|P95%-1292|P99%-1827|MAX-9790|SAMPLES-427
Vuser 2:|gettimestamp|MIN-3|P50%-4|P95%-7|P99%-14|MAX-22|SAMPLES-8639
Vuser 2:+-----------------+--------------+------+--------+--------------+--------------+
Vuser 2:|PERCENTILES 2019-07-05 09:59:36 to 2019-07-05 09:59:46
Vuser 2:|neword|MIN-404|P50%-702|P95%-1296|P99%-4318|MAX-71663|SAMPLES-3804
Vuser 2:|payment|MIN-331|P50%-597|P95%-1250|P99%-4190|MAX-47539|SAMPLES-3879
Vuser 2:|delivery|MIN-1306|P50%-2131|P95%-4013|P99%-8742|MAX-25095|SAMPLES-398
Vuser 2:|slev|MIN-713|P50%-913|P95%-1438|P99%-2043|MAX-7434|SAMPLES-386
Vuser 2:|ostat|MIN-268|P50%-703|P95%-1414|P99%-3381|MAX-249963|SAMPLES-416
Vuser 2:|gettimestamp|MIN-3|P50%-4|P95%-8|P99%-16|MAX-27|SAMPLES-8079
Vuser 2:+-----------------+--------------+------+--------+--------------+--------------+
Vuser 1:3 ...,
Vuser 1:Test complete, Taking end Transaction Count.
Vuser 1:4 Active Virtual Users configured
Vuser 1:TEST RESULT : System achieved 468610 SQL Server TPM at 101789 NOPM
Vuser 2:+-----------------+--------------+------+--------+--------------+--------------+
Vuser 2:|PROCNAME | EXCLUSIVETOT| %| CALLNUM| AVGPERCALL| CUMULTOT|
Vuser 2:+-----------------+--------------+------+--------+--------------+--------------+
Vuser 2:|neword | 82051665|39.96%| 93933| 873| 88760245|
Vuser 2:|payment | 73823956|35.95%| 93922| 786| 80531339|
Vuser 2:|delivery | 22725292|11.07%| 9577| 2372| 23418195|
Vuser 2:|slev | 14396765| 7.01%| 9340| 1541| 14402033|
Vuser 2:|ostat | 10202116| 4.97%| 9412| 1083| 10207260|
Vuser 2:|gettimestamp | 2149552| 1.05%| 197432| 10| 13436919|
Vuser 2:|TOPLEVEL | 2431| 0.00%| 1| 2431| NOT AVAILABLE|
Vuser 2:|prep_statement | 1935| 0.00%| 5| 387| 1936|
Vuser 2:+-----------------+--------------+------+--------+--------------+--------------+</programlisting>

      <para>The output from etprof taken from each system should be used in
      context with the overall performance profile to break down the overall
      system throughput to the timing of the individual transactions
      themselves and then graphed to show the comparison.</para>

      <figure>
        <title>Response Times</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch3-3.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>

    <section>
      <title>Publishing database performance results</title>

      <para>The goal of HammerDB is to make database performance data open
      source and enable database professionals a fast and low-cost to compare
      and contrast database systems. HamerDB maintains a list of 3rd party
      publications on the HammerDB website and shares performance results on
      Twitter where anyone is free to share their own test data.</para>

      <figure>
        <title>Publishing Results</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch3-5.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>
  </chapter>

  <chapter>
    <title>How to Run an OLTP Workload</title>

    <para>This Chapter provides a general overview on the HammerDB OLTP
    workload and gives you an introduction to conducting OLTP (Online
    Transaction Processing) workloads on all of the supported databases. This
    will equip you with the essentials for assessing the ability of any system
    for processing transactional workloads.</para>

    <section>
      <title>Test Network Configuration</title>

      <para>You require the database server to be tested known as the system
      under test (SUT) installed and configured with the target database
      server. You also require a load generation server to run HammerDB
      installed with the HammerDB software and a database client. Typically
      the load generation server is run on a separate system from the SUT with
      the load generated across the network. It is possible to run HammerDB on
      the same system as the SUT however this will be expected to produce
      different results from a network based load. Both the SUT and the load
      generation server may be virtualized or container databases although
      similarly results may differ from a native hardware based
      installation.</para>

      <section>
        <title>SUT Database Server Configuration</title>

        <para>The database server architecture to be tested must meet the
        minimum requirements for your chosen database software, however to
        reach maximum performance it is likely that the specifications will
        considerably exceed these standard. To run a HammerDB transactional
        load test there are minimum requirements in memory and I/O (disk
        performance) to prevent these components being a bottleneck on
        performance. For a configuration requiring the minimal level of memory
        and I/O to maximize CPU utilization keying and thinking time should be
        set to FALSE (keying and thinking time is detailed later in this
        guide). To achieve this you should aim to create a schema with
        approximately 250-500 warehouses per CPU socket. As long as it is not
        too small resulting in contention the schema size should not
        significantly impact results. You should have sufficient memory to
        cache as much of your test schema in memory as possible. If keying and
        thinking time is set to TRUE you will need a significantly larger
        schema and number of virtual users to create a meaningful system load.
        Reductions in memory will place more emphasis on the I/O performance
        of the database containing the schema. If the allocated memory is
        sufficient most of the data will be cached during an OLTP test and I/O
        to the data area will be minimal. As a consequence the key I/O
        dependency will be to the redo/WAL/transaction logs for both bandwidth
        and sequential write latency. Modern PCIe SSDs when correctly
        configured have been shown to provide the capabilities to sustain high
        performance transaction logging.</para>
      </section>

      <section>
        <title>Load Generation Server Configuration</title>

        <para>The most important component of the load generation server is
        the server processor. The overall load generation server capacity
        required depends on the system capabilities of the SUT. It is
        recommend to use an up to date multi-core processor. HammerDB is a
        multi-threaded application and implicitly benefits from a multi-core
        server CPU. To determine whether CPU capacity is sufficient for
        testing you can monitor the CPU utilisation with HammerDB Metrics. CPU
        utilisation reaching 100% is an indication that the CPU on the load
        generation server is limiting performance. Load generation memory
        requirements are dependent on the operating system configuration and
        the number of virtual users created with each virtual user requiring
        its own database client. Typically server sizing guidelines should be
        within the limits expected to support a real user count. Multiple load
        generation servers connected in a “master-slave” configuration are
        enabled within HammerDB to exceed the capacity of a single load
        generation client. The load generation server does not need to be
        running the same version of SQL Server as the SUT.</para>
      </section>

      <section>
        <title>Administrator PC Configuration</title>

        <para>When using the graphical version of HammerDB the administrator
        PC must have the minimal requirement to display the graphical output
        from the load generation server. The PC should also have the ability
        to connect to the SUT to monitor performance by the installation of an
        appropriate database client.</para>
      </section>
    </section>

    <section>
      <title>Installation and Configuration</title>

      <para>This section details database specific installation and
      configuration requirements.</para>

      <section>
        <title>Oracle</title>

        <para>You should have the Oracle database software installed and a
        test database created and running. During the installation make a note
        of your system user password, you will need it for the test schema
        creation. You may at your discretion use an existing database however
        please note that HammerDB load testing can drive your system
        utilization to maximum levels and therefore testing an active
        production system is not recommended. After your database server is
        installed you should create a tablespace into which the test data will
        be installed allowing disk space according to the guide previously in
        this chpater. For example the following shows creating the tablespace
        in the ASM disk group DATA:</para>

        <programlisting>SQL&gt; create bigfile tablespace tpcctab datafile '+DATA' size 100g; </programlisting>

        <para>f you are running HammerDB against Oracle on Windows there is
        long established bug in Oracle that can cause application crashes for
        multi-threaded applications on Windows. Note that again this bug is an
        Oracle bug and not a HammerDB bug and can be investigated on the My
        Oracle Support website with the following reference. Bug 12733000
        OCIStmtRelease crashes or hangs if called after freeing the service
        context handle. To resolve this Oracle issue add the following entry
        to your SQLNET.ORA file.</para>

        <programlisting>SQLNET.AUTHENTICATION_SERVICES = (NTS)
DIAG_ADR_ENABLED=OFF DIAG_SIGHANDLER_ENABLED=FALSE
DIAG_DDE_ENABLED=FALSE</programlisting>

        <para>You must be able to connect from your load generation server to
        your SUT database server across the network using Oracle TNS. This
        will involve successful configuration of your listener on the SUT
        database server and the tnsnames.ora file on the load generation
        server. You can troubleshoot connectivity issues using the ping,
        tnsping and sqlplus commands on the load generation client and the
        lsnrctl command on the SUT database server. For example a successful
        tnsping test looks as follows:</para>

        <programlisting>[oracle@MERLIN ~]$ tnsping PDB1

TNS Ping Utility for Linux: Version 12.1.0.1.0 - Production on 21-MAY-2014 05:40:49

Copyright (c) 1997, 2013, Oracle.  All rights reserved.

Used parameter files:
/u01/app/oracle/product/12.1.0/dbhome_1/network/admin/sqlnet.ora

Used TNSNAMES adapter to resolve the alias
Attempting to contact (DESCRIPTION = (ADDRESS = (PROTOCOL = TCP)(HOST = merlin)(PORT = 1521)) (CONNECT_DATA = (SERVER = DEDICATED) (SERVICE_NAME = pdb1)))
OK (30 msec)

</programlisting>

        <para>Note that where the instant client is being used on the load
        generation server you should configure the TNS_ADMIN environment
        variable to the location where the tnsnames.ora and sqlnet.ora files
        are installed. When you have installed the load generation server and
        SUT database and have verified that you can communicate between them
        using Oracle TNS you can proceed to building a test schema.</para>
      </section>

      <section>
        <title>Microsoft SQL Server</title>

        <para>You will have configured Microsoft SQL Server during
        installation to authenticate either with Windows Authentication or
        with SQL Server and Windows Authentication. HammerDB will permit
        either method to be used however you must have the corresponding
        configuration on your SQL Server. Additionally your chosen method of
        authentication is required to be compatible with your chosen ODBC
        driver. To discover the available drivers use the ODBC Data Source
        Administrator tool. The driver name should be entered into HammerDB
        exactly as shown in the Data Source Administrator. The default value
        is “ODBC Driver 13 for SQL Server” on Windows and “ODBC Driver 17 for
        SQL Server” on Linux.</para>

        <figure>
          <title>ODBC Drivers</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch4-1.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>

      <section>
        <title>Db2</title>

        <para>To connect to Db2 requires an ODBC interface and therefore it is
        also necessary to install the DB2 client software IBM Data Server
        Driver for ODBC and CLI. Note that HammerDB connects to Db2 via CLI as
        the db2tcl interface is C based interface enabling CLI connectivity.
        ODBC is not used for HammerDB connectivity to Db2. Configure your
        db2dsdriver.cfg file with the hostname, port and database that you
        have created on the server.</para>

        <programlisting>db2inst1:~/odbc_cli/clidriver/cfg&gt; more db2dsdriver.cfg
&lt;?xml version="1.0" encoding="UTF-8" standalone="no" ?&gt;
&lt;configuration&gt;
  &lt;dsncollection&gt;
    &lt;dsn alias="TPCC" host="db2v1032bit" name="TPCC" port="50001"/&gt;
  &lt;/dsncollection&gt;

  &lt;databases&gt;
    &lt;database host="db2v1032bit" name="TPCC" port="50001"/&gt;
  &lt;/databases&gt;
&lt;/configuration&gt;
</programlisting>

        <para>Options can be set in the db2cli.ini file.</para>

        <programlisting>[db2inst1@~/sqllib/cfg]$ more db2cli.ini 
[TPCC]
UID=db2inst1
PWD=oracle
SysSchema=SYSIBM
SchemaList=”’SYSIBM’,’TPCC’”
DeferredPrepare=1
ConnectTimeout=10
ReceiveTimeout=120
LockTimeout=-1
AppendForFetchOnly=0
AutoCommit=1
ConnectType=1
CursorHold=OFF
TxnIsolation=1
StmtConcentrator=OFF
</programlisting>

        <para>You should have the DB2 database software installed and ready to
        accept connections as shown below.</para>

        <programlisting>db2inst1~]$ db2stop
04/12/2015 10:12:27     0   0   SQL1064N  DB2STOP processing was successful.
SQL1064N  DB2STOP processing was successful.
[db2inst1~]$ db2start
12/04/2015 10:12:31     0   0   SQL1063N  DB2START processing was successful.
SQL1063N  DB2START processing was successful.
[db2inst1~]$ db2
(c) Copyright IBM Corporation 1993,2007
Command Line Processor for DB2 Client 10.5.5

You can issue database manager commands and SQL statements from the command 
prompt. For example:
    db2 =&gt; connect to sample
    db2 =&gt; bind sample.bnd

For general help, type: ?.
For command help, type: ? command, where command can be
the first few keywords of a database manager command. For example:
 ? CATALOG DATABASE for help on the CATALOG DATABASE command
 ? CATALOG          for help on all of the CATALOG commands.

To exit db2 interactive mode, type QUIT at the command prompt. Outside 
interactive mode, all commands must be prefixed with 'db2'.
To list the current command option settings, type LIST COMMAND OPTIONS.

For more detailed help, refer to the Online Reference Manual.

db2 =&gt;
</programlisting>

        <para>With DB2 installed and running manually create and configure a
        DB2 Database according to your requirements. Pay particular attention
        to setting a LOGFILSIZ appropriate to your environment, otherwise you
        are likely to receive a transaction log full error message during the
        schema build. Additionally HammerDB is bufferpool and tablespace aware
        and therefore you may wish to create additional bufferpools specific
        to the tables that you are going create. The example below shows a
        configuration where a separate bufferpool has been created for each
        table solely to illustrate the usage of HammerDB parameters. You
        should also use the db2set command to set parameters appropriate to
        your system, for example setting DB2_LARGE_PAGE_MEM=DB for a large
        page configuration. Note that the commands below are examples only and
        should not (and are not) recommendations for optimal
        performance.</para>

        <programlisting>[db2inst1@ ~]$ db2 create database tpcc pagesize 8 k
DB20000I  The CREATE DATABASE command completed successfully.
[db2inst1@ ~]$ db2 update db cfg for tpcc using PCKCACHESZ 1631072
DB20000I  The UPDATE DATABASE CONFIGURATION command completed successfully.
[db2inst1@ ~]$ db2 update db cfg for tpcc using LOGFILSIZ 1048572
DB20000I  The UPDATE DATABASE CONFIGURATION command completed successfully.
[db2inst1@ ~]$ db2 update db cfg for tpcc using LOGPRIMARY 25 
DB20000I  The UPDATE DATABASE CONFIGURATION command completed successfully.
[db2inst1@ ~]$ db2 update db cfg for tpcc using LOGSECOND 5
DB20000I  The UPDATE DATABASE CONFIGURATION command completed successfully.
[db2inst1@ ~]$ db2 update db cfg for tpcc using LOGBUFSZ 17264
DB20000I  The UPDATE DATABASE CONFIGURATION command completed successfully.
[db2inst1@ ~]$ db2 update db cfg for tpcc using MINCOMMIT 1
DB20000I  The UPDATE DATABASE CONFIGURATION command completed successfully.
[db2inst1@ ~]$ db2 update db cfg for tpcc using NUM_IOSERVERS AUTOMATIC
DB20000I  The UPDATE DATABASE CONFIGURATION command completed successfully.
[db2inst1@ ~]$ db2 update db cfg for tpcc using DFT_PREFETCH_SZ AUTOMATIC
DB20000I  The UPDATE DATABASE CONFIGURATION command completed successfully.
[db2inst1@ ~]$ db2 update db cfg for tpcc using LOCKTIMEOUT 15
DB20000I  The UPDATE DATABASE CONFIGURATION command completed successfully.
[db2inst1@ ~]$ db2 update db cfg for tpcc using SOFTMAX 2500
DB20000I  The UPDATE DATABASE CONFIGURATION command completed successfully.
[db2inst1@ ~]$ 
[db2inst1@ ~]$ db2 connect to tpcc

   Database Connection Information

 Database server        = DB2/LINUXX8664 10.5.5
 SQL authorization ID   = DB2INST1
 Local database alias   = TPCC

[db2inst1@ ~]$ db2 create bufferpool C_BP immediate size 2500000 pagesize 8k
DB20000I  The SQL command completed successfully.
[db2inst1@ ~]$ db2 create large tablespace C_TS pagesize 8k managed by automatic storage bufferpool C_BP
DB20000I  The SQL command completed successfully.
[db2inst1@ ~]$ db2 create bufferpool D_BP immediate size 500000 pagesize 4k
DB20000I  The SQL command completed successfully.
[db2inst1@ ~]$ db2 create large tablespace D_TS pagesize 4k managed by automatic storage bufferpool D_BP
DB20000I  The SQL command completed successfully.
[db2inst1@ ~]$ db2 create bufferpool W_BP immediate size 500000 pagesize 4k
DB20000I  The SQL command completed successfully.
[db2inst1@ ~]$ db2 create large tablespace W_TS pagesize 4k managed by automatic storage bufferpool W_BP
DB20000I  The SQL command completed successfully.
[db2inst1@ ~]$ db2 create bufferpool I_BP immediate size 500000 pagesize 4k
DB20000I  The SQL command completed successfully.
[db2inst1@ ~]$ db2 create large tablespace I_TS pagesize 4k managed by automatic storage bufferpool I_BP
DB20000I  The SQL command completed successfully.
[db2inst1@ ~]$ db2 create bufferpool H_BP immediate size 2000000 pagesize 8k
DB20000I  The SQL command completed successfully.
[db2inst1@ ~]$ db2 create large tablespace H_TS pagesize 8k managed by automatic storage bufferpool H_BP
DB20000I  The SQL command completed successfully.
[db2inst1@ ~]$ db2 create bufferpool S_BP immediate size 2000000 pagesize 8k
DB20000I  The SQL command completed successfully.
[db2inst1@ ~]$ db2 create large tablespace S_TS pagesize 8k managed by automatic storage bufferpool S_BP
DB20000I  The SQL command completed successfully.
[db2inst1@ ~]$ db2 create bufferpool NO_BP immediate size 3000000 pagesize 8k
DB20000I  The SQL command completed successfully.
[db2inst1@ ~]$ db2 create large tablespace NO_TS pagesize 8k managed by automatic storage bufferpool NO_BP
DB20000I  The SQL command completed successfully.
[db2inst1@ ~]$ db2 create bufferpool OR_BP immediate size 3000000 pagesize 8k
DB20000I  The SQL command completed successfully.
[db2inst1@ ~]$ db2 create large tablespace OR_TS pagesize 8k managed by automatic storage bufferpool OR_BP
DB20000I  The SQL command completed successfully.
[db2inst1@ ~]$ db2 create bufferpool OL_BP immediate size 5000000 pagesize 8k
DB20000I  The SQL command completed successfully.
[db2inst1@ ~]$ db2 create large tablespace OL_TS pagesize 8k managed by automatic storage bufferpool OL_BP
DB20000I  The SQL command completed successfully.
</programlisting>
      </section>

      <section>
        <title>MySQL</title>

        <para>You should have the MySQL database software installed and
        running. Make sure you set a password for either the root user or a
        user with the correct privileges to create the TPC-C database, for
        example:</para>

        <para><programlisting>-bash-4.1$ ./mysqladmin -u root password mysql</programlisting>By
        default a MySQL installation will allow connection to the local server
        only, you must grant permission to connect to the MySQL database from
        your load generation server, the following example grants all
        permissions to the root user on the system called merlin.home.</para>

        <programlisting>mysql&gt; grant all on *.* to root@'hummingbird.home' identified by 'mysql';
Query OK, 0 rows affected (0.00 sec)
mysql&gt; flush privileges;
Query OK, 0 rows affected (0.00 sec)
</programlisting>

        <para>Alternatively after the test database is created you can
        restrict the privileges to that databases only.</para>

        <programlisting>mysql&gt; grant all on tpcc.* to root@'hummingbird.home' identified by 'mysql';</programlisting>

        <para>When choosing a MySQL Server to test note that HammerDB load
        testing can drive your system utilization to maximum levels and
        therefore testing an active production system is not recommended. When
        you have installed the load generation server and SUT database and
        have verified that you can communicate between them by logging in
        remotely you can proceed to building a test schema.</para>

        <programlisting>mysql@hummingbird:~&gt; mysql -u root -p -h merlin.home
Enter password: 
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 19
Server version: 5.6.17 MySQL Community Server (GPL)
Copyright (c) 2000, 2014, Oracle and/or its affiliates. All rights reserved.
Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.
Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
mysql&gt; 
</programlisting>
      </section>

      <section>
        <title>PostgreSQL</title>

        <para>You should have the PostgreSQL database software installed and a
        test database created and running. It is important to note that
        EnterpriseDB produce an enhanced version of the PostgreSQL database
        termed Postgres Plus Advanced Server. This version of PostgreSQL in
        particular includes compatibility features with the Oracle database
        such as PL/SQL support. For this reason the Hammer OLTP workload for
        PostgreSQL can operate in 2 modes. Firstly Oracle compatible mode uses
        PL/SQL and additional Postgres Plus Advanced Server features (such as
        DRITA snapshots) that will only operate against Enterprise DB Postgres
        Plus Advanced Server. Secondly by not selecting Oracle compatibility
        HammerDB can continue to operate against Postgres Plus Advanced Server
        but additionally against a regular PostgreSQL build using native
        PostgreSQL features. You must ensure before proceeding with OLTP that
        you are aware of the version of PostgreSQL you have installed and the
        features available, if you wish to test Oracle compatibility then you
        must use Postgres Plus Advanced Server from EnterpriseDB and install
        in Oracle compatible mode.</para>

        <para>During the installation make a note of your postgres superuser
        password, you will need it for the test schema creation. You must be
        able to connect from your load generation server to your SUT database
        server across the network. Firstly check your postgresql.conf file for
        the listen_addresses parameter. If this is set to localhost then only
        connections from the local server will be permitted. Use
        listen_addresses = ‘*’ to permit connections from all servers.
        Successful network connections will also involve successful
        configuration of your pg_hba.conf on the SUT database server. For
        example the following extract from a pg_hba.conf file from a
        PostgreSQL 9.3 installation shows trusted local connections on the SUT
        permitting connection without a password and remote connections from
        the Load Generation server with IP address 192.168.1.67 if the correct
        password is supplied. Note that the syntax of pg_hba.conf has changed
        for different versions of PostgreSQL and you should therefore consult
        the PostgreSQL documentation and sections further in this document to
        troubleshoot connectivity issues.</para>

        <programlisting># TYPE  DATABASE        USER            ADDRESS                 METHOD
# "local" is for Unix domain socket connections only
local      all       all  trust 
# IPv4 local connections:
host  all  all  127.0.0.1/32 md5
host  all  all  192.168.1.67/32 md5
</programlisting>
      </section>

      <section>
        <title>Redis</title>

        <para>Redis is described as a key-value cache and store as opposed to
        a database and is by design a single-threaded server and is not
        designed to benefit from multiple CPUs or multiple CPU cores. On the
        other hand HammerDB is a multi-threaded load-testing and benchmarking
        tool and is specifically designed to use multiple CPU cores and test
        multi-threaded databases. Therefore the expectation should be set that
        a single Redis server will not achieve the same level of performance
        as one of the traditional RDBMS servers with a standard OLTP workload.
        Scalability of Redis beyond a single core can be achieved with
        clustering or sharding which is beyond the scope of this guide. It is
        important to note however that as HammerDB is multi-threaded this does
        not necessarily mean that a single virtual user scenario within Redis
        is always optimal. HammerDB may generate a workload with multiple
        virtual users serviced by a single single-threaded Redis server and
        achieve a higher level of performance and therefore HammerDB testing
        can help determine this optimal Redis client to server ratio.</para>
      </section>
    </section>

    <section>
      <title>Configuring Schema Build Options</title>

      <para>To create the OLTP test schema based on the TPC-C specification
      you will need to select which benchmark and database you wish to use by
      choosing select benchmark from under the Options menu or under the
      benchmark tree-view. The initial settings are determined by the values
      in your XML configuration files. The following example shows the
      selection of SQL Server however the process is the same for all
      databases.</para>

      <figure>
        <title>Benchmark Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch4-2.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>To create the TPC-C schema select the TPC-C schema options menu
      tab from the benchmark tree-view or the options menu. This menu will
      change dynamically according to your chosen database.</para>

      <figure>
        <title>Schema Build Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch4-3.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>If selected from the Options menu the schema options window is
      divided into two sections. The “Build Options” section details the
      general login information and where the schema will be built and the
      “Driver Options” for the Driver Script to run after the schema is built.
      If selected from the benchmark tree-view only the “Build Options” are
      shown and these are the only options of importance at this stage. Note
      that in any circumstance you do not have to rebuild the schema every
      time you change the “Driver Options”, once the schema has been built
      only the “Driver Options” may need to be modified. For the “Build
      Options” fill in the values according to the database where the schema
      will be built as follows.</para>

      <section>
        <title>Oracle Schema Build Options</title>

        <para><figure>
            <title>Oracle Build Options</title>

            <mediaobject>
              <imageobject>
                <imagedata fileref="docs/images/ch4-4.PNG"/>
              </imageobject>
            </mediaobject>
          </figure></para>

        <table>
          <title>Oracle Build Options</title>

          <tgroup cols="2">
            <thead>
              <row>
                <entry align="center">Option</entry>

                <entry align="center">Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>Oracle Service Name</entry>

                <entry>The Oracle Service Name is the service name that your
                load generation server will use to connect to the database
                running on the SUT database server.</entry>
              </row>

              <row>
                <entry>System User</entry>

                <entry>The “system” user or a user with system level
                privileges</entry>
              </row>

              <row>
                <entry>System User Password</entry>

                <entry>The system user password is the password for the
                “system” user you entered during database creation. The system
                user already exists in all Oracle databases and has the
                necessary permissions to create the TPC-C user.</entry>
              </row>

              <row>
                <entry>TPC-C User</entry>

                <entry>The TPC-C user is the name of a user to be created that
                will own the TPC-C schema. This user can have any name you
                choose but must not already exist and adhere to the standard
                rules for naming Oracle users. You may if you wish run the
                schema creation multiple times and have multiple TPC-C schemas
                created with ownership under a different user you create each
                time.</entry>
              </row>

              <row>
                <entry>TPC-C User Password</entry>

                <entry>The TPC-C user password is the password to be used for
                the TPC-C user you create and must adhere to the standard
                rules for Oracle user password. You will need to remember the
                TPC-C user name and password for running the TPC-C driver
                script after the schema is built.</entry>
              </row>

              <row>
                <entry>TPC-C Default Tablespace</entry>

                <entry>The TPC-C default tablespace is the tablespace that
                will be the default for the TPC-C user and therefore the
                tablespace to be used for the schema creation. The tablespace
                must have sufficient free space for the schema to be
                created.</entry>
              </row>

              <row>
                <entry>Order Line Tablespace</entry>

                <entry>If the “Number of Warehouses” as described below is set
                to 200 or more then the “Partition Order Line Table” option
                becomes active. If this is selected then the option to select
                a different tablespace for the Order Line table only becomes
                active. For high performance schemas this gives the option of
                using both a separate tablespace and memory cache for the
                order line table with a different block size.</entry>
              </row>

              <row>
                <entry>TPC-C Temporary Tablespace</entry>

                <entry>The TPC-C temporary tablespace is the temporary
                tablespace that already exists in the database to be used by
                the TPC-C User.</entry>
              </row>

              <row>
                <entry>TimesTen Database Compatible</entry>

                <entry>When selected this option means that the Oracle Service
                Name should be a TimesTen Data Source Name and will grey out
                non-compatible options.</entry>
              </row>

              <row>
                <entry>Use Hash Clusters</entry>

                <entry>When Partitioning is selected this option enables the
                building of static tables as single table hash clusters and
                also disables table locks. These options can provide
                additional levels of scalability on high performance systems
                where contention is observed however will not provide
                significant at entry level.</entry>
              </row>

              <row>
                <entry>Partition Order Line Table</entry>

                <entry>When more than 200 warehouses are selected this option
                uses Oracle partitioning to divide the Order Line table into
                partitions of 100 warehouses each. Using partitioning enables
                scalability for high performance schemas and should be
                considered with using a separate tablespace for the Order Line
                table.</entry>
              </row>

              <row>
                <entry>Number of Warehouses</entry>

                <entry>The Number of Warehouses is selected by a listbox. You
                should set this value to number of warehouses you have chosen
                for your test.</entry>
              </row>

              <row>
                <entry>Virtual Users to Build Schema</entry>

                <entry>The Virtual Users to Build Schema is the number of
                Virtual Users to be created on the Load Generation Server that
                will complete your multi-threaded schema build. You should set
                this value to either the number of warehouses you are going to
                create (You cannot set the number of virtual users lower than
                the number of warehouses value) or the number of
                cores/Hyper-Threads on your Load Generation Server. If you
                have a significantly larger core/Hyper-Thread count on your
                Database Server then also installing HammerDB locally on this
                server as well to run the schema build can take advantage of
                the higher core count to run the build more quickly.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </section>

      <section>
        <title>Microsoft SQL Server Schema Build Options</title>

        <para>The In-Memory OLTP implementation of HammerDB is intended to be
        as close as possible to the original on-disk HammerDB SQL Server
        Schema to enable comparisons between the two. The key areas for memory
        optimization are in-Memory optimized tables, the isolation level and
        the implementation of the stored procedures. Familiarity with the
        architecture of In-memory OLTP can benefit the understanding of the
        performance characteristics.</para>

        <section>
          <title>In-Memory Optimized Tables</title>

          <para>The key difference with the In-memory schema from the on-disk
          database is the organization of the tables. In-memory tables are
          implemented with hash indexes with no additional indexes created
          during the schema creation. Although the differences between hash
          and standard indexes are out of scope for this guide it is
          recommended to become familiar with the architecture as a key
          difference is the requirement to create all of the tables memory
          requirements ‘up-front’ with too little or too much memory impacting
          performance and therefore monitoring of the memory configuration
          usage is essential for workloads operating on In-memory databases.
          For a full implementation of in-memory tables a primary key is
          mandatory, however by definition the HISTORY table does not have a
          primary key. Therefore to implement all tables as in-memory and
          identity column has been added to the HISTORY table. It is important
          to note that despite the nomenclature of in-memory and on-disk
          databases in fact most of the workload of the on-disk database
          actually operates in-memory and high performance implementations can
          limit disk activity almost entirely to transaction logging in
          similarity to an in-memory database with persistence. Consequently
          orders of magnitude performance improvements should not be expected
          by moving to in-memory compared to a well optimised on-disk
          database.</para>

          <para>During schema creation HammerDB sets the option
          MEMORY_OPTIMIZED_ELEVATE_TO_SNAPSHOT for the memory optimized
          database. As a result the use of the snapshot isolation mode is
          mandatory and this will be set without intervention of the user. For
          the on-disk schema the default isolation level of READ COMMITTED is
          used with the addition of hints within the stored procedures for
          specific statements.</para>

          <para>In-memory OLTP introduces the concept of Native Compilation
          for stored procedures that access in-memory tables and the tables
          configured for HammerDB have been implemented with this in mind.
          However at current releases supported features of native compilation
          are highly restricted to the extent that it would not be possible to
          implement stored procedures in a native compilation form that would
          then provide a fair comparison with the on-disk schema. For this
          reason the same T-SQL stored procedures have been implemented with
          minor changes in areas such as removed hints locks and transaction
          isolation levels. Native compilation remains a consideration for
          future releases when the necessary features are supported to provide
          a fair comparison.</para>

          <para>An in-memory database must reside in a memory optimized
          filegroup with one or more containers. This database must be
          pre-created before running the HammerDB schema creation. If the
          database does not exist HammerDB will report the following
          error:</para>

          <programlisting>Database imoltp must be pre-created in a MEMORY_OPTIMIZED_DATA filegroup and empty, to specify an In-Memory build</programlisting>

          <para>If the database exists but is not in a
          MEMORY_OPTIMIZED_FILEGROUP HammerDB will report the following
          error.</para>

          <programlisting>Database imoltp exists but is not in a MEMORY_OPTIMIZED_DATA filegroup</programlisting>

          <para>Therefore to create an in-memory database firstly create a
          standard database using SSMS or at the command line as
          follows:</para>

          <programlisting>use imoltp
GO
ALTER DATABASE imoltp ADD FILEGROUP imoltp_mod CONTAINS memory_optimized_data
GO  
ALTER DATABASE imoltp ADD FILE (NAME='imoltp_mod', FILENAME='C:\Program Files\Microsoft SQL Server\MSSQL13.SQLDEVELOP\MSSQL\data\imoltp_mod') TO FILEGROUP imoltp_mod
GO

</programlisting>

          <para>For SQL Server on Linux specify the filesystem as
          follows:</para>

          <programlisting>ALTER DATABASE imoltp ADD FILE (NAME='imoltp_mod', FILENAME='C:\var\opt\mssql\data\imoltp_mod') TO FILEGROUP imoltp_mod
GO
</programlisting>

          <para>Once the above statements have been run successfully the
          database is ready for an in-memory schema creation.</para>
        </section>

        <section>
          <title>Build Options</title>

          <para><figure>
              <title>SQL Server Build Options</title>

              <mediaobject>
                <imageobject>
                  <imagedata fileref="docs/images/ch4-5.PNG"/>
                </imageobject>
              </mediaobject>
            </figure></para>

          <para><table>
              <title>SQL Server Build Options</title>

              <tgroup cols="2">
                <thead>
                  <row>
                    <entry align="center">Option</entry>

                    <entry align="center">Description</entry>
                  </row>
                </thead>

                <tbody>
                  <row>
                    <entry>SQL Server</entry>

                    <entry>The Microsoft SQL Server is the host name or host
                    name and instance that your load generation server will
                    use to connect to the database running on the SUT database
                    server.</entry>
                  </row>

                  <row>
                    <entry>TCP</entry>

                    <entry>Use the TCP Protocol</entry>
                  </row>

                  <row>
                    <entry>SQL Server Port</entry>

                    <entry>When TCP is enabled, the SQL Server Port is the
                    network port that your load generation server will use to
                    connect to the database running on the SUT database
                    server. In most cases this will be the default port of
                    1433 and will not need to be changed.</entry>
                  </row>

                  <row>
                    <entry>Azure</entry>

                    <entry>Include the Database name in the connect string
                    typical of Azure connections. To successfully build the
                    schema this database must be created and empty.</entry>
                  </row>

                  <row>
                    <entry>SQL Server ODBC Driver</entry>

                    <entry>The Microsoft SQL ODBC Driver is the ODBC driver
                    you will use to connect to the SQL Server database. To
                    view which drivers are available on Windows view the ODBC
                    Data Source Administrator Tool.</entry>
                  </row>

                  <row>
                    <entry>Authentication</entry>

                    <entry>When installing SQL Server on Windows you will have
                    configured SQL Server for Windows or Windows and SQL
                    Server Authentication. On Linux you will be using SQL
                    Server Authentication. If you specify Windows
                    Authentication then SQL Server will use a trusted
                    connection to your SQL Server using your Windows
                    credentials without requiring a username and password. If
                    SQL Server Authentication is specified and SQL
                    Authentication is enabled on your SQL Server then you will
                    be able connect by specifying a username and password that
                    you have already configured on your SQL Server.</entry>
                  </row>

                  <row>
                    <entry>SQL Server User ID</entry>

                    <entry>The SQL Server User ID is the User ID of a user
                    that you have already created on your SQL Server.</entry>
                  </row>

                  <row>
                    <entry>SQL Server User Password</entry>

                    <entry>The SQL Server User Password is the Password
                    configured on the SQL Server for the User ID you have
                    specified. Note that when configuring the password on the
                    SQL Server there is a checkbox that when selected enforces
                    more complex rules for passwords or if unchecked enables a
                    simple password such as “admin”.</entry>
                  </row>

                  <row>
                    <entry>SQL Server Database</entry>

                    <entry>The SQL Server Database is the name of the Database
                    to be created on the SQL Server to contain the schema. If
                    this database does not already exist then HammerDB will
                    create it, if the database does already exist and the
                    database is empty then HammerDB will use this existing
                    database. Therefore if you wish to create a particular
                    layout or schema then pre-creating the database and using
                    this database is an advanced method to use this
                    configuration.</entry>
                  </row>

                  <row>
                    <entry>In-Memory OLTP</entry>

                    <entry>Creates the database as In-Memory OLTP. The
                    database must be pre-created in a MEMORY_OPTIMIZED_DATA
                    filegroup and empty to specify an In-Memory build.</entry>
                  </row>

                  <row>
                    <entry>In-Memory Hash bucket Multiplier</entry>

                    <entry>The size of the In-memory database is specified at
                    creation time, however the OLTP/TPC-C schema allows for
                    the insertion of additional rows. This value enables the
                    creation of larger tables for orders, new_order and
                    order_line to allow for these inserts. Note: Do not
                    specify too large a value or the table creation will fail
                    or performance will be significantly impacted. Typically
                    the default value of 1 is sufficient and will suffice for
                    manually run tests. For autopilot tests where are large
                    number of tests are to be run a value of 3 or 4 will
                    typically be sufficient, however of course the number of
                    inserts will depend on the performance of the system under
                    test and therefore testing is the best way to determine
                    the correct schema size for a particular
                    environment.</entry>
                  </row>

                  <row>
                    <entry>in-Memory Durability</entry>

                    <entry>Sets the durability option. If SCHEMA_ONLY is
                    chosen when SQL Server is stopped only the tables remain
                    without data loaded.</entry>
                  </row>

                  <row>
                    <entry>Number of Warehouses</entry>

                    <entry>The Number of Warehouses is selected by a listbox.
                    You should set this value to number of warehouses you have
                    chosen for your test.</entry>
                  </row>

                  <row>
                    <entry>Virtual Users to Build Schema</entry>

                    <entry>The Virtual Users to Build Schema is the number of
                    Virtual Users to be created on the Load Generation Server
                    that will complete your multi-threaded schema build. You
                    should set this value to either the number of warehouses
                    you are going to create (You cannot set the number of
                    virtual users lower than the number of warehouses value)
                    or the number of cores/Hyper-Threads on your Load
                    Generation Server. If you have a significantly larger
                    core/Hyper-Thread count on your Database Server then also
                    installing HammerDB locally on this server as well to run
                    the schema build can take advantage of the higher core
                    count to run the build more quickly.</entry>
                  </row>
                </tbody>
              </tgroup>
            </table></para>
        </section>
      </section>

      <section>
        <title>Db2 Schema Build Options</title>

        <para>Note that as previously described the host and port are defined
        externally in the db2dsdriver.cfg file.</para>

        <figure>
          <title>Db2 Build Options</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch4-6.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para><table>
            <title>Db2 Build Options</title>

            <tgroup cols="2">
              <thead>
                <row>
                  <entry align="center">Option</entry>

                  <entry align="center">Description</entry>
                </row>
              </thead>

              <tbody>
                <row>
                  <entry>Db2 User</entry>

                  <entry>The name of the operating system user to connect to
                  the DB2 database for example db2inst1.</entry>
                </row>

                <row>
                  <entry>Db2 Password</entry>

                  <entry>The password for the operating system DB2 user by
                  default “ibmdb2”</entry>
                </row>

                <row>
                  <entry>Db2 Database</entry>

                  <entry>The name of the Db2 database that you have already
                  created, for example “tpcc”</entry>
                </row>

                <row>
                  <entry>Db2 Default Tablespace</entry>

                  <entry>The name of the existing tablespace where tables
                  should be located if a specific tablespace has not been
                  defined for that table in the tablespace list. The default
                  is “USERSPACE1”.</entry>
                </row>

                <row>
                  <entry>Db2 Tablespace List (Space Separated Values)</entry>

                  <entry>When partitioning is selected, a space separated list
                  of Tablespace initials followed by a pre-existing tablespace
                  name in double-quotes into which to install a specific
                  table. If no tablespace is given for a specific table then
                  the default tablespace is used. The values are C: CUSTOMER
                  D: DISTRICT H: HISTORY I: ITEM W: WAREHOUSE S: STOCK NO:
                  NEW_ORDER OR: ORDERS OL: ORDER_LINE. And for example the
                  following list, would create all tables in the default. C ""
                  D "" H "" I "" W "" S "" NO "" OR "" OL "". Whereas the
                  following would create the ITEM table in the ITEM_TS
                  tablespace, the STOCK table in the STOCK_TS tablespace and
                  the other tables in the default. C "" D "" H "" I "ITEM_TS"
                  W "" S "STOCK_TS" NO "" OR "" OL "". You may configure all
                  or no distinct tablespaces according to your
                  requirements.</entry>
                </row>

                <row>
                  <entry>Number of Warehouses</entry>

                  <entry>The Number of Warehouses is selected by a listbox.
                  You should set this value to number of warehouses you have
                  chosen for your test.</entry>
                </row>

                <row>
                  <entry>Virtual Users to Build Schema</entry>

                  <entry>The Virtual Users to Build Schema is the number of
                  Virtual Users to be created on the Load Generation Server
                  that will complete your multi-threaded schema build. You
                  should set this value to either the number of warehouses you
                  are going to create (You cannot set the number of virtual
                  users lower than the number of warehouses value) or the
                  number of cores/Hyper-Threads on your Load Generation
                  Server. If you have a significantly larger core/Hyper-Thread
                  count on your Database Server then also installing HammerDB
                  locally on this server as well to run the schema build can
                  take advantage of the higher core count to run the build
                  more quickly.</entry>
                </row>

                <row>
                  <entry>Partition Tables</entry>

                  <entry>This check option becomes active when more than 10
                  warehouses are configured and transparently divides the
                  schema into 10 separate tables for the larger tables for
                  improved scalability and performance. This option is
                  recommended for larger configuration.</entry>
                </row>
              </tbody>
            </tgroup>
          </table></para>
      </section>

      <section>
        <title>MySQL Schema Build Options</title>

        <figure>
          <title>MySQL Build Options</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch4-7.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para><table>
            <title>MySQL Build Options</title>

            <tgroup cols="2">
              <thead>
                <row>
                  <entry align="center">Option</entry>

                  <entry align="center">Description</entry>
                </row>
              </thead>

              <tbody>
                <row>
                  <entry>MySQL Host</entry>

                  <entry>The MySQL Host Name is the host name that your load
                  generation server will use to connect to the database
                  running on the SUT database server.</entry>
                </row>

                <row>
                  <entry>MySQL Port</entry>

                  <entry>The MySQL Port is the network port that your load
                  generation server will use to connect to the database
                  running on the SUT database server. In most cases this will
                  be the default port of 3306.</entry>
                </row>

                <row>
                  <entry>MySQL User</entry>

                  <entry>The MySQL User is the user which has permission to
                  create a database and you previously granted access to from
                  the load generation server. The root user already exists in
                  all MySQL databases and has the necessary permissions to
                  create the TPC-C database.</entry>
                </row>

                <row>
                  <entry>MySQL User Password</entry>

                  <entry>The MySQL user password is the password for the user
                  defined as the MySQL User. You will need to remember the
                  MySQL user name and password for running the TPC-C driver
                  script after the database is built.</entry>
                </row>

                <row>
                  <entry>MySQL Database</entry>

                  <entry>The MySQL Database is the database that will be
                  created containing the TPC-C schema creation. There must
                  have sufficient free space for the database to be
                  created.</entry>
                </row>

                <row>
                  <entry>Transactional Storage Engine</entry>

                  <entry>Use the "show engine" command to display available
                  storage engines and select a storage engine that supports
                  transactions. By default set to InnoDB.</entry>
                </row>

                <row>
                  <entry>Number of Warehouses</entry>

                  <entry>The Number of Warehouses is selected by a listbox.
                  You should set this value to number of warehouses you have
                  chosen for your test.</entry>
                </row>

                <row>
                  <entry>Virtual Users to Build Schema</entry>

                  <entry>The Virtual Users to Build Schema is the number of
                  Virtual Users to be created on the Load Generation Server
                  that will complete your multi-threaded schema build. You
                  should set this value to either the number of warehouses you
                  are going to create (You cannot set the number of virtual
                  users lower than the number of warehouses value) or the
                  number of cores/Hyper-Threads on your Load Generation
                  Server. If you have a significantly larger core/Hyper-Thread
                  count on your Database Server then also installing HammerDB
                  locally n this server as well to run the schema build can
                  take advantage of the higher core count to run the build
                  more quickly.</entry>
                </row>

                <row>
                  <entry>Partition Order Line Table</entry>

                  <entry>Partition Order Line Table for improved
                  scalability.</entry>
                </row>
              </tbody>
            </tgroup>
          </table></para>
      </section>

      <section>
        <title>PostgreSQL Schema Build Options</title>

        <figure>
          <title>PostgreSQL Build Options</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch4-8.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para/>

        <para><table>
            <title>PostgreSQL Build Options</title>

            <tgroup cols="2">
              <thead>
                <row>
                  <entry align="center">Option</entry>

                  <entry align="center">Description</entry>
                </row>
              </thead>

              <tbody>
                <row>
                  <entry>PostgreSQL Host</entry>

                  <entry>The host name of the SUT running PostgreSQL which the
                  load generation server running HammerDB will connect
                  to.</entry>
                </row>

                <row>
                  <entry>PostgreSQL Port</entry>

                  <entry>The port of the PostgreSQL service. By default this
                  will be 5432 for a standard PostgreSQL installation or 5444
                  for EnterpriseDB.</entry>
                </row>

                <row>
                  <entry>PostgreSQL Superuser</entry>

                  <entry>The PostgreSQL Superuser is a user with sufficient
                  privileges to create both new users (roles) and databases to
                  enable the creation of the test schema.</entry>
                </row>

                <row>
                  <entry>PostgreSQL Superuser Password</entry>

                  <entry>The PostgreSQL Superuser Password is the password for
                  the PostgreSQL superuser which will have been defined during
                  installation. If you have forgotten the password it can be
                  reset from a psql prompt that has logged in from a trusted
                  connection therefore requiring no password using postgres=#
                  alter role postgres password ‘postgres’;</entry>
                </row>

                <row>
                  <entry>PostgreSQL Default Database</entry>

                  <entry>The PostgreSQL default databases is the database to
                  specify for the superuser connection. Typically this will be
                  postgres for a standard PostgreSQL installation or edb for
                  EnterpriseDB.</entry>
                </row>

                <row>
                  <entry>PostgreSQL User</entry>

                  <entry>The PostgreSQL User is the user (role) that will be
                  created that owns the database containing the TPC-C
                  schema.</entry>
                </row>

                <row>
                  <entry>PostgreSQL User Password</entry>

                  <entry>The PostgreSQL User Password is the password that
                  will be specified for the PostgreSQL user when it is
                  created.</entry>
                </row>

                <row>
                  <entry>PostgreSQL Database</entry>

                  <entry>The PostgreSQL Database is the database that will be
                  created and owned by the PostgreSQL User that contains the
                  TPC-C schema.</entry>
                </row>

                <row>
                  <entry>EnterpriseDB Oracle Compatible</entry>

                  <entry>Choosing EnterpriseDB Oracle compatible creates a
                  schema using the Oracle compatible features of EnterpriseDB
                  in an installation of Postgres Plus Advanced Server. This
                  build uses Oracle PL/SQL for the creation of the stored
                  procedures.</entry>
                </row>

                <row>
                  <entry>PostgreSQL Stored Procedures</entry>

                  <entry>When running on PostgreSQL v11 or upwards use
                  PostgreSQL stored procedures instead of functions.</entry>
                </row>

                <row>
                  <entry>Number of Warehouses</entry>

                  <entry>The Number of Warehouses is selected by a listbox.
                  You should set this value to number of warehouses you have
                  chosen for your test.</entry>
                </row>

                <row>
                  <entry>Virtual Users to Build Schema</entry>

                  <entry>The Virtual Users to Build Schema is the number of
                  Virtual Users to be created on the Load Generation Server
                  that will complete your multi-threaded schema build. You
                  should set this value to either the number of warehouses you
                  are going to create (You cannot set the number of virtual
                  users lower than the number of warehouses value) or the
                  number of cores/Hyper-Threads on your Load Generation
                  Server. If you have a significantly larger core/Hyper-Thread
                  count on your Database Server then also installing HammerDB
                  locally on this server as well to run the schema build can
                  take advantage of the higher core count to run the build
                  more quickly.</entry>
                </row>
              </tbody>
            </tgroup>
          </table></para>
      </section>

      <section>
        <title>Redis Schema Build Options</title>

        <figure>
          <title>Redis Build Options</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch4-9.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para><table>
            <title>Redis Build Options</title>

            <tgroup cols="2">
              <thead>
                <row>
                  <entry align="center">Option</entry>

                  <entry align="center">Description</entry>
                </row>
              </thead>

              <tbody>
                <row>
                  <entry>Redis Host</entry>

                  <entry>The host name of the SUT running Redis which the load
                  generation server running HammerDB will connect to.</entry>
                </row>

                <row>
                  <entry>Redis Port</entry>

                  <entry>The port of the Redis service. By default this will
                  be 6379.</entry>
                </row>

                <row>
                  <entry>Redis Namespace</entry>

                  <entry>The Redis namespace is equivalent to a separate
                  schema – assign a number for the namespace for the TPC-C
                  schema.</entry>
                </row>

                <row>
                  <entry>Number of Warehouses</entry>

                  <entry>The Number of Warehouses is selected by a listbox.
                  You should set this value to number of warehouses you have
                  chosen for your test.</entry>
                </row>

                <row>
                  <entry>Virtual Users to Build Schema</entry>

                  <entry>The Virtual Users to Build Schema is the number of
                  Virtual Users to be created on the Load Generation Server
                  that will complete your multi-threaded schema build. You
                  should set this value to either the number of warehouses you
                  are going to create (You cannot set the number of virtual
                  users lower than the number of warehouses value) or the
                  number of cores/Hyper-Threads on your Load Generation
                  Server. If you have a significantly larger core/Hyper-Thread
                  count on your Database Server then also installing HammerDB
                  locally on this server as well to run the schema build can
                  take advantage of the higher core count to run the build
                  more quickly.</entry>
                </row>
              </tbody>
            </tgroup>
          </table></para>
      </section>
    </section>

    <section>
      <title>Creating the Schema</title>

      <para>When you have completed your Build Options click OK to store the
      values you have entered. To begin the schema creation select Build from
      the tree-view.</para>

      <para><figure>
          <title>Build</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch4-10.PNG"/>
            </imageobject>
          </mediaobject>
        </figure></para>

      <para>On clicking this button a dialog box is shown</para>

      <para><figure>
          <title>Create Schema</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch4-11.PNG"/>
            </imageobject>
          </mediaobject>
        </figure></para>

      <para>When you click Yes HammerDB will login to your chosen database a
      monitor virtual user and depending on the database create the user with
      the password you have chosen. It will then log out and log in again as
      your chosen user, create the tables and then load the item table data
      before waiting and monitoring the other virtual users. The worker
      virtual users will wait for the monitor virtual user to complete its
      initial work. Subsequently the worker virtual users will create and
      insert the data for their assigned warehouses. There are no intermediate
      data files or manual builds required, HammerDB will both create and load
      your requested data dynamically. Data is inserted in a batch format for
      optimal network performance.</para>

      <figure>
        <title>Schema Build Start</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch4-12.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>When the worker virtual users are complete the monitor virtual
      users will depending on the database create the indexes, stored
      procedures and gather the statistics. When the schema build is complete
      Virtual User 1 will display the message SCHEMA COMPLETE and all virtual
      users will show that they completed their action successfully.</para>

      <figure>
        <title>Schema complete</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch4-13.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <section>
        <title>Deleting or Verifying the Oracle Schema</title>

        <para>If you have made a mistake simply close the application and run
        the following SQL to undo the user you have created.</para>

        <programlisting>SQL&gt;drop user tpcc cascade;</programlisting>

        <para>When you have created your schema you can verify the contents
        with SQL*PLUS or your favourite admin tool as the newly created
        user.</para>

        <para><programlisting>SQL&gt; select tname, tabtype from tab;

TNAME                          TABTYPE
------------------------------ -------
HISTORY                          TABLE
CUSTOMER                         TABLE
DISTRICT                         TABLE
ITEM                             TABLE
WAREHOUSE                        TABLE
STOCK                            TABLE
NEW_ORDER                        TABLE
ORDERS                           TABLE
ORDER_LINE                       TABLE

9 rows selected.

SQL&gt; select * from warehouse;

      W_ID      W_YTD      W_TAX W_NAME     W_STREET_1
---------- ---------- ---------- ---------- --------------------
W_STREET_2           W_CITY               W_ W_ZIP
-------------------- -------------------- -- ---------
         1  773095764        .11 4R0mUe     rM8f7zFYdx
JyiNY5zg1gQNBDO      v2973cRoiFSJ0z       OF 374311111


SQL&gt; select index_name, index_type from ind;

INDEX_NAME                     INDEX_TYPE
------------------------------ ---------------------------
IORDL                          IOT - TOP
ORDERS_I1                      NORMAL
ORDERS_I2                      NORMAL
INORD                          IOT - TOP
STOCK_I1                       NORMAL
WAREHOUSE_I1                   NORMAL
ITEM_I1                        NORMAL
DISTRICT_I1                    NORMAL
CUSTOMER_I1                    NORMAL
CUSTOMER_I2                    NORMAL

10 rows selected.

SQL&gt;

SQL&gt; select object_name from user_procedures;

OBJECT_NAME
------------------------------
NEWORD
DELIVERY
PAYMENT
OSTAT
SLEV

SQL&gt; select sum(bytes)/1024/1024 as MB from user_segments;
        MB
----------
   838.125

</programlisting></para>
      </section>

      <section>
        <title>Deleting or Verifying the SQL Server Schema and In-memory
        Schema</title>

        <para>If you have made a mistake simply close the application and in
        SQL Server Management Studio right-click the database and choose
        Delete. Select the Close existing connections checkbox and click OK.
        When you have created your schema you can verify the contents with the
        SQL Server Management Studio or SQL Connection, for example:</para>

        <para><programlisting>C:\Users&gt;sqlcmd -S (local)\SQLDEVELOP -E -Q "use tpcc; select name from sys.tables"
Changed database context to 'tpcc'.
name
---------------------------------------------------------------------------
CUSTOMER
DISTRICT
HISTORY
ITEM
NEW_ORDER
ORDERS
ORDER_LINE
STOCK
WAREHOUSE
(9 rows affected)

C:\Users&gt;sqlcmd -S (local)\SQLDEVELOP -E -Q "use tpcc; select * from wareh
ouse where w_id = 1"
Changed database context to 'tpcc'.
w_id        w_ytd                 w_tax        w_name     w_street_1           w
_street_2           w_city               w_state w_zip     padding
          1          3000000.0000        .1000 s21C90Ft   pd1mYv9GlqyIww      u
6sOhAB9HF7iOZpM     llz9x35NhpVcrJc47Wy  VL      182111111 xxxxxxxxxxxxxxxxxxxxx(....)
(1 rows affected)
</programlisting>When an In-memory schema has been created under SSMS right
        click the created database and select reports followed memory usage by
        memory optimized objects, this produces a report such as follows for a
        10 warehouse configuration. As with an on-disk schema a rough estimate
        of 100MB per warehouse can be used for the space required. In
        particular note that SQL Server Express has a particularly small
        memory allocation of 252MB and can be used for tests on 1 warehouse
        only for a short period of time before this limit will be reached. The
        error reported in the log will be as follows:</para>

        <programlisting>Could not perform the operation because the database has reached its quota for in-memory tables.

</programlisting>

        <figure>
          <title>SQL Server in-Memory</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch4-22.png"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>Additionally after schema creation and during testing monitor
        bucket usage as follows:</para>

        <programlisting>use imoltp 
SELECT  
    QUOTENAME(SCHEMA_NAME(t.schema_id)) + N'.' + QUOTENAME(OBJECT_NAME(h.object_id)) as [table],   
    i.name                   as [index],   
    h.total_bucket_count,  
    h.empty_bucket_count,  
      
    FLOOR((  
      CAST(h.empty_bucket_count as float) /  
        h.total_bucket_count) * 100)  
                             as [empty_bucket_percent],  
    h.avg_chain_length,   
    h.max_chain_length  
  FROM  
         sys.dm_db_xtp_hash_index_stats  as h   
    JOIN sys.indexes                     as i  
            ON h.object_id = i.object_id  
           AND h.index_id  = i.index_id  
    JOIN sys.memory_optimized_tables_internal_attributes ia ON h.xtp_object_id=ia.xtp_object_id
    JOIN sys.tables t on h.object_id=t.object_id
  WHERE ia.type=1
  ORDER BY [table], [index];

</programlisting>

        <para>This script produces a report as follows where the
        empty_bucket_percent should indicate a good level of free space and
        the max_chain_length is not too long.</para>

        <figure>
          <title>In-memory report</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch4-23.png"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>

      <section>
        <title>Deleting or Verifying the Db2 Schema</title>

        <para>If you have made a mistake simply close the application and run
        the following SQL to undo the user you have created.</para>

        <programlisting>db2inst1 ~]$ db2 drop database tpcc
DB20000I  The DROP DATABASE command completed successfully.
</programlisting>

        <para>To browse the Db2 schema do the following.</para>

        <programlisting>[db2inst1 ~]$ db2 connect to tpcc

   Database Connection Information

 Database server        = DB2/LINUXX8664 10.5.5
 SQL authorization ID   = DB2INST1
 Local database alias   = TPCC

[db2inst1 ~]$ db2
(c) Copyright IBM Corporation 1993,2007
Command Line Processor for DB2 Client 10.5.5

db2 =&gt;  select * from warehouse fetch first 10 rows only

W_NAME     W_STREET_1           W_STREET_2           W_CITY               W_STATE W_ZIP     W_TAX                    W_YTD          W_ID       
---------- -------------------- -------------------- -------------------- ------- --------- ------------------------ -------------- -----------
5SuPObQR4  FCPEw6PzfOCdp5DHDq7e d9lOkysRKPyPtqB      G0Nt9PuUyR8qZxCOXms0 9Y      546011111            +1.70000E-001     3000000.00           1
QP75kKTagb sOaOeFYpGjc5lvA8BW   f6HbFCH2S6mh         cCPt1emu6hFjobgOqeP  TT      533211111            +1.50000E-001     3000000.00           2
Hu3QQhR    KwwcMmuWbpoiQRM      9MaTxygtYX4Dz        NFSkHHdHyEChXclP4iqA cE      919511111            +1.60000E-001     3000000.00           3
aqN3Df     PAJg6lOtk7r          XxWjB1HMQhOlJ        jknxafMFlirG8pUpntm  mG      217211111            +1.80000E-001     3000000.00           4
zZBreP     gCMDTWuJUHh          AG0vp9mbvGh          t7dDHFKFhd72WKP      xa      342611111            +1.30000E-001     3000000.00           5
bleOmY     pzPzlBidlwneHdMkq    dmZvxDxmrL4WdQNg     jC2DTpxGc1g1LQlk5P8n bt      980911111            +1.50000E-001     3000000.00           6
BFmMdkLUUK joucFFovxwZWcdsBPZ   IBjiEBzqn7dtuU       8FNwUX40bJ56Iwh      gC      751911111            +1.00000E-001     3000000.00           7
xWY9EugeeD t5dK0z1bQWwEuMGMnb59 sYEzAdgb9FeuX        K7PkSQHSno0NSHEet4xr 1Q      270611111            +1.70000E-001     3000000.00           8
5XtsHe1kw  uNJGs1Y1lQnYLAX      qvOfjMIqml5kHzm      C3iX14JTbnCyoRVR     ai      203011111            +2.00000E-001     3000000.00           9
t89Pm591   CKjgdxmZ5AgvZ        LqyRXzAoFUO          2O0j38eGPNMXFb       XU      372011111            +1.40000E-001     3000000.00          10

  10 record(s) selected.

db2 =&gt; 
</programlisting>
      </section>

      <section>
        <title>Deleting or Verifying the MySQL Schema</title>

        <para>If you have made a mistake simply close the application and run
        the following SQL to undo the database you have created.</para>

        <programlisting>SQL&gt;drop database tpcc;</programlisting>

        <para>you can verify the contents with SQL or your favourite admin
        tool as the newly created user.</para>

        <programlisting>mysql&gt; use tpcc;
Database changed
mysql&gt; show tables;
+----------------+
| Tables_in_tpcc |
+----------------+
| customer       |
| district       |
| history        |
| item           |
| new_order      |
| order_line     |
| orders         |
| stock          |
| warehouse      |
+----------------+
9 rows in set (0.00 sec)

mysql&gt; select * from warehouse limit 1 \G
*************************** 1. row ***************************
      w_id: 1
     w_ytd: 3000000.00
     w_tax: 0.1300
    w_name: mBr6dkgK
w_street_1: FH0SO5CUEREo
w_street_2: cBcStSxKcIIs4IAUUsJy
    w_city: FKaak9ZBgtJr3Tr6gESW
   w_state: Tt
     w_zip: 432611111
1 row in set (0.00 sec)

mysql&gt; show indexes from warehouse \G
*************************** 1. row ***************************
        Table: warehouse
   Non_unique: 0
     Key_name: PRIMARY
 Seq_in_index: 1
  Column_name: w_id
    Collation: A
  Cardinality: 10
     Sub_part: NULL
       Packed: NULL
         Null:
   Index_type: BTREE
      Comment:
Index_comment:
1 row in set (0.00 sec)

mysql&gt; select routine_name from information_schema.routines where routine_schema
 = 'TPCC';
+--------------+
| routine_name |
+--------------+
| DELIVERY     |
| NEWORD       |
| OSTAT        |
| PAYMENT      |
| SLEV         |
+--------------+
5 rows in set (0.03 sec)
</programlisting>
      </section>

      <section>
        <title>Deleting or Verifying the PostgreSQL Schema</title>

        <para>If you have made a mistake simply close the application and run
        the following SQL to undo the user you have created.</para>

        <programlisting>postgres=# drop database tpcc;
postgres=# drop role tpcc;
</programlisting>

        <para>You can browse the created schema, for example:</para>

        <para><programlisting>-bash-4.1$ ./bin/psql -d tpcc
Password: 
psql.bin (9.3.4.10)
Type "help" for help.

tpcc=# select relname, n_tup_ins - n_tup_del as rowcount from pg_stat_user_tables;
  relname   | rowcount 
------------+----------
 orders     |   300000
 district   |      100
 stock      |  1000000
 warehouse  |       10
 history    |   300000
 new_order  |    90000
 item       |   100000
 order_line |  3001170
 customer   |   300000
(9 rows)
</programlisting></para>
      </section>

      <section>
        <title>Deleting or Verifying the Redis Schema</title>

        <para>If you have made a mistake simply close the application and run
        the following SQL to undo the user you have created.</para>

        <programlisting>127.0.0.1:6379[1]&gt; flushdb
OK
</programlisting>

        <para>you can browse the created schema, for example:</para>

        <para><programlisting>[redis@MERLIN redis-2.8.13]$ ./src/redis-cli 
127.0.0.1:6379&gt; select 1
OK
127.0.0.1:6379[1]&gt; keys WAREHOUSE:*
 1) "WAREHOUSE:9"
 2) "WAREHOUSE:5"
 3) "WAREHOUSE:4"
 4) "WAREHOUSE:2"
 5) "WAREHOUSE:8"
 6) "WAREHOUSE:6"
 7) "WAREHOUSE:3"
 8) "WAREHOUSE:10"
 9) "WAREHOUSE:1"
10) "WAREHOUSE:7"
(0.53s)
127.0.0.1:6379[1]&gt; 
</programlisting></para>
      </section>
    </section>

    <section>
      <title>Configuring Driver Script options</title>

      <para>To configure the Driver Script select Options under the Driver
      Script tree-view.</para>

      <figure>
        <title>Driver Script Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch4-14.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>This displays the Driver Script Options dialog. The connection
      options are common to the Schema Build Dialog in addition to new Driver
      Options.</para>

      <figure>
        <title>Driver Script Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch4-15.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <table>
        <title>Driver Script Options</title>

        <tgroup cols="2">
          <thead>
            <row>
              <entry align="center">Option</entry>

              <entry align="center">Description</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>TPC-C Driver Script</entry>

              <entry>For all databases you have the option of selecting a Test
              Driver Script or a Timed Driver Script. The This choice will
              dynamically change the Driver Script that is loaded when the
              TPC-C Driver Script menu option is chosen. The Test Driver
              Script is intended for verifying and testing a configuration
              only by displaying virtual user output for a small number of
              virtual users. In particular both Windows and Linux graphical
              displays are single-threaded permitting only one Virtual User to
              write to the display at any one time. Therefore the performance
              of writing to the display will limit throughput. Consequently
              once a schema is verified to conduct measured tests you should
              select the Timed Driver Script Option.</entry>
            </row>

            <row>
              <entry>Total Transactions per User</entry>

              <entry>Total transactions per user is reported as
              total_iterations within the EDITABLE OPTIONS section of the
              driver script. This value will set the number of transactions
              each virtual user will process before logging off. You can use
              this value to determine how long the virtual user will remain
              active for. The length of time for activity will depend upon the
              performance of the Database Server under test. A higher
              performing server will process the defined number of
              transactions more quickly than a lower performing one. It is
              important to draw the distinction between the total_iterations
              value and the Iterations value set in the Virtual User Options
              window. The Iterations value in the Virtual User Options window
              determines the number of times that a script will be run in its
              entirety. The total_iterations value is internal to the TPC-C
              driver script and determines the number of times the internal
              loop is iterated ie for {set it 0} {$it &lt; $total_iterations}
              {incr it} { ... } In other words if total_iterations is set to
              1000 then the executing user will log on once execute 1000
              transactions and then log off. If on the other hand Iterations
              in the Virtual User Options window is set to 1000 and
              total_iterations in the script set to 1 then the executing user
              will log on execute one transaction and then log off 1000
              times.You should ensure that the number of transactions is set
              to a suitably high vale to ensure that the virtual users do not
              complete their tests before the timed test is complete, doing so
              will mean the you will be timing idle virtual users and the
              results will be invalid.</entry>
            </row>

            <row>
              <entry>Exit on Error</entry>

              <entry>Exit on Error is shown as the parameter RAISEERROR in the
              Driver Script. RAISEERROR impacts the behaviour of an individual
              virtual user on detecting a database error. If set to TRUE on
              detecting an error the user will report the error into the
              HammerDB console and then terminate execution. If set to FALSE
              the virtual user will ignore the error and proceed with
              executing the next transaction. It is therefore important to be
              aware that if set to FALSE firstly if there has been a
              configuration error resulting in repeated errors then the
              workload might not be reported accurately and secondly you may
              not be aware of any occasional errors being reported as they are
              silently ignored.</entry>
            </row>

            <row>
              <entry>Keying and Thinking Time</entry>

              <entry>Keying and Thinking Time is shown as KEYANDTHINK in the
              Driver Script. A good introduction to the importance of keying
              and thinking time is to read the TPC-C specification. This
              parameter will have the biggest impact on the type of workload
              that your test will take. Keying and thinking time is an
              integral part of an official TPC-C test in order to simulate the
              effect of the workload being run by a real user who takes time
              to key in an actual order and think about the output. If
              KEYANDTHINK is set to TRUE each user will simulate this real
              user type workload. An official TPC-C benchmark implements 10
              users per warehouse all simulating this real user experience and
              it should therefore be clear that the main impact of KEYANDTHINK
              being set to TRUE is that you will need a significant number of
              warehouses and users in order to generate a meaningful workload
              and hence an extensive testing infrastructure. The positive side
              is that when testing hundreds or thousands of virtual users you
              will be testing a workload scenario that will be closer to a
              real production environment. Whereas with KEYANDTHINK set to
              TRUE each user will execute maybe 2 or 3 transactions a minute
              you should not underestimate the radical difference that setting
              KEYANDTHINK to FALSE will have on your workload. Instead of 2 or
              3 transactions each user will now execute tens of thousands of
              transactions a minute. Clearly KEYANDTHINK will have a big
              impact on the number of virtual users and warehouses you will
              need to configure to run an accurate workload, if this parameter
              is set to TRUE you will need at least hundreds or thousands of
              virtual users and warehouses, if FALSE then you will need to
              begin testing with 1 or 2 threads, building from here up to a
              maximum workload with the number of warehouses set to a level
              where the users are not contending for the same data. A common
              error is to set KEYANDTHINK to FALSE and then create hundreds of
              users for an initial test, this form of testing will only
              exhibit contention for data between users and nothing about the
              potential of the system. If you do not have an extensive testing
              infrastructure and a large number of warehouses configured set
              KEYANDTHINK to FALSE (whilst remembering that you are not
              simulating a real TPC-C type test) and beginning your testing
              with 1 virtual user building up the number of virtual users for
              each subsequent test in order to plot a transaction
              profile.</entry>
            </row>

            <row>
              <entry>Checkpoint/Vacuum when complete</entry>

              <entry>Where available the database will trigger a checkpoint
              after the workload is complete to write out the modified data
              from the in-memory cache to the disk. if the database is
              correctly configured this will prevent this activity being
              conducted during a test to result in higher performance.</entry>
            </row>

            <row>
              <entry>Minutes of Rampup Time</entry>

              <entry>The Minutes of Ramup Time is shown as rampup in the
              Driver Script. The rampup time defines the time in minutes for
              the monitoring virtual user to wait for the virtual users
              running the workload to connect to the database and build up the
              transaction rate by caching data in the database buffer cache
              before taking the first timed value and timing the test. The
              rampup time should be sufficiently long enough for a workload to
              reach a steady transaction rate before the first timed value is
              taken.</entry>
            </row>

            <row>
              <entry>Minutes for Test Duration</entry>

              <entry>The Minutes for Test Duration is shown as duration in the
              Driver Script. The test duration defines the time of the test
              measured as the time the monitor thread waits after the first
              timed value before taking the second one to signal the test is
              complete and the active virtual users to complete their
              workload.</entry>
            </row>

            <row>
              <entry>Use All Warehouses</entry>

              <entry>By default each Virtual User selects a home warehouse at
              random from at the start of a test and remains with that home
              warehouse. Therefore for example if there are 100 warehouses
              created and 10 virtual users selected to run the Driver Script
              then most of the activity will take place on 10 warehouse only.
              This option means that the Virtual Users select a new warehouse
              for each transaction from an available list divided between all
              Virtual Users at the start of the test therefore ensuring
              greater I/O activity.</entry>
            </row>

            <row>
              <entry>Time Profile</entry>

              <entry>This option should be selected in conjunction with having
              enabled output to the logfile. When selected client side time
              profiling will be conducted for the first active virtual user
              and output written to the logfile.</entry>
            </row>

            <row>
              <entry>Mode</entry>

              <entry>The mode value is taken from the operational mode setting
              set under the Mode Options menu tab under the Mode menu. If set
              to Local or Master then the monitor thread takes snapshots, if
              set to Slave no snapshots are taken. This is useful if multiple
              instances of HammerDB are running in Master and Slave mode to
              ensure that only one instance takes the snapshots.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>

    <section>
      <title>Additional Driver Script Options for Server Side Reports: Oracle,
      Db2 and EnterpriseDB PostgreSQL</title>

      <section>
        <title>Oracle AWR Reports</title>

        <para>The Generation of Oracle AWR reports is built-in functionality
        with the Oracle Timed Test. At the end of the test HammerDB will
        report the snapshot numbers between which the report corresponds to
        the test.</para>
      </section>

      <section>
        <title>Db2 MONREPORT</title>

        <para>In the Db2 driver script options the Minutes for Test Duration
        is shown as monreportinterval in the Driver Script. This defines the
        period of time taken from the minutes for test duration that the
        monitoring user runs a monreport capture. The results are output at
        the end of the test and therefore selecting this option should be done
        in conjunction with the logfile enabled. While the MONREPORT is being
        captured the monitoring virtual user cannot bet terminated as control
        is handed over to the DB2 database and therefore shorter periods of
        report are optimal. In all cases in the MONREPORT interval specified
        is longer than the minutes for test duration then no MONREPORT will be
        captured.</para>
      </section>

      <section>
        <title>EnterpriseDB PostgreSQL DRITA</title>

        <para>If you have Enterprise DB installed and DRITA functionality
        enabled, by selecting this option HammerDB will automatically take
        DRITA snapshots for performance analysis of the workload between
        tests. For DRITA functionality to work you need the parameter
        timed_statistics = on set in your postgresql.conf file. With the test
        complete and the values you recorded if you selected the DRITA option
        you should next generate the DRITA report that corresponds to the
        reported SNAPIDs to show the PostgreSQL wait events, in the example
        below snapshots 2 and 3.</para>

        <programlisting>edb=# select * from sys_rpt(2,3,1000);
                                   sys_rpt                                   
-----------------------------------------------------------------------------
 WAIT NAME                                COUNT      WAIT TIME       % WAIT
 ---------------------------------------------------------------------------
 wal insert lock acquire                  1054357    2.300713        88.25
 xid gen lock acquire                     83471      0.195263        7.49
 db file read                             5523       0.067953        2.61
 buffer free list lock acquire            11133      0.029317        1.12
 query plan                               205        0.013703        0.53
 freespace lock acquire                   3          0.000007        0.00
 rel cache init lock acquire              0          0.000000        0.00
(9 rows)

edb=# 
</programlisting>
      </section>
    </section>

    <section>
      <title>Loading the Driver Script</title>

      <para>After selected the Driver Script Options the Driver Script is
      loaded. The configured options can be seen in the Driver Script window
      and also modified directly there. The Load option can also be used to
      refresh the script to the configured Options.</para>

      <figure>
        <title>Driver Script Loaded</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch4-16.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>

    <section>
      <title>Configure Virtual Users</title>

      <para>Select Virtual User Options from the tree-view.</para>

      <figure>
        <title>Virtual User Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch4-17.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>This displays the Virtual User Options dialog.</para>

      <figure>
        <title>Virtual User</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch4-18.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The values have the following meaning.</para>

      <para><table>
          <title>Virtual User Options</title>

          <tgroup cols="2">
            <thead>
              <row>
                <entry align="center">Option</entry>

                <entry align="center">Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>Virtual Users</entry>

                <entry>The number of Virtual Users to create. Note that when
                running a Timed Workload HammerDB will automatically create an
                additional Virtual User to monitor the workload.</entry>
              </row>

              <row>
                <entry>User Delay(ms)</entry>

                <entry>User Delay(ms) defines the time to wait a Virtual User
                will wait behind the previous Virtual User before starting its
                test, this is to prevent a login storm with all Virtual Users
                attempting to login at the same time.</entry>
              </row>

              <row>
                <entry>Repeat Delay(ms)</entry>

                <entry>Repeat Delay(ms) is the time that each Virtual User
                will wait before running its next Iteration of the Driver
                Script. For the TPC-C workload this should be considered as an
                'outer loop' to the 'inner loop' of the Total Transactions per
                User in the TPC-C Driver Script.</entry>
              </row>

              <row>
                <entry>Iterations</entry>

                <entry>Iterations is the number of times that the Driver
                Script is run in its entirety.</entry>
              </row>

              <row>
                <entry>Show Output</entry>

                <entry>Show Output will report Virtual User Output to the
                Virtual User Output Window, For TPC-C tests this should be
                enabled.</entry>
              </row>

              <row>
                <entry>Log Output to Temp</entry>

                <entry>When enabled this appends all Virtual User Output to a
                text file in an available temp directory named
                hammerdb.log</entry>
              </row>

              <row>
                <entry>Use Unique Log Name</entry>

                <entry>Use a unique identifier for the Log Name.</entry>
              </row>

              <row>
                <entry>No Log Buffer</entry>

                <entry>By default text log output is buffered in memory before
                being written, this option writes the log output
                immediately.</entry>
              </row>

              <row>
                <entry>Log Timestamps</entry>

                <entry>Add an additional line of output with a timestamp every
                time that the log is written to.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>Select the Virtual User options, Press OK.</para>
    </section>

    <section>
      <title>Create and Run Virtual Users</title>

      <para>Double-click Create in the tree-view. The Virtual Users will be
      created and waiting in an idle status ready to run the Driver Script in
      the Script Editor Window. If you press Run instead it will both Create
      and Run the Virtual Users. If you have selected a Timed Workload the
      additional Virtual User created will be shown as a monitor.</para>

      <para><figure>
          <title>Virtual Users Create</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch4-19.PNG"/>
            </imageobject>
          </mediaobject>
        </figure></para>

      <para>Double-click on Run and the Virtual Users will login to the target
      database and begin running their workload.</para>

      <para><figure>
          <title>Virtual Users Running</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch4-20.PNG"/>
            </imageobject>
          </mediaobject>
        </figure></para>

      <para>When complete the Monitor Virtual User will report the Test
      Result, if logging has been selected these values will also be reported
      in the log. Where supported additional database side server report
      information will also be reported.</para>

      <para><figure>
          <title>Virtual Users Complete</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch4-21.PNG"/>
            </imageobject>
          </mediaobject>
        </figure></para>

      <para>you may choose to run all of your performance tests manually in
      this way, however generating a performance profile is the key to
      successful database performance analysis requiring the running of a
      sequence of tests. Consequently HammerDB enables a way to automate the
      running of this sequence of tests with the Autopilot Feature.</para>
    </section>
  </chapter>

  <chapter>
    <title>Autopilot</title>

    <para>To automate this process of repeated tests HammerDB provides the
    autopilot feature that enables you to configure a single test to be
    repeated by a different numbers of virtual users a number of times.
    Conceptually autopilot is best understood as having instructed a virtual
    DBA to manually repeat the test you have configured a number of times at a
    pre-determined time interval. That virtual DBA will then run the tests by
    ‘virtually’ pressing exactly the same buttons on the HammerDB interface
    that you would press as if running the test manually yourself. It is
    important to understand this concept as the most frequent user errors in
    using autopilot are as a result of not following this approach. Before
    running autopilot you should ensure that you have run a number of tests
    manually and your system is in an optimal configuration for running tests
    up to your planned maximum Virtual User count. For example you should
    enable enough space to schema growth throughout all of the tests you plan
    to run.</para>

    <section>
      <title>Configure and Run Autopilot</title>

      <para>To begin configuring Autopilot mode follow the steps described in
      the previous Chapter for Running OLTP Timed Tests before creating and
      running the Virtual Users, these will be configured automatically.
      Select Autopilot Options from the tree-view as shown.</para>

      <para><figure>
          <title>Autopilot Options</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch5-1.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>This shows the Autopilot Options Dialog.</para>

      <para><figure>
          <title>Autopilot Options</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch5-2.PNG"/>
            </imageobject>
          </mediaobject>
        </figure></para>

      <para>Configure the Autopilot options in the same manner as you would
      use to instruct your Virtual DBA:</para>

      <table>
        <title>Autopilot Options</title>

        <tgroup cols="2">
          <thead>
            <row>
              <entry align="center">Option</entry>

              <entry align="center">Description</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>Autopilot Disabled/Autopilot Enabled</entry>

              <entry>This Autopilot Disabled/Autopilot Enabled Radio buttons
              give you the option to select whether the Autopilot button is
              enabled on the main window.</entry>
            </row>

            <row>
              <entry>Minutes per Test in Virtual User Sequence</entry>

              <entry>The minutes for test duration defines the time interval
              between which your virtual DBA will create the Virtual Users,
              stop the test and create the next Virtual Users in the sequence.
              You should configure this value in relation to the Minutes for
              Ramup Time and Minutes for Test Duration given in the Timed Test
              options shown in Figure 31. For example if the values in the
              test script are 2 and 5 minutes respectively then 10 minutes for
              the Autopilot Options is a good value to allow the test to
              complete before the next test in the sequence is run. If the
              test overruns the time interval and the Virtual Users are still
              running the sequence will wait for the Virtual Users to complete
              before proceeding however note any pending output will be
              discarded and therefore for example if the TPM and NOPM values
              have not been reported by the time the test is stopped they will
              not be reported at all.</entry>
            </row>

            <row>
              <entry>Virtual User Sequence (Space Separated Values)</entry>

              <entry>The Virtual User Sequence defines the number of Virtual
              Users to be configured in order for a sequence of tests
              separated by the Minutes for Test Duration. Note that for a
              Timed workload the Monitor Virtual User will be added and
              therefore the sequence defines the number of active worker
              Virtual Users. Therefore in this example the actual users
              running the workload will be 1, 2, 4, 8, 12, 16, 20 and 24
              however and additional one will be created.</entry>
            </row>

            <row>
              <entry>Virtual User Options</entry>

              <entry>These values are exactly the same as set when defining
              the Virtual User Options, you should ensure that Output is
              enabled and configure preferred logging options.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <para>Double-click on the Autopilot Icon to begin running the sequence
      of tests</para>

      <figure>
        <title>Run Autopilot</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch5-3.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The Autopilot Window is shown tracking the Monitor Virtual User
      output and the time interval, no further interaction is required.</para>

      <figure>
        <title>Autopilot Running</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch5-4.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>After the first test HammerDB reports the output and then
      configures the Virtual Users and runs the second test
      automatically.</para>

      <figure>
        <title>Autopilot Continuing</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch5-5.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>When the sequence is complete you will see the message Autopilot
      Sequence ended. You can now gather all of your test results.</para>

      <para><figure>
          <title>Autopilot Complete</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch5-6.PNG"/>
            </imageobject>
          </mediaobject>
        </figure></para>
    </section>

    <section>
      <title>Autopilot Troubleshooting</title>

      <para>The most Frequent Autopilot Configuration Error is caused by
      configuring the Autopilot Time Interval to be less than the combined
      rampup and duration time of the test that is running. When viewed from
      the concept of a "Virtual DBA" this User has been instructed to press
      the Stop button before the test has ended, consequently a warning is
      produced and no output results are reported. To resolve this issue
      ensure that the time interval is set long enough to allow the configured
      tests to complete inside this interval.</para>

      <para><figure>
          <title>Autopilot Error</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch5-7.PNG"/>
            </imageobject>
          </mediaobject>
        </figure></para>
    </section>

    <section>
      <title>Extending Autopilot</title>

      <para>Autopilot can be started automatically by adding the keyword
      “auto” followed by the name of a script to run, this script must end in
      the extension .tcl.</para>

      <programlisting>./hammerdb.tcl auto
Usage: hammerdb.tcl [ auto [ script_to_autoload.tcl  ] ]
</programlisting>

      <para>For example</para>

      <programlisting>./hammerdb.tcl auto newtpccscript.tcl</programlisting>

      <para>On doing so HammerDB will now load the script newtpccscript.tcl at
      startup and immediately enter the autopilot sequence defined in
      config.xml. Upon completion HammerDB will exit. As detailed in the post
      linked above this functionality enables the potential to run workloads
      such as the following with multiple sequences of autopilot interspersed
      with a database refresh.</para>

      <programlisting>#!/bin/bash

set -e
SEQ1="4 6 8 10"
SEQ2="12 14 16 18"
SEQ3="20 22 24 26"
CONFIGFILE='/usr/local/hammerDB/config.xml'
RUNS=6

for x in $(eval echo "{1..$RUNS}")
do
        # Running a number of passes for this autopilot sequence
        echo "running run $x of $RUNS"

        for s in "$SEQ1" "$SEQ2" "$SEQ3"
        do
                echo "Running tests for series: $s"
                sed -i "s/&lt;autopilot_sequence&gt;.*&lt;\/autopilot_sequence&gt;/&lt;autopilot_sequence&gt;${s}&lt;\/autopilot_sequence&gt;/" $CONFIGFILE

                (cd /usr/local/hammerDB/ &amp;&amp; ./hammerdb.tcl auto TPCC.postgres.tcl)

                echo "Reloading data"
                ssh postgres@postgres  '/var/lib/pgsql/reloadData.sh'
        done
done
</programlisting>
    </section>
  </chapter>

  <chapter>
    <title>Transactions</title>

    <para>HammerDB includes a Transaction Counter that logs into the target
    database and samples the transaction rate displaying it in graph format to
    view the TPM of a test in real time. Note that the TPM value is displayed
    as opposed to the NOPM value as TPM is selected from a database in-memory
    table and therefore sampling does not impact the test being measured. NOPM
    on the other hand is sampled from the schema itself and is therefore only
    measured at the start and end of the test to minimize the impact. To
    configure the Transaction Counter select the Transactions tree-view. If
    Virtual Users are running the Transaction Counter Options can be selected
    from the menu.</para>

    <figure>
      <title>Transaction Counter Options</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="docs/images/ch6-1.PNG"/>
        </imageobject>
      </mediaobject>
    </figure>

    <section>
      <title>Oracle Transaction Counter</title>

      <para>For Oracle the connection parameters are the same as the schema
      options. There is also an option to query a TimesTen database instead of
      an Oracle one and to select transactions from an Oracle RAC cluster. The
      refresh rate determines the sampling interval.</para>

      <figure>
        <title>Oracle TX Counter Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch6-2.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>For single instance Oracle, transactions are sampled with the
      following statement. This displays transactions with the same value as
      used in Oracle Enterprise Manager and in Oracle AWR reports.</para>

      <programlisting>select sum(value) from v$sysstat where name = 'user commits' or name = 'user rollbacks'</programlisting>

      <para>For Oracle RAC gv$sysstat is queried for global
      transactions.</para>

      <programlisting>select sum(value) from gv$sysstat where name = 'user commits' or name = 'user rollbacks'</programlisting>

      <para>for TimesTen the following SQL is used.</para>

      <programlisting>select (xact_commits + xact_rollbacks) from sys.monitor</programlisting>
    </section>

    <section>
      <title>SQL Server Transaction Counter</title>

      <para>For SQL Server the connection parameters are the same as the
      schema options. The refresh rate determines the sampling
      interval.</para>

      <para/>

      <figure>
        <title>SQL Server TX Counter Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch6-3.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The following SQL is used to sample the transaction rate
      displaying the same value as can be seen in the Activity Monitor in
      SSMS.</para>

      <programlisting>select cntr_value from sys.dm_os_performance_counters where counter_name = 'Batch Requests/sec'</programlisting>
    </section>

    <section>
      <title>Db2 Transaction Counter</title>

      <para>For Db2 the connection parameters are the same as the schema
      options. The refresh rate determines the sampling interval.</para>

      <figure>
        <title>Db2 TX Counter Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch6-4.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The following SQL is used to sample the transaction rate.</para>

      <programlisting>select total_app_commits + total_app_rollbacks from sysibmadm.mon_db_summary</programlisting>
    </section>

    <section>
      <title>MySQL Transaction Counter</title>

      <para>For MySQL the connection parameters are the same as the schema
      options. The refresh rate determines the sampling interval.</para>

      <figure>
        <title>MySQL TX Counter Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch6-5.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The following SQL is used to sample the transaction rate.</para>

      <programlisting>show global status where Variable_name = 'Com_commit' or Variable_name =  'Com_rollback'</programlisting>

      <para>Note that Com_commit is used instead of the handler_commit value
      used in previous releases of HammerDB as a result of MySQL Bug #52453
      handler_commit is incremented for InnoDB SELECT queries.</para>
    </section>

    <section>
      <title>PostgreSQL Transaction Counter</title>

      <para>For PostgreSQL the connection parameters are the same as the
      schema options. The refresh rate determines the sampling
      interval.</para>

      <figure>
        <title>PostgreSQL TX Counter Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch6-6.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The following SQL is used to sample the transaction rate.</para>

      <programlisting>select sum(xact_commit + xact_rollback) from pg_stat_database</programlisting>
    </section>

    <section>
      <title>Redis Transaction Counter</title>

      <para>For Redis the connection parameters are the same as the schema
      options. The refresh rate determines the sampling interval.</para>

      <figure>
        <title>Redis TX Counter Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch6-7.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>the info command is used to sample the transaction rate extracting
      the value total_commands_processed.</para>

      <programlisting>info (total_commands_processed)</programlisting>
    </section>

    <section>
      <title>Running the Transaction Counter</title>

      <para>During a test, select the start transaction counter button.</para>

      <figure>
        <title>Start Transaction Counter</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch6-8.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>On starting the transaction counter will begin sampling the
      transaction data.</para>

      <figure>
        <title>Transaction Counter Starting</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch6-9.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The transaction counter will be displayed and continually sample
      and display the transaction rate during the test. It is important to
      note that the transaction rate is sampled with the SQL detailed above
      for the database selected and therefore all transactions on the database
      are sampled whether from HammerDB or another application running at the
      same time. Similarly if 2 or more instances of HammerDB are run against
      the same database at the same time, the cumulative transaction is
      sampled.</para>

      <figure>
        <title>Transaction Counter Running</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch6-10.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>While active the Transaction Counter Window can be dragged out of
      the main HammerDB display to be displayed in an standalone window. To
      return to the main display close the window and it will be
      re-embedded.</para>

      <figure>
        <title>Transaction Counter standalone.</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch6-11.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>
  </chapter>

  <chapter>
    <title>CPU and Database Metrics</title>

    <para>By default HammerDB metrics displays the CPU utilisation per core
    across the target system. HammerDB has also introduced a database metrics
    display initially for the Oracle Database. HammerDB Metrics uses an agent
    and display configuration meaning that the agent must be installed on the
    SUT. This can be accomplished by installing HammerDB on the SUT as well as
    the server. On Linux the sysstat package must be pre-installed where the
    agent is running.</para>

    <programlisting>$ mpstat -V
sysstat version 11.5.7
(C) Sebastien Godard (sysstat &lt;at&gt; orange.fr</programlisting>

    <para>On Windows a version of mpstat is included with HammerDB.</para>

    <section>
      <title>Start the Agent</title>

      <para>To start the agent on Linux run the agent program locally in the
      agent directory.</para>

      <programlisting>$./agent 
Initializing HammerDB Metric Agent 3.0
HammerDB Metric Agent active @ id 13029 hostname CRANE (Ctrl-C to Exit)</programlisting>

      <para>On Windows double-click on agent.bat in the agent
      directory.</para>

      <figure>
        <title>agent.bat</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch7-1.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>On Windows you may see the following security alert as the agent
      will open a port for communication, access needs to be permitted to
      enable communication.</para>

      <figure>
        <title>Security Alert</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch7-2.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>A window will open indicating the id that the agent is listening
      on. Pressing Ctrl-C or closing the window will close the agent.</para>

      <figure>
        <title>Windows agent</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch7-3.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>

    <section>
      <title>Configure Metrics</title>

      <para>In HammerDB select Metrics Options</para>

      <figure>
        <title>Metrics Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch7-4.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Enter the id and canonical hostname of the agent and press
      OK.</para>

      <figure>
        <title>Agent Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch7-5.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Double-click on Display</para>

      <figure>
        <title>Display</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch7-6.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>the agent will report the connection of the display</para>

      <figure>
        <title>Agent connected</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch7-7.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>and the display will report the connection information of the
      agent</para>

      <para><figure>
          <title>Display connected</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch7-8.PNG"/>
            </imageobject>
          </mediaobject>
        </figure></para>
    </section>

    <section>
      <title>Monitor Metrics</title>

      <para>The display will now report the CPU utilisation per core on the
      SUT during a workload with user CPU utilisation shown in green and
      system utilisation shown in red. This per core about is particularly
      useful for diagnosing database workload issues where the load is not
      evenly distributed across all cores such as interrupt handling.</para>

      <figure>
        <title>Metrics running</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch7-9.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>As with the transaction counter the Metrics display can be dragged
      out of the main window for separate viewing and the scrollbar used for
      reviewing large core counts.</para>

      <figure>
        <title>Large Core count</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch7-10.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>If the agent or display is closed the corresponding connection
      will also close and wait for a new connection. The Metrics Display can
      be closed by pressing the corresponding stop button.</para>
    </section>

    <section>
      <title>Oracle Database Metrics</title>

      <para>When the Oracle Database is selected on both Windows and Linux an
      additional option is available to connect to the Oracle Database.</para>

      <figure>
        <title>Oracle Metrics Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch7-11.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>When the metrics button is pressed HammerDB connects to the
      database and displays graphical information from the Active Session
      History detailing wait events. In the example below the graph shows that
      at the beginning of a workload the top wait event was User IO, followed
      by CPU activity however a significant wait event on Commit.</para>

      <figure>
        <title>Oracle Metrics Display Linux</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch7-12.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>As the graph extracts information from the Active Session History,
      it is possible to select a section from the window and display the wait
      events related to that period of time. The buttons enable the viewing of
      SQL text, the explain plan, IO statistics and SQL statistics. Note that
      the CPU metrics functionality as previously described is available under
      the CPU button.</para>

      <figure>
        <title>Oracle Metrics Display Windows</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch7-13.png"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>
  </chapter>

  <chapter>
    <title>Command Line Interface (CLI)</title>

    <para>HammerDB can be run from the command line without a graphical
    interface. It is recommend that new users become familiar with using the
    graphical interface before using the command line as the command line
    offers the same workflow and therefore once the graphical interface is
    understood learning the command line will be more straightforward. The CLI
    implements equivalent readline functionality for navigation. The CLI can
    be used in conjunction with scripting to build a powerful automated
    environment.</para>

    <section>
      <title>Start the CLI</title>

      <para>To start the command line in interactive mode on Linux run:</para>

      <programlisting>hammerdb&gt;steve@CRANE:~/HammerDB-3.0$ ./hammerdbcli 
HammerDB CLI v3.0
Copyright (C) 2003-2018 Steve Shaw
Type "help" for a list of commands
The xml is well-formed, applying configuration
hammerdb&gt;</programlisting>

      <para>On Windows double-click hammerdbcli.bat</para>

      <figure>
        <title>hammerdbcli.bat</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch8-1.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>This will display a console command Window. On Windows this
      console command window has been designed to run with white text on a
      black background. If necessary the colours can be changed using the
      Windows COLOR command.</para>

      <figure>
        <title>CLI Windows</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch8-2.PNG"/>
          </imageobject>
        </mediaobject>

        <para>It is also possible to run a script directly from the command
        line by providing the auto argument preceding the name of a script to
        tun. For example the following script is named buildcli.tcl on a
        Windows based system. Note that the line "vwait forever" has been
        added to the end of the script to be run. This is required to enter
        the event loop for correct processing of the script when being called
        in this manner.</para>

        <programlisting>#!/bin/tclsh
puts "SETTING CONFIGURATION"
global complete
proc wait_to_complete {} {
global complete 
set complete [vucomplete]
if {!$complete} { after 5000 wait_to_complete } else { exit }
}
dbset db mssqls
diset connection mssqls_server (local)\\SQLDEVELOP
diset tpcc mssqls_count_ware 2
diset tpcc mssqls_num_vu 2
print dict
buildschema
wait_to_complete
vwait forever</programlisting>

        <para>By following hammerdbcli.bat (or hammerdbcli on Linux) by the
        auto argument and the name of the script,</para>

        <figure>
          <title>Run buildcli.tcl</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch8-2a.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>The script is then run directly without interaction.</para>

        <figure>
          <title>buildcli.tcl running</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch8-2b.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>
      </figure>
    </section>

    <section>
      <title>CLI Commands</title>

      <para>To learn CLI commands type "help".</para>

      <programlisting>HammerDB v3.1 CLI Help Index

Type "help command" for more details on specific commands below

        buildschema
        clearscript
        customscript
        datagenrun
        dbset
        dgset
        diset
        librarycheck
        loadscript
        print
        quit
        vucomplete
        vucreate
        vudestroy
        vurun
        vuset
        vustatus
</programlisting>

      <para>The commands have the following functionality.</para>

      <para><table>
          <title>CLI commands</title>

          <tgroup cols="3">
            <colspec colwidth="297*"/>

            <colspec colwidth="300*"/>

            <colspec colwidth="403*"/>

            <thead>
              <row>
                <entry align="center">Command</entry>

                <entry align="center">Usage</entry>

                <entry align="center">Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>buildschema</entry>

                <entry>Usage: buildschema</entry>

                <entry>Runs the schema build for the database and benchmark
                selected with dbset and variables selected with diset.
                Equivalent to the Build command in the graphical interface.
                Note that the buildschema command will assume the "yes" reply
                to the Yes or no prompt to proceed with the schema build. This
                is to prevent the stalling of CLI scripts during
                builds.</entry>
              </row>

              <row>
                <entry>clearscript</entry>

                <entry>Usage: clearscript</entry>

                <entry>Clears the script. Equivalent to the "Clear the Screen"
                button in the graphical interface.</entry>
              </row>

              <row>
                <entry>customscript</entry>

                <entry>Usage: customscript scriptname.tcl</entry>

                <entry>Load an external script. Equivalent to the "Open
                Existing File" button in the graphical interface.</entry>
              </row>

              <row>
                <entry>datagenrun</entry>

                <entry>Usage: datagenrun</entry>

                <entry>Run Data Generation. Equivalent to the Generate option
                in the graphical interface.</entry>
              </row>

              <row>
                <entry>dbset</entry>

                <entry>Usage: dbset [db|bm] value</entry>

                <entry>Sets the database (db) or benchmark (bm). Equivalent to
                the Benchmark Menu in the graphical interface. Database value
                is set by the database prefix in the XML
                configuration.</entry>
              </row>

              <row>
                <entry>dgset</entry>

                <entry>Usage: dgset [vu|ware|directory]</entry>

                <entry>Set the Datagen options. Equivalent to the Datagen
                Options dialog in the graphical interface.</entry>
              </row>

              <row>
                <entry>diset</entry>

                <entry>Usage: diset dict key value</entry>

                <entry>Set the dictionary variables for the current database.
                Equivalent to the Schema Build and Driver Options windows in
                the graphical interface. Use "print dict" to see what these
                variables area and diset to change: Example: hammerdb&gt;diset
                tpcc count_ware 10 Changed tpcc:count_ware from 1 to 10 for
                Oracle.</entry>
              </row>

              <row>
                <entry>librarycheck</entry>

                <entry>Usage: librarycheck</entry>

                <entry>Attempts to load the vendor provided 3rd party library
                for all databases and reports whether the attempt was
                successful.</entry>
              </row>

              <row>
                <entry>loadscript</entry>

                <entry>Usage: loadscript</entry>

                <entry>Load the script for the database and benchmark set with
                dbset and the dictionary variables set with diset. Use "print
                script" to see the script that is loaded. Equivalent to
                loading a Driver Script in the Script Editor window in the
                graphical interface.</entry>
              </row>

              <row>
                <entry>print</entry>

                <entry>Usage: print [db|bm|dict|script|vuconf
                |vucreated|vustatus|datagen]</entry>

                <entry>prints the current configuration: db: database bm:
                benchmark dict: the dictionary for the current database ie all
                active variables script: the loaded script vuconf: the virtual
                user configuration vucreated: the number of virtual users
                created vustatus: the status of the virtual users datagen :
                the configuration to build when datagen is run</entry>
              </row>

              <row>
                <entry>quit</entry>

                <entry>Usage: quit Shuts down the HammerDB CLI.</entry>

                <entry>Calls the exit command and terminates the CLI
                interface</entry>
              </row>

              <row>
                <entry>vucomplete</entry>

                <entry>Usage: vucomplete</entry>

                <entry>Returns "true" or "false" depending on whether all
                virtual users that started a workload have completed
                regardless of whether the status was "FINISH SUCCESS" or
                "FINISH FAILED".</entry>
              </row>

              <row>
                <entry>vucreate</entry>

                <entry>Usage: vucreate</entry>

                <entry>Create the virtual users. Equivalent to the Virtual
                User Create option in the graphical interface. Use "print
                vucreated" to see the number created, vustatus to see the
                status and vucomplete to see whether all active virtual users
                have finished the workload. A script must be loaded before
                virtual users can be created.</entry>
              </row>

              <row>
                <entry>vudestroy</entry>

                <entry>Usage: vudestroy</entry>

                <entry>Destroy the virtual users. Equivalent to the Destroy
                Virtual Users button in the graphical interface that replaces
                the Create Virtual Users button after virtual user
                creation.</entry>
              </row>

              <row>
                <entry>vurun</entry>

                <entry>Usage: vurun</entry>

                <entry>Send the loaded script to the created virtual users for
                execution. Equivalent to the Run command in the graphical
                interface.</entry>
              </row>

              <row>
                <entry>vuset</entry>

                <entry>Usage: vuset [vu|delay|repeat|iterations|showoutput
                |logtotemp|unique|nobuff|timestamps</entry>

                <entry>Configure the virtual user options. Equivalent to the
                Virtual User Options window in the graphical
                interface.</entry>
              </row>

              <row>
                <entry>vustatus</entry>

                <entry>Usage: vustatus</entry>

                <entry>Show the status of virtual users. Status will be "WAIT
                IDLE" for virtual users that are created but not running a
                workload,"RUNNING" for virtual users that are running a
                workload, "FINISH SUCCESS" for virtual users that completed
                successfully or "FINISH FAILED" for virtual users that
                encountered an error.</entry>
              </row>
            </tbody>
          </tgroup>
        </table></para>
    </section>

    <section>
      <title>Configure Schema Build</title>

      <para>Use the dbset command to choose a database and benchmark. For the
      database the database prefix shown in the XML configuration is used. IIf
      an incorrect database is selected the available values are
      prompted.</para>

      <programlisting>hammerdb&gt;dbset db orac
Unknown prefix orac, choose one from ora mssqls db2 mysql pg redis</programlisting>

      <para>When a valid option is chosen the database is set.</para>

      <programlisting>hammerdb&gt;dbset db redis
Database set to Redis</programlisting>

      <para>The print command can be used to confirm the chosen database and
      available options.</para>

      <programlisting>hammerdb&gt;print db
Database Redis set.
To change do: dbset db prefix, one of:
Oracle = ora MSSQLServer = mssqls Db2 = db2 MySQL = mysql PostgreSQL = pg Redis = redis </programlisting>

      <para>Similarly the workload is also selected from the available
      configuration also prompting if an incorrect value is chosen.</para>

      <programlisting>hammerdb&gt;dbset bm TPC-H
Unknown benchmark TPC-H, choose one from TPC-C</programlisting>

      <para>when a correct value is chosen the selection is confirmed</para>

      <programlisting>hammerdb&gt;dbset bm TPC-C
Benchmark set to TPC-C for Redis</programlisting>

      <para>The print bm command is used to confirm the benchmark</para>

      <programlisting>hammerdb&gt;print bm
Benchmark set to TPC-C</programlisting>

      <para>After the database and workload is selected the print dict command
      lists all of the available configuration variables for that
      database.</para>

      <para><programlisting>hammerdb&gt;print dict
Dictionary Settings for Redis
connection {
 redis_host      = 127.0.0.1
 redis_port      = 6379
 redis_namespace = 1
}
tpcc       {
 redis_count_ware       = 1
 redis_num_vu           = 1
 redis_total_iterations = 1000000
 redis_raiseerror       = false
 redis_keyandthink      = false
 redis_driver           = test
 redis_rampup           = 2
 redis_duration         = 5
 redis_allwarehouse     = false
 redis_timeprofile      = false
}
</programlisting>Use the diset command to change these values for example for
      the number of warehouses to build.</para>

      <programlisting>hammerdb&gt;diset tpcc redis_count_ware 10
Changed tpcc:redis_count_ware from 1 to 10 for Redis</programlisting>

      <para>and the number of virtual users to build them.</para>

      <programlisting>hammerdb&gt;diset tpcc redis_num_vu 4
Changed tpcc:redis_num_vu from 1 to 4 for Redis</programlisting>

      <para>print dict can confirm the selection.</para>

      <programlisting>hammerdb&gt;print dict
Dictionary Settings for Redis
connection {
 redis_host      = 127.0.0.1
 redis_port      = 6379
 redis_namespace = 1
}
tpcc       {
 redis_count_ware       = 10
 redis_num_vu           = 4
 redis_total_iterations = 1000000
 redis_raiseerror       = false
 redis_keyandthink      = false
 redis_driver           = test
 redis_rampup           = 2
 redis_duration         = 5
 redis_allwarehouse     = false
 redis_timeprofile      = false
}</programlisting>
    </section>

    <section>
      <title>Building the Schema</title>

      <para>Run the buildschema command and the build will commence without
      prompting using your configuration and if successful report the status
      at the end of the build.</para>

      <programlisting>hammerdb&gt;buildschema
Script cleared
Building 10 Warehouses with 5 Virtual Users, 4 active + 1 Monitor VU(dict value redis_num_vu is set to 4)
Ready to create a 10 Warehouse Redis TPC-C schema
in host 127.0.0.1:6379 in namespace 1?
Enter yes or no: replied yes
Vuser 1 created - WAIT IDLE
Vuser 2 created - WAIT IDLE
Vuser 3 created - WAIT IDLE
Vuser 4 created - WAIT IDLE
Vuser 5 created - WAIT IDLE
RUNNING - TPC-C creation
Vuser 1:RUNNING
Vuser 1:Monitor Thread
Vuser 1:CREATING REDIS SCHEMA IN NAMESPACE 1
Vuser 1:Connection made to Redis at 127.0.0.1:6379
Vuser 1:Selecting Namespace 1
Vuser 1:Loading Item
Vuser 2:RUNNING
Vuser 2:Worker Thread
Vuser 2:Waiting for Monitor Thread...
Vuser 2:Connection made to Redis at 127.0.0.1:6379
Vuser 2:Selecting Namespace 1
Vuser 2:Loading 2 Warehouses start:1 end:2
Vuser 2:Start:Mon Apr 09 11:20:43 BST 2018
Vuser 2:Loading Warehouse
Vuser 2:Loading Stock Wid=1
Vuser 3:RUNNING
Vuser 3:Worker Thread
Vuser 3:Waiting for Monitor Thread...
Vuser 3:Connection made to Redis at 127.0.0.1:6379
Vuser 3:Selecting Namespace 1
Vuser 3:Loading 2 Warehouses start:3 end:4
Vuser 3:Start:Mon Apr 09 11:20:44 BST 2018
Vuser 3:Loading Warehouse
Vuser 3:Loading Stock Wid=3
Vuser 4:RUNNING
Vuser 4:Worker Thread
Vuser 4:Waiting for Monitor Thread...
Vuser 4:Connection made to Redis at 127.0.0.1:6379
Vuser 4:Selecting Namespace 1
Vuser 4:Loading 2 Warehouses start:5 end:6
Vuser 4:Start:Mon Apr 09 11:20:44 BST 2018
Vuser 4:Loading Warehouse
Vuser 4:Loading Stock Wid=5
Vuser 5:RUNNING
Vuser 5:Worker Thread
Vuser 5:Waiting for Monitor Thread...
Vuser 5:Connection made to Redis at 127.0.0.1:6379
Vuser 5:Selecting Namespace 1
Vuser 5:Loading 2 Warehouses start:7 end:10
Vuser 5:Start:Mon Apr 09 11:20:45 BST 2018
Vuser 5:Loading Warehouse
Vuser 5:Loading Stock Wid=7

.....

Vuser 5:End:Mon Apr 09 11:27:13 BST 2018
Vuser 5:FINISHED SUCCESS
Vuser 1:Workers: 0 Active 4 Done
Vuser 1:REDIS SCHEMA COMPLETE
Vuser 1:FINISHED SUCCESS
ALL VIRTUAL USERS COMPLETE
</programlisting>

      <para>The vustatus command can confirm the status of each Virtual
      User.</para>

      <programlisting>hammerdb&gt;vustatus
1 = FINISH SUCCESS
2 = FINISH SUCCESS
3 = FINISH SUCCESS
4 = FINISH SUCCESS
5 = FINISH SUCCESS</programlisting>

      <para>When the build is complete destroy the Virtual Users and confirm
      the status.</para>

      <programlisting>hammerdb&gt;vudestroy
Destroying Virtual Users
Virtual Users Destroyed

hammerdb&gt;vustatus
No Virtual Users found</programlisting>
    </section>

    <section>
      <title>Configure Driver</title>

      <para>Set the type of workload to run. A timed workload with suppressed
      output is strongly recommended as a test workload will print
      considerable output to the command prompt.</para>

      <programlisting>hammerdb&gt;diset tpcc redis_driver timed
Clearing Script, reload script to activate new setting
Script cleared
Changed tpcc:reddriver from test to timed for Redis</programlisting>

      <para>Configure workload settings, in this example the rampup and
      duration times are set.</para>

      <programlisting>hammerdb&gt;diset tpcc redis_rampup 1
Changed tpcc:redis_rampup from 2 to 1 for Redis

hammerdb&gt;diset tpcc redis_duration 3
Changed tpcc:redis_duration from 5 to 3 for Redis</programlisting>

      <para>Confirm the settings with the print dict command.</para>

      <programlisting>hammerdb&gt;print dict
Dictionary Settings for Redis
connection {
 redis_host      = 127.0.0.1
 redis_port      = 6379
 redis_namespace = 1
}
tpcc       {
 redis_count_ware       = 10
 redis_num_vu           = 4
 redis_total_iterations = 1000000
 redis_raiseerror       = false
 redis_keyandthink      = false
 redis_driver           = timed
 redis_rampup           = 1
 redis_duration         = 3
 redis_allwarehouse     = false
 redis_timeprofile      = false
}</programlisting>

      <para>When all the settings have been chosen load the driver script with
      the loadscript command.</para>

      <programlisting>hammerdb&gt;loadscript
Script loaded, Type "print script" to view</programlisting>

      <para>The loaded script can be viewed with the print script
      command.</para>

      <programlisting>hammerdb&gt;print script
#!/usr/local/bin/tclsh8.6
#THIS SCRIPT TO BE RUN WITH VIRTUAL USER OUTPUT ENABLED
#EDITABLE OPTIONS##################################################
set library redis ;# Redis Library
set total_iterations 1000000 ;# Number of transactions before logging off
set RAISEERROR "false" ;# Exit script on Redis error (true or false)
set KEYANDTHINK "false" ;# Time for user thinking and keying (true or false)
set rampup 1;  # Rampup time in minutes before first Transaction Count is taken
set duration 3;  # Duration in minutes before second Transaction Count is taken
set mode "Local" ;# HammerDB operational mode
set host "127.0.0.1" ;# Address of the server hosting Redis 
set port "6379" ;# Port of the Redis Server, defaults to 6379
set namespace "1" ;# Namespace containing the TPC Schema
#EDITABLE OPTIONS##################################################

...</programlisting>
    </section>

    <section>
      <title>Configure Virtual Users</title>

      <para>With the schema built and the driver script loaded the next step
      in the workflow is to configure the Virtual Users.</para>

      <para>The print command can be used to show the number of Virtual Users
      currently created. As the Virtual Users were destroyed after the build
      it is reported that none are created.</para>

      <programlisting>hammerdb&gt;print vucreated
0 Virtual Users created</programlisting>

      <para>The vuset command is used to configure the Virtual User options,
      for example the number for create.</para>

      <programlisting>hammerdb&gt;vuset vu 4</programlisting>

      <para>print vuconf confirms the configuration.</para>

      <programlisting>hammerdb&gt;print vuconf
Virtual Users = 4
User Delay(ms) = 500
Repeat Delay(ms) = 500
Iterations = 1
Show Output = 1
Log Output = 0
Unique Log Name = 0
No Log Buffer = 0
Log Timestamps = 0
</programlisting>

      <para>Then run vucreate to create the Virtual Users who will be created
      in an idle state not yet running. Note that when a timed test is
      selected a Monitor Virtual User is also created as is the case with the
      graphical interface.</para>

      <programlisting>hammerdb&gt;vucreate
Vuser 1 created MONITOR - WAIT IDLE
Vuser 2 created - WAIT IDLE
Vuser 3 created - WAIT IDLE
Vuser 4 created - WAIT IDLE
Vuser 5 created - WAIT IDLE
5 Virtual Users Created with Monitor VU</programlisting>

      <para>vustatus can confirm this status.</para>

      <programlisting>hammerdb&gt;vustatus
1 = WAIT IDLE
2 = WAIT IDLE
3 = WAIT IDLE
4 = WAIT IDLE
5 = WAIT IDLE</programlisting>
    </section>

    <section>
      <title>Run the workload</title>

      <para>To begin the workload type vurun.</para>

      <programlisting>hammerdb&gt;vurun
RUNNING - Redis TPC-C
Vuser 1:RUNNING
Vuser 1:Connection made to Redis at 127.0.0.1:6379
Vuser 1:Selecting Namespace 1
Vuser 1:Beginning rampup time of 1 minutes
Vuser 2:RUNNING
Vuser 2:Connection made to Redis at 127.0.0.1:6379
Vuser 2:Selecting Namespace 1
Vuser 2:Processing 1000000 transactions with output suppressed...
Vuser 3:RUNNING
Vuser 3:Connection made to Redis at 127.0.0.1:6379
Vuser 3:Selecting Namespace 1
Vuser 3:Processing 1000000 transactions with output suppressed...
Vuser 4:RUNNING
Vuser 4:Connection made to Redis at 127.0.0.1:6379
Vuser 4:Selecting Namespace 1
Vuser 4:Processing 1000000 transactions with output suppressed...
Vuser 5:RUNNING
Vuser 5:Connection made to Redis at 127.0.0.1:6379
Vuser 5:Selecting Namespace 1
Vuser 5:Processing 1000000 transactions with output suppressed...</programlisting>

      <para>The vustatus command can confirm the change in status.</para>

      <programlisting>hammerdb&gt;vustatus
1 = RUNNING
2 = RUNNING
3 = RUNNING
4 = RUNNING
5 = RUNNING</programlisting>

      <para>The vucomplete command returns a boolean value to confirm whether
      an entire workload is still running or finished.</para>

      <para><programlisting>hammerdb&gt;vucomplete
false</programlisting>The test runs as per the configuration and reports the
      result at the end and the Virtual User status. Note that when complete
      the vucomplete command can confirm this.</para>

      <programlisting>Vuser 1:Rampup 1 minutes complete ...
Vuser 1:Rampup complete, Taking start Transaction Count.
Vuser 1:Timing test period of 3 in minutes
Vuser 1:1 ...,
Vuser 1:2 ...,
Vuser 1:3 ...,
Vuser 1:Test complete, Taking end Transaction Count.
Vuser 1:4 Active Virtual Users configured
Vuser 1:TEST RESULT : System achieved 2887487 Redis TPM at 24374 NOPM
Vuser 1:FINISHED SUCCESS
Vuser 5:FINISHED SUCCESS
Vuser 4:FINISHED SUCCESS
Vuser 3:FINISHED SUCCESS
Vuser 2:FINISHED SUCCESS
ALL VIRTUAL USERS COMPLETE
hammerdb&gt;vucomplete
true</programlisting>

      <para>To complete the test type vudestroy.</para>

      <programlisting>hammerdb&gt;vudestroy
Destroying Virtual Users
Virtual Users Destroyed
</programlisting>

      <para>and clear the script.</para>

      <programlisting>hammerdb&gt;clearscript
Script cleared</programlisting>
    </section>

    <section>
      <title>CLI Scripting</title>

      <para>The CLI enables a powerful automated test environment through
      scripting in the TCL language. A recommended updated guide to TCL is
      "The Tcl Programming Language: A Comprehensive Guide by Ashok P.
      Nadkarni (ISBN: 9781548679644)"</para>

      <para>The following example shows an automated test script for a Redis
      database that has previously been created. In this example the script
      runs a timed tests for a duration of a minute for 1, 2 and 4 Virtual
      Users in a similar manner to autopilot functionality with a timer set to
      run for 2 minutes. Note that in the timer the update command is included
      to process events received from the Virtual Users during the test.
      Similarly the functionality of the vucomplete command can be observed.
      When called as a command ie [ vucomplete ] and returning a boolean value
      this command can be used in the timing loop to observe when the Virtual
      Users have completed and once notified stop the timer and proceed with
      the next test in the sequence.</para>

      <programlisting>#!/usr/bin/tclsh
proc runtimer { seconds } {
set x 0
set timerstop 0  
while {!$timerstop} {       
 incr x
 after 1000
  if { ![ expr {$x % 60} ] } {
  set y [ expr $x / 60 ]
  puts "Timer: $y minutes elapsed"
  }
 update
 if {  [ vucomplete ] || $x eq $seconds } { set timerstop 1 }
    }
return
}
puts "SETTING CONFIGURATION"
dbset db redis
diset tpcc redis_driver timed
diset tpcc redis_rampup 0
diset tpcc redis_duration 1
vuset logtotemp 1
loadscript
puts "SEQUENCE STARTED"
foreach z { 1 2 4 } {
puts "$z VU TEST"
vuset vu $z
vucreate
vurun
runtimer 120
vudestroy
after 5000
}
puts "TEST SEQUENCE COMPLETE"</programlisting>

      <para>Run the hammerdbcli command and at the prompt type source and the
      name of the script. The following output is produced without further
      intervention whilst also writing the output to the logfile.</para>

      <programlisting>./hammerdbcli 
HammerDB CLI v3.0
Copyright (C) 2003-2018 Steve Shaw
Type "help" for a list of commands
The xml is well-formed, applying configuration
hammerdb&gt;source cliexample.tcl
SETTING CONFIGURATION
Database set to Redis
Clearing Script, reload script to activate new setting
Script cleared
Changed tpcc:redis_driver from test to timed for Redis
Changed tpcc:redis_rampup from 2 to 0 for Redis
Changed tpcc:redis_duration from 5 to 1 for Redis
Script loaded, Type "print script" to view
SEQUENCE STARTED
1 VU TEST
Vuser 1 created MONITOR - WAIT IDLE
Vuser 2 created - WAIT IDLE
Logging activated
to /tmp/hammerdb.log
2 Virtual Users Created with Monitor VU
RUNNING - Redis TPC-C
Vuser 1:RUNNING
Vuser 1:Connection made to Redis at 127.0.0.1:6379
Vuser 1:Selecting Namespace 1
Vuser 1:Beginning rampup time of 0 minutes
Vuser 1:Rampup complete, Taking start Transaction Count.
Vuser 1:Timing test period of 1 in minutes
Vuser 2:RUNNING
Vuser 2:Connection made to Redis at 127.0.0.1:6379
Vuser 2:Selecting Namespace 1
Vuser 2:Processing 1000000 transactions with output suppressed...
Vuser 1:1 ...,
          Timer: 1 minutes elapsed
Vuser 1:Test complete, Taking end Transaction Count.
Vuser 1:1 Active Virtual Users configured
Vuser 1:TEST RESULT : System achieved 1564723 Redis TPM at 13100 NOPM
Vuser 1:FINISHED SUCCESS
Vuser 2:FINISHED SUCCESS
ALL VIRTUAL USERS COMPLETE
Destroying Virtual Users
Virtual Users Destroyed
2 VU TEST
Vuser 1 created MONITOR - WAIT IDLE
Vuser 2 created - WAIT IDLE
Vuser 3 created - WAIT IDLE
Logging activated
to /tmp/hammerdb.log
3 Virtual Users Created with Monitor VU
RUNNING - Redis TPC-C
Vuser 1:RUNNING
Vuser 1:Connection made to Redis at 127.0.0.1:6379
Vuser 1:Selecting Namespace 1
Vuser 1:Beginning rampup time of 0 minutes
Vuser 1:Rampup complete, Taking start Transaction Count.
Vuser 1:Timing test period of 1 in minutes
Vuser 2:RUNNING
Vuser 2:Connection made to Redis at 127.0.0.1:6379
Vuser 2:Selecting Namespace 1
Vuser 2:Processing 1000000 transactions with output suppressed...
Vuser 3:RUNNING
Vuser 3:Connection made to Redis at 127.0.0.1:6379
Vuser 3:Selecting Namespace 1
Vuser 3:Processing 1000000 transactions with output suppressed...
Vuser 1:1 ...,
          Timer: 1 minutes elapsed
Vuser 1:Test complete, Taking end Transaction Count.
Vuser 1:2 Active Virtual Users configured
Vuser 1:TEST RESULT : System achieved 2266472 Redis TPM at 19068 NOPM
Vuser 1:FINISHED SUCCESS
Vuser 3:FINISHED SUCCESS
Vuser 2:FINISHED SUCCESS
ALL VIRTUAL USERS COMPLETE
Destroying Virtual Users
Virtual Users Destroyed
4 VU TEST
Vuser 1 created MONITOR - WAIT IDLE
Vuser 2 created - WAIT IDLE
Vuser 3 created - WAIT IDLE
Vuser 4 created - WAIT IDLE
Vuser 5 created - WAIT IDLE
Logging activated
to /tmp/hammerdb.log
5 Virtual Users Created with Monitor VU
RUNNING - Redis TPC-C
Vuser 1:RUNNING
Vuser 1:Connection made to Redis at 127.0.0.1:6379
Vuser 1:Selecting Namespace 1
Vuser 1:Beginning rampup time of 0 minutes
Vuser 1:Rampup complete, Taking start Transaction Count.
Vuser 1:Timing test period of 1 in minutes
Vuser 2:RUNNING
Vuser 2:Connection made to Redis at 127.0.0.1:6379
Vuser 2:Selecting Namespace 1
Vuser 2:Processing 1000000 transactions with output suppressed...
Vuser 3:RUNNING
Vuser 3:Connection made to Redis at 127.0.0.1:6379
Vuser 3:Selecting Namespace 1
Vuser 3:Processing 1000000 transactions with output suppressed...
Vuser 4:RUNNING
Vuser 4:Connection made to Redis at 127.0.0.1:6379
Vuser 4:Selecting Namespace 1
Vuser 4:Processing 1000000 transactions with output suppressed...
Vuser 5:RUNNING
Vuser 5:Connection made to Redis at 127.0.0.1:6379
Vuser 5:Selecting Namespace 1
Vuser 5:Processing 1000000 transactions with output suppressed...
Vuser 1:1 ...,
Vuser 1:Test complete, Taking end Transaction Count.
          Timer: 1 minutes elapsed
Vuser 1:4 Active Virtual Users configured
Vuser 1:TEST RESULT : System achieved 2648261 Redis TPM at 22397 NOPM
Vuser 1:FINISHED SUCCESS
Vuser 4:FINISHED SUCCESS
Vuser 3:FINISHED SUCCESS
Vuser 5:FINISHED SUCCESS
Vuser 2:FINISHED SUCCESS
ALL VIRTUAL USERS COMPLETE
Destroying Virtual Users
Virtual Users Destroyed
TEST SEQUENCE COMPLETE

hammerdb&gt;</programlisting>
    </section>
  </chapter>

  <chapter>
    <title>Web Service Interface (WS)</title>

    <para>In addition to the CLI there is a HTTP Web Service that provides the
    same commands as the CLI that can be accessed with a HTTP/REST client
    passing parameters and returning output in JSON format. The key difference
    from the configuration of the CLI is the addition of jobs. Under the web
    service output from schema builds or tests are stored in a SQLite database
    and retrieved at a later point using a job id. A rest interface has been
    provided in HammerDB for interacting with the web service using TCL,
    however this is not a necessity and although the examples in this section
    are given using TCL the web service can be driven with scripts written in
    any language. Additionally the huddle package has been provided for TCL to
    JSON formatting.</para>

    <section>
      <title>Web Service Configuration</title>

      <para>There are 2 configuration parameters for the webservice in the
      file generic.xml in the config directory, ws_port and sqlite_db. ws_port
      defines the port on which the service will run and sqlite_db defines the
      location of the SQLite database file. By default an in-memory location
      is used. Alternatively the name of a file can be given or "TMP" or
      "TEMP" for HammerDB to find a temporary directory to use for the
      file.</para>

      <programlisting>  &lt;webservice&gt;
   &lt;ws_port&gt;8080&lt;/ws_port&gt; 
   &lt;sqlite_db&gt;:memory:&lt;/sqlite_db&gt; 
  &lt;/webservice&gt;
</programlisting>
    </section>

    <section>
      <title>Starting the Web Service and Help Screen</title>

      <para>On starting the Web service with the hammerdbws command HammerDB
      will listen on the specified port for HTTP requests.</para>

      <programlisting>[oracle@vulture HammerDB-3.2]$ ./hammerdbws 
HammerDB Web Service v3.2
Copyright (C) 2003-2019 Steve Shaw
Type "help" for a list of commands
The xml is well-formed, applying configuration
Initialized new SQLite in-memory database
Starting HammerDB Web Service on port 8080
Listening for HTTP requests on TCP port 8080
</programlisting>

      <para>Navigating to the configured port without further argument will
      return the help screen.</para>

      <programlisting>HammerDB Web Service

See the HammerDB Web Service Environment
HAMMERDB REST/HTTP API

GET db: Show the configured database.
get http://localhost:8080/print?db / get http://localhost:8080/db
{
"ora": "Oracle",
"mssqls": "MSSQLServer",
"db2": "Db2",
"mysql": "MySQL",
"pg": "PostgreSQL",
"redis": "Redis"
}


GET bm: Show the configured benchmark.
get http://localhost:8080/print?bm / get http://localhost:8080/bm
{"benchmark": "TPC-C"}


GET dict: Show the dictionary for the current database ie all active variables.
get http://localhost:8080/print?dict /  http://localhost:8080/dict
{
"connection": {
"system_user": "system",
"system_password": "manager",
"instance": "oracle",
"rac": "0"
},
"tpcc": {
"count_ware": "1",
"num_vu": "1",
"tpcc_user": "tpcc",
"tpcc_pass": "tpcc",
"tpcc_def_tab": "tpcctab",
"tpcc_ol_tab": "tpcctab",
"tpcc_def_temp": "temp",
"partition": "false",
"hash_clusters": "false",
"tpcc_tt_compat": "false",
"total_iterations": "1000000",
"raiseerror": "false",
"keyandthink": "false",
"checkpoint": "false",
"ora_driver": "test",
"rampup": "2",
"duration": "5",
"allwarehouse": "false",
"timeprofile": "false"
}
}

 
GET script: Show the loaded script.
get http://localhost:8080/print?script / http://localhost:8080/script
{"script": "#!/usr/local/bin/tclsh8.6
#TIMED AWR SNAPSHOT DRIVER SCRIPT##################################
#THIS SCRIPT TO BE RUN WITH VIRTUAL USER OUTPUT ENABLED
#EDITABLE OPTIONS##################################################
set library Oratcl ;# Oracle OCI Library
set total_iterations 1000000 ;# Number of transactions before logging off
set RAISEERROR \"false\" ;# Exit script on Oracle error (true or false)
set KEYANDTHINK \"false\" ;# Time for user thinking and keying (true or false)
set CHECKPOINT \"false\" ;# Perform Oracle checkpoint when complete (true or false)
set rampup 2;  # Rampup time in minutes before first snapshot is taken
set duration 5;  # Duration in minutes before second AWR snapshot is taken
set mode \"Local\" ;# HammerDB operational mode
set timesten \"false\" ;# Database is TimesTen
set systemconnect system/manager@oracle ;# Oracle connect string for system user
set connect tpcc/new_password@oracle ;# Oracle connect string for tpc-c user
#EDITABLE OPTIONS##################################################
#LOAD LIBRARIES AND MODULES …. 
"}

 
GET vuconf: Show the virtual user configuration.
get http://localhost:8080/print?vuconf / http://localhost:8080/vuconf
{
"Virtual Users": "1",
"User Delay(ms)": "500",
"Repeat Delay(ms)": "500",
"Iterations": "1",
"Show Output": "1",
"Log Output": "0",
"Unique Log Name": "0",
"No Log Buffer": "0",
"Log Timestamps": "0"
}

 
GET vucreate: Create the virtual users. Equivalent to the Virtual User Create option in the graphical interface. Use vucreated to see the number created, vustatus to see the status and vucomplete to see whether all active virtual users have finished the workload. A script must be loaded before virtual users can be created.
get http://localhost:8080/vucreate
{"success": {"message": "4 Virtual Users Created"}}

 
GET vucreated: Show the number of virtual users created.
get http://localhost:8080/print?vucreated / get http://localhost:8080/vucreated
{"Virtual Users created": "10"}

 
GET vustatus: Show the status of virtual users, status will be "WAIT IDLE" for virtual users that are created but not running a workload,"RUNNING" for virtual users that are running a workload, "FINISH SUCCESS" for virtual users that completed successfully or "FINISH FAILED" for virtual users that encountered an error.
get http://localhost:8080/print?vustatus / get http://localhost:8080/vustatus
{"Virtual User status": "1 {WAIT IDLE} 2 {WAIT IDLE} 3 {WAIT IDLE} 4 {WAIT IDLE} 5 {WAIT IDLE} 6 {WAIT IDLE} 7 {WAIT IDLE} 8 {WAIT IDLE} 9 {WAIT IDLE} 10 {WAIT IDLE}"}

 
GET datagen: Show the datagen configuration
get http://localhost:8080/print?datagen /  get http://localhost:8080/datagen
{
"schema": "TPC-C",
"database": "Oracle",
"warehouses": "1",
"vu": "1",
"directory": "/tmp\""
}

 
GET vucomplete: Show if virtual users have completed. returns "true" or "false" depending on whether all virtual users that started a workload have completed regardless of whether the status was "FINISH SUCCESS" or "FINISH FAILED".
get http://localhost:8080/vucomplete
{"Virtual Users complete": "true"}

 
GET vudestroy: Destroy the virtual users. Equivalent to the Destroy Virtual Users button in the graphical interface that replaces the Create Virtual Users button after virtual user creation.
get http://localhost:8080/vudestroy
{"success": {"message": "vudestroy success"}}

 
GET loadscript: Load the script for the database and benchmark set with dbset and the dictionary variables set with diset. Use print?script to see the script that is loaded. Equivalent to loading a Driver Script in the Script Editor window in the graphical interface. Driver script must be set to timed for the script to be loaded. Test scripts should be run in the GUI environment.  
get http://localhost:8080/loadscript
{"success": {"message": "script loaded"}}

 
GET clearscript: Clears the script. Equivalent to the "Clear the Screen" button in the graphical interface.
get http://localhost:8080/clearscript
{"success": {"message": "Script cleared"}}

 
GET vurun: Send the loaded script to the created virtual users for execution. Equivalent to the Run command in the graphical interface. Creates a job id associated with all output. 
get http://localhost:8080/vurun
{"success": {"message": "Running Virtual Users: JOBID=5CEFBFE658A103E253238363"}}


GET datagenrun: Run Data Generation. Equivalent to the Generate option in the graphical interface. Not supported in web service. Generate data using GUI or CLI. 


GET buildschema: Runs the schema build for the database and benchmark selected with dbset and variables selected with diset. Equivalent to the Build command in the graphical interface. Creates a job id associated with all output. 
get http://localhost:8080/buildschema
{"success": {"message": "Building 6 Warehouses with 4 Virtual Users, 3 active + 1 Monitor VU(dict value num_vu is set to 3): JOBID=5CEFA68458A103E273433333"}}


GET jobs: Show the job ids, output, status and results of jobs created by buildschema and vurun. Job output is equivalent to the output viewed in the graphical interface or command line.
GET http://localhost:8080/jobs: Show all job ids
get http://localhost:8080/jobs
[
"5CEE889958A003E203838313",
"5CEFA68458A103E273433333"
]
GET http://localhost:8080/jobs?jobid=TEXT: Show output for the specified job id.
get http://localhost:8080/jobs?jobid=5CEFA68458A103E273433333
[
"0",
"Ready to create a 6 Warehouse Oracle TPC-C schema
in database VULPDB1 under user TPCC in tablespace TPCCTAB?",
"0",
"Vuser 1:RUNNING",
"1",
"Monitor Thread",
"1",
"CREATING TPCC SCHEMA",
...
"1",
"TPCC SCHEMA COMPLETE",
"0",
"Vuser 1:FINISHED SUCCESS",
"0",
"ALL VIRTUAL USERS COMPLETE"
]
GET http://localhost:8080/jobs?jobid=TEXT&amp;vu=INTEGER: Show output for the specified job id and virtual user.
get http://localhost:8080/jobs?jobid=5CEFA68458A103E273433333&amp;vu=1
[
"1",
"Monitor Thread",
"1",
"CREATING TPCC SCHEMA",
"1",
"CREATING USER tpcc",
"1",
"CREATING TPCC TABLES",
"1",
"Loading Item",
"1",
"Loading Items - 50000",
"1",
"Loading Items - 100000",
"1",
"Item done",
"1",
"Monitoring Workers...",
"1",
"Workers: 3 Active 0 Done"
]
GET http://localhost:8080/jobs?jobid=TEXT&amp;status: Show status for the specified job id. Equivalent to virtual user 0.
get http://localhost:8080/jobs?jobid=5CEFA68458A103E273433333&amp;status
[
"0",
"Ready to create a 6 Warehouse Oracle TPC-C schema
in database VULPDB1 under user TPCC in tablespace TPCCTAB?",
"0",
"Vuser 1:RUNNING",
"0",
"Vuser 2:RUNNING",
"0",
"Vuser 3:RUNNING",
"0",
"Vuser 4:RUNNING",
"0",
"Vuser 4:FINISHED SUCCESS",
"0",
"Vuser 3:FINISHED SUCCESS",
"0",
"Vuser 2:FINISHED SUCCESS",
"0",
"Vuser 1:FINISHED SUCCESS",
"0",
"ALL VIRTUAL USERS COMPLETE"
]
GET http://localhost:8080/jobs?jobid=TEXT&amp;result: Show the test result for the specified job id. If job is not a test job such as build job then no result will be reported. 
get http://localhost:8080/jobs?jobid=5CEFA68458A103E273433333&amp;result
[
"5CEFA68458A103E273433333",
"Jobid has no test result"
]
GET http://localhost:8080/jobs?jobid=TEXT&amp;delete: Delete all output for the specified jobid.
get http://localhost:8080/jobs?jobid=5CEFA68458A103E273433333&amp;delete
{"success": {"message": "Deleted Jobid 5CEFA68458A103E273433333"}} 


GET killws: Terminates the webservice and reports message to the console.
get http://localhost:8080/killws
Shutting down HammerDB Web Service


POST dbset: Usage: dbset [db|bm] value. Sets the database (db) or benchmark (bm). Equivalent to the Benchmark Menu in the graphical interface. Database value is set by the database prefix in the XML configuration.
set body { "db": "ora" }
rest::post http://localhost:8080/dbset $body


POST diset: Usage: diset dict key value. Set the dictionary variables for the current database. Equivalent to the Schema Build and Driver Options windows in the graphical interface. Use print?dict to see what these variables are and diset to change.
set body { "dict": "tpcc", "key": "rampup", "value": "0" }
rest::post http://localhost:8080/diset $body
set body { "dict": "tpcc", "key": "duration", "value": "1" }
rest::post http://localhost:8080/diset $body


POST vuset: Usage: vuset [vu|delay|repeat|iterations|showoutput|logtotemp|unique|nobuff|timestamps]. Configure the virtual user options. Equivalent to the Virtual User Options window in the graphical interface.
set body { "vu": "4" }
rest::post http://localhost:8080/vuset $body


POST customscript: Load an external script. Equivalent to the "Open Existing File" button in the graphical interface. Script must be converted to JSON format before post as shown in the example:
set customscript "testscript.tcl"
set _ED(file) $customscript
if {$_ED(file) == ""} {return}
if {![file readable $_ED(file)]} {
puts "File [$_ED(file)] is not readable."
return
}
if {[catch "open \"$_ED(file)\" r" fd]} {
puts "Error while opening $_ED(file): [$fd]"
} else {
set _ED(package) "[read $fd]"
close $fd
}
set huddleobj [ huddle compile {string} "$_ED(package)" ]
set jsonobj [ huddle jsondump $huddleobj ]
set body [ subst { {"script": $jsonobj}} ]
set res [ rest::post http://localhost:8080/customscript $body ] 


POST dgset: Usage: dgset [vu|ware|directory]. Set the Datagen options. Equivalent to the Datagen Options dialog in the graphical interface.
set body { "directory": "/home/oracle" }
rest::post http://localhost:8080/dgset $body 


DEBUG
GET dumpdb: Dumps output of the SQLite database to the console.
GET http://localhost:8080/dumpdb
***************DEBUG***************
5CEE889958A003E203838313 0 {Ready to create a 6 Warehouse Oracle TPC-C schema
in database VULPDB1 under user TPCC in tablespace TPCCTAB?} 5CEE889958A003E203838313 0 {Vuser 1:RUNNING} 5CEE889958A003E203838313 1 {Monitor Thread} 5CEE889958A003E203838313 1 {CREATING TPCC SCHEMA} 5CEE889958A003E203838313 0 {Vuser 2:RUNNING} 5CEE889958A003E203838313 2 {Worker Thread} 5CEE889958A003E203838313 2 {Waiting for Monitor Thread...} 5CEE889958A003E203838313 1 {Error: ORA-12541: TNS:no listener} 5CEE889958A003E203838313 0 {Vuser 1:FINISHED FAILED} 5CEE889958A003E203838313 0 {Vuser 3:RUNNING} 5CEE889958A003E203838313 3 {Worker Thread} 5CEE889958A003E203838313 3 {Waiting for Monitor Thread...} 5CEE889958A003E203838313 0 {Vuser 4:RUNNING} 5CEE889958A003E203838313 4 {Worker Thread} 5CEE889958A003E203838313 4 {Waiting for Monitor Thread...} 5CEE889958A003E203838313 2 {Monitor failed to notify ready state} 5CEE889958A003E203838313 0 {Vuser 2:FINISHED SUCCESS} 5CEE889958A003E203838313 3 {Monitor failed to notify ready state} 5CEE889958A003E203838313 0 {Vuser 3:FINISHED SUCCESS} 5CEE889958A003E203838313 4 {Monitor failed to notify ready state} 5CEE889958A003E203838313 0 {Vuser 4:FINISHED SUCCESS} 5CEE889958A003E203838313 0 {ALL VIRTUAL USERS COMPLETE}
***************DEBUG***************

</programlisting>

      <para>As an example the following script shows printing the output of
      print commands in both JSON and text format.</para>

      <programlisting>set UserDefaultDir [ file dirname [ info script ] ]
::tcl::tm::path add "$UserDefaultDir/modules"
package require rest
package require huddle
puts "TEST DIRECT PRINT COMMANDS"
puts "--------------------------------------------------------"
foreach i {db bm dict script vuconf vucreated vustatus datagen}  {
puts "Printing output for $i and converting JSON to text"
    set res [rest::get http://localhost:8080/$i "" ]
puts "JSON format"
puts $res
puts "TEXT format"
    set res [rest::format_json $res]
    puts $res
}
puts "--------------------------------------------------------"
puts "PRINT COMMANDS COMPLETE"
puts "--------------------------------------------------------"
</programlisting>

      <para>Once the Web Service is running in another port, run the TCL shell
      as follows and run the script above, the output is shown as
      follows.</para>

      <programlisting>$ ./bin/tclsh8.6 
% source restchk.tcl
TEST DIRECT PRINT COMMANDS
--------------------------------------------------------
Printing output for db and converting JSON to text
JSON format
{
  "ora": "Oracle",
  "mssqls": "MSSQLServer",
  "db2": "Db2",
  "mysql": "MySQL",
  "pg": "PostgreSQL",
  "redis": "Redis"
}
TEXT format
ora Oracle mssqls MSSQLServer db2 Db2 mysql MySQL pg PostgreSQL redis Redis
Printing output for bm and converting JSON to text
JSON format
{"benchmark": "TPC-C"}
TEXT format
benchmark TPC-C
Printing output for dict and converting JSON to text
JSON format
{
  "connection": {
    "system_user": "system",
    "system_password": "manager",
    "instance": "oracle",
    "rac": "0"
  },
  "tpcc": {
    "count_ware": "1",
    "num_vu": "1",
    "tpcc_user": "tpcc",
    "tpcc_pass": "tpcc",
    "tpcc_def_tab": "tpcctab",
    "tpcc_ol_tab": "tpcctab",
    "tpcc_def_temp": "temp",
    "partition": "false",
    "hash_clusters": "false",
    "tpcc_tt_compat": "false",
    "total_iterations": "1000000",
    "raiseerror": "false",
    "keyandthink": "false",
    "checkpoint": "false",
    "ora_driver": "test",
    "rampup": "2",
    "duration": "5",
    "allwarehouse": "false",
    "timeprofile": "false"
  }
}
TEXT format
connection {system_user system system_password manager instance oracle rac 0} tpcc {count_ware 1 num_vu 1 tpcc_user tpcc tpcc_pass tpcc tpcc_def_tab tpcctab tpcc_ol_tab tpcctab tpcc_def_temp temp partition false hash_clusters false tpcc_tt_compat false total_iterations 1000000 raiseerror false keyandthink false checkpoint false ora_driver test rampup 2 duration 5 allwarehouse false timeprofile false}
Printing output for script and converting JSON to text
JSON format
{"error": {"message": "No Script loaded: load with loadscript"}}
TEXT format
error {message {No Script loaded: load with loadscript}}
Printing output for vuconf and converting JSON to text
JSON format
{
  "Virtual Users": "1",
  "User Delay(ms)": "500",
  "Repeat Delay(ms)": "500",
  "Iterations": "1",
  "Show Output": "1",
  "Log Output": "0",
  "Unique Log Name": "0",
  "No Log Buffer": "0",
  "Log Timestamps": "0"
}
TEXT format
{Virtual Users} 1 {User Delay(ms)} 500 {Repeat Delay(ms)} 500 Iterations 1 {Show Output} 1 {Log Output} 0 {Unique Log Name} 0 {No Log Buffer} 0 {Log Timestamps} 0
Printing output for vucreated and converting JSON to text
JSON format
{"Virtual Users created": "0"}
TEXT format
{Virtual Users created} 0
Printing output for vustatus and converting JSON to text
JSON format
{"Virtual User status": "No Virtual Users found"}
TEXT format
{Virtual User status} {No Virtual Users found}
Printing output for datagen and converting JSON to text
JSON format
{
  "schema": "TPC-C",
  "database": "Oracle",
  "warehouses": "1",
  "vu": "1",
  "directory": "\/tmp\""
}
TEXT format
schema TPC-C database Oracle warehouses 1 vu 1 directory /tmp\"
--------------------------------------------------------
PRINT COMMANDS COMPLETE
--------------------------------------------------------
% </programlisting>
    </section>

    <section>
      <title>Retrieving Output</title>

      <para>As an example the following script shows printing the output of
      print commands in both JSON and text format.</para>

      <programlisting>set UserDefaultDir [ file dirname [ info script ] ]
::tcl::tm::path add "$UserDefaultDir/modules"
package require rest
package require huddle
puts "TEST DIRECT PRINT COMMANDS"
puts "--------------------------------------------------------"
foreach i {db bm dict script vuconf vucreated vustatus datagen}  {
puts "Printing output for $i and converting JSON to text"
    set res [rest::get http://localhost:8080/$i "" ]
puts "JSON format"
puts $res
puts "TEXT format"
    set res [rest::format_json $res]
    puts $res
}
puts "--------------------------------------------------------"
puts "PRINT COMMANDS COMPLETE"
puts "--------------------------------------------------------"
</programlisting>

      <para>Once the Web Service is running in another port, run the TCL shell
      as follows and run the script above, the output is shown as
      follows.</para>

      <programlisting>$ ./bin/tclsh8.6 
% source restchk.tcl
TEST DIRECT PRINT COMMANDS
--------------------------------------------------------
Printing output for db and converting JSON to text
JSON format
{
  "ora": "Oracle",
  "mssqls": "MSSQLServer",
  "db2": "Db2",
  "mysql": "MySQL",
  "pg": "PostgreSQL",
  "redis": "Redis"
}
TEXT format
ora Oracle mssqls MSSQLServer db2 Db2 mysql MySQL pg PostgreSQL redis Redis
Printing output for bm and converting JSON to text
JSON format
{"benchmark": "TPC-C"}
TEXT format
benchmark TPC-C
Printing output for dict and converting JSON to text
JSON format
{
  "connection": {
    "system_user": "system",
    "system_password": "manager",
    "instance": "oracle",
    "rac": "0"
  },
  "tpcc": {
    "count_ware": "1",
    "num_vu": "1",
    "tpcc_user": "tpcc",
    "tpcc_pass": "tpcc",
    "tpcc_def_tab": "tpcctab",
    "tpcc_ol_tab": "tpcctab",
    "tpcc_def_temp": "temp",
    "partition": "false",
    "hash_clusters": "false",
    "tpcc_tt_compat": "false",
    "total_iterations": "1000000",
    "raiseerror": "false",
    "keyandthink": "false",
    "checkpoint": "false",
    "ora_driver": "test",
    "rampup": "2",
    "duration": "5",
    "allwarehouse": "false",
    "timeprofile": "false"
  }
}
TEXT format
connection {system_user system system_password manager instance oracle rac 0} tpcc {count_ware 1 num_vu 1 tpcc_user tpcc tpcc_pass tpcc tpcc_def_tab tpcctab tpcc_ol_tab tpcctab tpcc_def_temp temp partition false hash_clusters false tpcc_tt_compat false total_iterations 1000000 raiseerror false keyandthink false checkpoint false ora_driver test rampup 2 duration 5 allwarehouse false timeprofile false}
Printing output for script and converting JSON to text
JSON format
{"error": {"message": "No Script loaded: load with loadscript"}}
TEXT format
error {message {No Script loaded: load with loadscript}}
Printing output for vuconf and converting JSON to text
JSON format
{
  "Virtual Users": "1",
  "User Delay(ms)": "500",
  "Repeat Delay(ms)": "500",
  "Iterations": "1",
  "Show Output": "1",
  "Log Output": "0",
  "Unique Log Name": "0",
  "No Log Buffer": "0",
  "Log Timestamps": "0"
}
TEXT format
{Virtual Users} 1 {User Delay(ms)} 500 {Repeat Delay(ms)} 500 Iterations 1 {Show Output} 1 {Log Output} 0 {Unique Log Name} 0 {No Log Buffer} 0 {Log Timestamps} 0
Printing output for vucreated and converting JSON to text
JSON format
{"Virtual Users created": "0"}
TEXT format
{Virtual Users created} 0
Printing output for vustatus and converting JSON to text
JSON format
{"Virtual User status": "No Virtual Users found"}
TEXT format
{Virtual User status} {No Virtual Users found}
Printing output for datagen and converting JSON to text
JSON format
{
  "schema": "TPC-C",
  "database": "Oracle",
  "warehouses": "1",
  "vu": "1",
  "directory": "\/tmp\""
}
TEXT format
schema TPC-C database Oracle warehouses 1 vu 1 directory /tmp\"
--------------------------------------------------------
PRINT COMMANDS COMPLETE
--------------------------------------------------------
% </programlisting>
    </section>

    <section>
      <title>Running Jobs</title>

      <para>The following script run in the same shows how this can be
      extended so that an external script can interact with the web service
      and run a build and then a test successively. Note that wait_to_complete
      procedures can properly sleep using the after command without activity
      and without affecting the progress of the jobs as the driving script is
      run in one interpreter and the web service in another.</para>

      <programlisting>set UserDefaultDir [ file dirname [ info script ] ]
::tcl::tm::path add "$UserDefaultDir/modules"
package require rest
package require huddle

proc wait_for_run_to_complete { runjob } {
global complete
set res [rest::get http://localhost:8080/vucomplete "" ]
set complete [ lindex [rest::format_json $res] 1]
if {!$complete} {
#sleep for 20 seconds and recheck
after 20000 
wait_for_run_to_complete $runjob
	} else {
set res [rest::get http://localhost:8080/vudestroy "" ]
puts "Test Complete"
set jobid [ lindex [ split [ lindex [ lindex [ lindex [rest::format_json $runjob ] 1 ] 1 ] 3 ] \= ] 1 ]
set res [rest::get http://localhost:8080/jobs?jobid=$jobid&amp;result "" ]
puts "Test result: $res"
  }
}

proc wait_for_build_to_complete {} {
global complete
set res [rest::get http://localhost:8080/vucomplete "" ]
set complete [ lindex [rest::format_json $res] 1]
if {!$complete} {
#sleep for 20 seconds and recheck
after 20000 
wait_for_build_to_complete 
	} else {
set res [rest::get http://localhost:8080/vudestroy "" ]
puts "Build Complete"
set complete false
  }
}

proc run_test {} {
puts "Setting Db values"
set body { "db": "ora" }
    set res [ rest::post http://localhost:8080/dbset $body ] 
set body { "bm": "TPC-C" }
    set res [ rest::post http://localhost:8080/dbset $body ] 
puts "Setting Vusers"
set body { "vu": "5" }
    set res [ rest::post http://localhost:8080/vuset $body ] 
puts $res
puts "Setting Dict Values"
set body { "dict": "connection", "key": "system_password", "value": "oracle" }
    set res [rest::post http://localhost:8080/diset $body ]
set body { "dict": "connection", "key": "instance", "value": "VULPDB1" }
    set res [rest::post http://localhost:8080/diset $body ]
set body { "dict": "tpcc", "key": "tpcc_pass", "value": "oracle" }
    set res [rest::post http://localhost:8080/diset $body ]
set body { "dict": "tpcc", "key": "ora_driver", "value": "timed" }
    set res [rest::post http://localhost:8080/diset $body ]
set body { "dict": "tpcc", "key": "rampup", "value": "1" }
    set res [rest::post http://localhost:8080/diset $body ]
set body { "dict": "tpcc", "key": "duration", "value": "2" }
    set res [rest::post http://localhost:8080/diset $body ]
set body { "dict": "tpcc", "key": "checkpoint", "value": "false" }
    set res [rest::post http://localhost:8080/diset $body ]
puts "Config"
set res [rest::get http://localhost:8080/dict "" ]
puts $res
puts "Clearscript"
    set res [rest::post http://localhost:8080/clearscript "" ]
puts $res
puts "Loadscript"
    set res [rest::post http://localhost:8080/loadscript "" ]
puts $res
puts "Create VU"
 set res [rest::post http://localhost:8080/vucreate "" ]
puts $res
puts "Run VU"
 set res [rest::post http://localhost:8080/vurun "" ]
puts $res
wait_for_run_to_complete $res
}

proc run_build {} {
puts "running build"
set body { "db": "ora" }
    set res [ rest::post http://localhost:8080/dbset $body ] 
set body { "bm": "TPC-C" }
    set res [ rest::post http://localhost:8080/dbset $body ] 
puts "Setting Dict Values"
set body { "dict": "connection", "key": "system_password", "value": "oracle" }
    set res [rest::post http://localhost:8080/diset $body ]
set body { "dict": "connection", "key": "instance", "value": "VULPDB1" }
    set res [rest::post http://localhost:8080/diset $body ]
set body { "dict": "tpcc", "key": "count_ware", "value": "10" }
    set res [rest::post http://localhost:8080/diset $body ]
set body { "dict": "tpcc", "key": "tpcc_pass", "value": "oracle" }
    set res [rest::post http://localhost:8080/diset $body ]
set body { "dict": "tpcc", "key": "num_vu", "value": "5" }
    set res [rest::post http://localhost:8080/diset $body ]
puts "Starting Schema Build"
    set res [rest::post http://localhost:8080/buildschema "" ]
puts $res
wait_for_build_to_complete
	}
#Run build followed by run test
run_build
run_test
</programlisting>

      <para>An example of the output running the script is shown.</para>

      <programlisting>./bin/tclsh8.6 
% source buildrun_tpcc.tcl
running build
Setting Dict Values
Starting Schema Build
{"success": {"message": "Building 10 Warehouses with 6 Virtual Users, 5 active + 1 Monitor VU(dict value num_vu is set to 5): JOBID=5D1F4CA858CE03E213431323"}}
Build Complete
Setting Db values
Setting Vusers
{"success": {"message": "Virtual users set to 5"}}
Setting Dict Values
Config
{
  "connection": {
    "system_user": "system",
    "system_password": "oracle",
    "instance": "VULPDB1",
    "rac": "0"
  },
  "tpcc": {
    "count_ware": "10",
    "num_vu": "5",
    "tpcc_user": "tpcc",
    "tpcc_pass": "oracle",
    "tpcc_def_tab": "tpcctab",
    "tpcc_ol_tab": "tpcctab",
    "tpcc_def_temp": "temp",
    "partition": "false",
    "hash_clusters": "false",
    "tpcc_tt_compat": "false",
    "total_iterations": "1000000",
    "raiseerror": "false",
    "keyandthink": "false",
    "checkpoint": "false",
    "ora_driver": "timed",
    "rampup": "1",
    "duration": "2",
    "allwarehouse": "false",
    "timeprofile": "false"
  }
}
Clearscript
{"success": {"message": "Script cleared"}}
Loadscript
{"success": {"message": "script loaded"}}
Create VU
{"success": {"message": "6 Virtual Users Created with Monitor VU"}}
Run VU
{"success": {"message": "Running Virtual Users: JOBID=5D1F4FF558CE03E223730313"}}
Test Complete
Test result: [
  "5D1F4FF558CE03E223730313",
  "TEST RESULT : System achieved 0 Oracle TPM at 27975 NOPM"
]
% </programlisting>
    </section>

    <section>
      <title>Query Job Output</title>

      <para>Note that no output is seen directly in the script and no output
      recorded to a logfile. Instead the output is stored as a job by the web
      service. For example the following script would retrieve the output for
      the run job.</para>

      <programlisting>set UserDefaultDir [ file dirname [ info script ] ]
::tcl::tm::path add "$UserDefaultDir/modules"
package require rest
package require huddle
    set res [rest::get http://localhost:8080/jobs?jobid=5D1F4FF558CE03E223730313 "" ]
puts "JSON format"
puts $res
</programlisting>

      <para>With the output as follows.</para>

      <programlisting>% source joboutput.tcl
JSON format
[
  "0",
  "Vuser 1:RUNNING",
  "1",
  "Beginning rampup time of 1 minutes",
  "0",
  "Vuser 2:RUNNING",
  "2",
  "Processing 1000000 transactions with output suppressed...",
  "0",
  "Vuser 3:RUNNING",
  "3",
  "Processing 1000000 transactions with output suppressed...",
  "0",
  "Vuser 4:RUNNING",
  "4",
  "Processing 1000000 transactions with output suppressed...",
  "0",
  "Vuser 5:RUNNING",
  "5",
  "Processing 1000000 transactions with output suppressed...",
  "0",
  "Vuser 6:RUNNING",
  "6",
  "Processing 1000000 transactions with output suppressed...",
  "1",
  "Rampup 1 minutes complete ...",
  "1",
  "Rampup complete, Taking start AWR snapshot.",
  "1",
  "Start Snapshot 298 taken at 05 JUL 2019 14:20 of instance VULCDB1 (1) of database VULCDB1 (1846545596)",
  "1",
  "Timing test period of 2 in minutes",
  "1",
  "1  ...,",
  "1",
  "2  ...,",
  "1",
  "Test complete, Taking end AWR snapshot.",
  "1",
  "End Snapshot 298 taken at 05 JUL 2019 14:20 of instance VULCDB1 (1) of database VULCDB1 (1846545596)",
  "1",
  "Test complete: view report from SNAPID  298 to 298",
  "1",
  "5 Active Virtual Users configured",
  "1",
  "TEST RESULT : System achieved 0 Oracle TPM at 27975 NOPM",
  "0",
  "Vuser 2:FINISHED SUCCESS",
  "0",
  "Vuser 1:FINISHED SUCCESS",
  "0",
  "Vuser 6:FINISHED SUCCESS",
  "0",
  "Vuser 5:FINISHED SUCCESS",
  "0",
  "Vuser 3:FINISHED SUCCESS",
  "0",
  "Vuser 4:FINISHED SUCCESS",
  "0",
  "ALL VIRTUAL USERS COMPLETE"
]
</programlisting>

      <para>The dumpdb command can be used to dump all of the SQLite database
      to the web service console for debugging and the killws command cause
      the web service terminate.</para>
    </section>
  </chapter>

  <chapter>
    <title>Introduction to Analytic Testing (TPC-H) and Cloud Queries</title>

    <para>Analytic workloads can also be interchangeably described as Decision
    Support, Data Warehousing or Business Intelligence, the basis of these
    workloads is the ability to process complex ad-hoc queries on large
    volumes of data. In contrast to a transactional workload the focus is upon
    reading as opposed to modifying data and therefore requires a distinct
    approach. The ability of a database to process transactions gives limited
    information towards the ability of a database to support query based
    workloads and vice-versa, therefore both TPC-C and TPC-H based workloads
    complement each other in investigating the capabilities of a particular
    database. When reading large volumes of data to satisfy query workloads it
    should be apparent that if multiple CPU cores are available reading with a
    single processing thread is going to leave a significant amount of
    resources underutilized. Consequently the most effective Analytic Systems
    employ a feature called Parallel Query to break down such queries into
    multiple sub tasks to complete the query more quickly. Additional features
    such as column orientation, compression and partitioning can also be used
    to improve parallel query performance. Advances in server technologies in
    particular large numbers of CPU cores available with large memory
    configurations have popularised both in-memory and column store
    technologies as a means to enhance Parallel Query performance. Examples of
    databases supported by HammerDB that support some or all of these enhanced
    query technologies are the Oracle Database, SQL Server, Db2, MariaDB and
    PostgreSQL, databases that do not support any of these technologies are
    single threaded query workloads and cannot be expected to complete these
    workloads as quickly. As a NoSQL database Redis does not support Analytic
    workloads. If you are unfamiliar with row-oriented and column-store
    technologies then it is beneficial to read one of the many guides
    explaining the differences and familiarising with the technologies
    available in the database that you have chosen to test. With commercial
    databases you should also ensure that your license includes the ability to
    run Parallel workloads as you may have a version of a database that
    supports single-threaded workloads only.</para>

    <section>
      <title>What is TPC-H?</title>

      <para>To complement the usage of the TPC-C specification for OLTP
      workloads HammerDB also uses the relevant TPC specification for Analytic
      Systems called TPC-H. Just as with the TPC-C specification the “TPC
      benchmarks are industry standards. The TPC, at no charge, distributes
      its benchmark specifications to the public.”Therefore HammerDB includes
      an implementation of the specification of the TPC-H benchmark that can
      be run in any supported database environment. As with the load tests
      based upon TPC-C it is important to note that the implementation is not
      a full specification TPC-H benchmark and the query results cannot be
      compared with the official published benchmarks in any way. Instead the
      same approach has been taken to enable you to run an accurate and
      repeatable query based workload against your own database. TPC-H in
      simple terms can be thought of as complementing the workload implemented
      in TPC-C related to the activities of a wholesale supplier. However
      whereas TPC-C simulates an online ordering system TPC-H represents the
      typical workload of business users inquiring about the performance of
      their business. To do this TPC-H is represented by a set of business
      focused ad-hoc queries (in addition to concurrent data updates and
      deletes) and is measured upon the time it takes to complete these
      queries. In particular the focus is upon highly complex queries that
      require the processing of large volumes of data. Also in similarity to
      TPC-C the schema size is not fixed and is dependent upon a Scale Factor
      and there your schema your test schema can also be as small or large as
      you wish with a larger schema requiring a more powerful computer system
      to process the increased data volume for queries. However in contrast to
      TPC-C it is not valid to compare the test results of query load tests
      taken at different Scale Factors shown as SF in the Schema
      diagram.</para>

      <figure>
        <title>TPC-H Schema.</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch9-1.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The workload is represented by users executing a stream of 22
      ad-hocs queries against the database with an example query as
      follows:</para>

      <programlisting>-- using 647655760 as a seed to the RNG
 select
        l_returnflag,
        l_linestatus,
        sum(l_quantity) as sum_qty,
        sum(l_extendedprice) as sum_base_price,
        sum(l_extendedprice * (1 - l_discount)) as sum_disc_price,
        sum(l_extendedprice * (1 - l_discount) * (1 + l_tax)) as sum_charge,
        avg(l_quantity) as avg_qty,
        avg(l_extendedprice) as avg_price,
        avg(l_discount) as avg_disc,
        count(*) as count_order 
from
        lineitem 
where
        l_shipdate &lt;= date '1998-12-01' – interval '69' day (3)
group by
        l_returnflag,
        l_linestatus 
order by
        l_returnflag,
        l_linestatus;
</programlisting>

      <para>In measuring the results the key aspect is the time the queries
      take to complete and it is recommended to use the geometric mean of the
      query times for comparison. A typical performance profile is represented
      by the time it takes the system to process a query set from Q1 to Q22
      (run in a pre-determined random order).</para>

      <figure>
        <title>Power Query</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch9-2.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>

    <section>
      <title>Running the Power and Throughput Test and Calculating the
      Geometric Mean</title>

      <para>The audited metric for TPC-H workloads is called QphH, however
      because of the differences in how these workloads are run, in particular
      using bulk operations for data loads it is not recommended that the QphH
      be calculated for HammerDB workloads. Instead it is recommended to
      measure and compare the geometric mean of the power and throughput test
      query times. For the audited results the following 3 aspects of the
      capability of the system to process queries are considered:</para>

      <orderedlist>
        <listitem>
          <para>Database size.</para>
        </listitem>

        <listitem>
          <para>Query processing power of queries in a single stream.</para>
        </listitem>

        <listitem>
          <para>Total query throughput of queries from multiple concurrent
          users.</para>
        </listitem>
      </orderedlist>

      <para>For the multiple concurrent user tests the throughput test always
      follows the power test and the number of Virtual Users is based upon the
      following table where each Stream is processed by a Virtual User in
      HammerDB. This can also serve as a guide when running throughput tests
      with HammerDB taking the metric as the geomean of the query times of the
      slowest virtual user to complete the query set.</para>

      <table>
        <title>Query Streams and Scale Factors</title>

        <tgroup cols="2">
          <thead>
            <row>
              <entry align="center">SF ( Scale Factor )</entry>

              <entry align="center">S (Streams)</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>100000</entry>

              <entry>11</entry>
            </row>

            <row>
              <entry>30000</entry>

              <entry>10</entry>
            </row>

            <row>
              <entry>10000</entry>

              <entry>9</entry>
            </row>

            <row>
              <entry>3000</entry>

              <entry>8</entry>
            </row>

            <row>
              <entry>1000</entry>

              <entry>7</entry>
            </row>

            <row>
              <entry>300</entry>

              <entry>6</entry>
            </row>

            <row>
              <entry>100</entry>

              <entry>5</entry>
            </row>

            <row>
              <entry>30</entry>

              <entry>4</entry>
            </row>

            <row>
              <entry>10</entry>

              <entry>3</entry>
            </row>

            <row>
              <entry>1</entry>

              <entry>2</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <para>There is also the availability for a simultaneous data refresh
      set. HammerDB provides full capabilities to run this refresh set both
      automatically as part of a Power test and concurrently with a Throughput
      test. Note however that once a refresh set is run the schema is required
      to be refreshed and it is prudent to backup and restore a HammerDB TPC-H
      based schema where running a refresh set is planned.</para>
    </section>

    <section>
      <title>Choosing a Database for running TPC-H workloads</title>

      <para>TPC-H workloads run complex queries scanning large volumes of data
      and therefore require the use of database features such as parallel
      query and in-memory column stores to maximise performance. With the
      available HammerDB TPC-H based workloads the three databases that
      support these features are the Enterprise Editions of Oracle, SQL Server
      and Db2 and therefore these databases will deliver the best experience
      for building and running TPC-H. Over time there has been improvement
      with open-source and open-source derived databases in the ability to run
      TPC-H workloads. For example PostgreSQL supports Parallel Query and the
      PostgreSQL derived versions of Amazon Redshift and Greenplum offer
      further accelerated query solutions.MySQL does not support an analytic
      storage engine however the MariaDB column store storage is best suited
      for running analytic tests against MySQL. Nevertheless it is known that
      with some or all of the open source solutions a number of queries either
      fail or are extrmemly long running due to the limitations of the
      databases themselves (and not HammerDB) therefore these workloads should
      be viewed in an experimental manner as they will not result in the
      ability to generate a QphH value.</para>

      <section>
        <title>Oracle</title>

        <para>The Oracle database is fully featured for running TPC-H based
        workloads and presents two options for configuring the database either
        row oriented parallel query or the In-Memory Column Store (IM column
        store). Both of these configurations are able to run a full TPC-H
        workload and are configured on the database as opposed to configuring
        with HammerDB.</para>
      </section>

      <section>
        <title>Microsoft SQL Server</title>

        <para>SQL Server is able to support a full TPC-H workload and offers
        row oriented parallel query as well as in-memory column store
        configured. The clustered columnstore build is selected through the
        HammerDB Build Options.</para>

        <figure>
          <title>Clustered Columnstore</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch9-9.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>

      <section>
        <title>Db2</title>

        <para>Db2 can support a full TPCH workload through row oriented
        parallel query and Db2 BLU in-memory column store. The column store is
        selected through the Db2 Organize by options.</para>

        <figure>
          <title>Db2 Organize By</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch9-10.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>

      <section>
        <title>PostgreSQL</title>

        <para>PostgreSQL supports standard row oriented parallel query. This
        offers significant performance improvement over single-threaded
        queries however not all queries at all schema sizes are expected to
        complete without database error and some run for a significant length
        of time. Options are also available to run the PostgreSQL queries
        against a Greenplum database. It is important to be aware that because
        of the Greenplum MPP architecture there is significant overhead in
        processing INSERT operations and therefore data should be loaded in
        bulk after generating with the HammerDB datagen operation.</para>

        <figure>
          <title>PostgreSQL TPC-H</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch9-11.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>

      <section>
        <title>MySQL / MariaDB</title>

        <para>MySQL does not support row oriented parallel query or a column
        store configuration and therefore queries run against a MySQL database
        are expected to be long-running. However the MySQL compatible MariaDB
        supports a separate installation of a column store based database
        which offers significantly improved query times. However some queries
        will result in database errors or long running queries. This option is
        selected with the Data Warehouse Storage Engine Option.</para>

        <figure>
          <title>MySQL MariaDB TPC-H</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch9-12.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>

      <section>
        <title>Redis</title>

        <para>Redis does not support analytic workloads and therefore HammerDB
        does not have a TPC-H workload for Redis.</para>
      </section>
    </section>

    <section>
      <title>Benchmarking Database Cloud Services</title>

      <para>In addition to the TPC-H workload for Oracle, PostgreSQL and MySQL
      there are also a set of Cloud Analytic Queries made publicly available
      by Oracle for comparison of Cloud Analytic services. These queries run
      against a TPC-H schema and are included with HammerDB for running
      against Oracle, Amazon Aurora and Amazon Redshift with Amazon Aurora and
      Redshift being based upon and compatible with MySQL and PostgreSQL
      respectively. Note however that in similarity to MySQL Amazon does not
      have the features to support analytics such as parallel query or a
      column store option and therefore running the analytic tests against
      Aurora although possible is not likely to generate the best results.
      Amazon Redshift however is a column oriented database based on
      PostgreSQL and suitable for running analytic workloads.</para>

      <para>For the Cloud Analytic workload the database is created and loaded
      according to the TPC-H specification – HammerDB by default can create
      and load schemas of scale factor 1, 10, 30, 100, 300 and 1000 which
      corresponds to 1GB, 10GB, 30GB, 100GB, 300GB and 1TB respectively.
      HammerDB will create the schema and load the data. The time this takes
      is dependent on multiple factors – in particular storage performance is
      a key factor for inserting a large number of rows into the database as
      each insert requires redo logging and there is the potential for
      contention as the number of virtual users increases all inserting rows
      into the same table at the same time. For this reason especially as the
      Oracle specification requires a schema size of 10TB, it is recommended
      to create the schema with HammerDB using the Generating and Bulk Loading
      Data feature and this guide details how to do this for both Oracle and
      Redshift and this is particularly recommended when uploading data to the
      cloud.</para>

      <para>You are permitted to run both the in-built TPC-H query workload
      and the Oracle provided query set against any size schema however the
      Oracle workload provides 13 new analytic queries – that is run in
      sequence as per the original power test of TPC-H. This new query set
      compatible with Oracle, Redshift/PostgreSQL and MySQL/Aurora is enabled
      under the TPC-H Driver Script Options dialog by selecting the Cloud
      Analytic Queries checkbox. This query set reports the geometric mean of
      the completed queries that returns rows for circumstances where the
      query set is run on a scale factor size of less than 10TB. Given the
      similarity of the Oracle implementation to the existing TPC-H workload
      the following example illustrates running the workload against Amazon
      Redshift.</para>

      <section>
        <title>Redshift Cloud Analytic Workload</title>

        <para>Ensure that your Redshift cluster is active and note your
        Endpoint name given above the cluster properties.</para>

        <figure>
          <title>Redshift console</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch9-13.png"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>Also ensure that access is enabled to the cluster both by
        defining a user and a security group and allowing access through your
        firewall.</para>

        <figure>
          <title>Create Security Group</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch9-14.png"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>Create the TPC-H schema within Redshift using the HammerDB
        Generating and Bulk Loading Data feature. Under PostgreSQL TPC-H
        Driver Options use the Redshift Endpoint as your PostgreSQL host and
        5439 as your port. Set the user and password to the credentials you
        have set under the Amazon AWS console. To run the Cloud Analytic
        Workload with HammerDB refer to the following Chapter on How to run an
        Analytic Workload and select the Cloud Analytic Queries and Redshift
        Compatible Checkbox with the reported metric being the geometric mean
        of the query times that complete for the one Virtual User used. Note
        that when running the queries against data sets smaller than the
        specified 10TB this may result in some queries not returning rows,
        therefore for your calculations HammerDB calculates the geometric mean
        only of queries that returned rows.</para>
      </section>
    </section>
  </chapter>

  <chapter>
    <title>How to Run an Analytic Workload</title>

    <para>The basis of Analytic or Decision Support Systems is the ability to
    process complex ad-hoc queries on large volumes of data. Processing this
    amount of data within a single process or thread on traditional
    row-oriented database is time consuming. Consequently it is beneficial
    Parallel Query to break down such queries into multiple sub tasks to
    complex the query more quickly. Additional features such as compression
    and partitioning are also used with Parallel Query to improve performance.
    As a consequence when planning analytic workloads for optimal performance
    you should consider the database features for in-memory and parallel
    execution configuration. In similarity to the HammerDB OLTP workload,
    HammerDB implements a fair usage of a TPC workload however the results
    should not be compared to official published TPC-H results in any
    way.</para>

    <section>
      <title>SUT Database Server Configuration</title>

      <para>For query based workloads there is no requirement for a load
      testing client although you may use one if you wish. It is entirely
      acceptable to run HammerDB directly on the SUT (System Under Test)
      Database system if you wish, the client workload is minimal compared to
      an OLTP workload. In the analytic workload the client sends long running
      queries to the SUT and awaits a response therefore requiring minimal
      resources on the client side. As with an OLTP configuration however the
      database server architecture to be tested must meet the standard
      requirements for a database server system. Similarly the database can be
      installed on any supported operating system, there is no restriction on
      the version of that is required. Before running a HammerDB analytic test
      depending on your configuration you should focus on memory and I/O (disk
      performance). Also in turn the number and type of multi-core and
      multi-threaded processors installed will have a significant impact on
      parallel performance to drive the workload. When using in-memory column
      store features processors that support SIMD/AVX instructions sets are
      also required for the vectorisation of column scans. HammerDB by default
      provides TPC-H schemas at Scale Factors 1,10,30,100,300 and 1000 (larger
      can be configured if required). The Scale Factors correspond to the
      schema size in Gigabytes. As with the official TPC-H tests the results
      at one schema size should not be compared with the results derived with
      another schema size. As the analytic workload utilizes parallel query
      where available it is possible for a single virtual user to use all of
      the CPU resources on the SUT at any schema size. Nevertheless there is
      still a relation with all of the hardware resources available including
      memory and I/O and a larger system will benefit from tests run a larger
      schema size. The actual sizing of hardware resources of hardware
      resources is beyond the scope of this document however at the basic
      level with traditional parallel execution and modern CPU capabilities
      I/O read performance is typically the constraining factor. Note that
      also in contrast to an OLTP workload high throughput transaction log
      write performance is not a requirement, however in similarity to the
      OLTP workload storage based on SSD disks will usually offer significant
      improvements in performance over standard hard disks although in this
      case it is the benefits of read bandwidth as opposed to the IOPs
      benefits of SSDs for OLTP. When using the in-memory column store memory
      capacity and bandwidth feature and if fully cached in memory storage
      performance is not directly a factor for query performance. Nevertheless
      data loads are an important consideration for in-memory data and
      therefore I/O and SSD read performance remain important for loading the
      data into memory to be available for scans.</para>
    </section>

    <section>
      <title>Installation and Configuration</title>

      <para>Before creating a test schema you should ensure that your database
      is configured to process analytic workloads instead of
      transactional.</para>

      <section>
        <title>Oracle</title>

        <para>When your database server is installed you should create a
        tablespace into which the test data will be installed allowing disk
        space according to the guide previously given in this document.</para>

        <programlisting>SQL&gt; create tablespace tpchtab datafile size 20g;

Tablespace created.
</programlisting>

        <para>When using parallel query ensure that the instance is configured
        for parallel execution, noting in particular the value for
        parallel_max_servers.</para>

        <programlisting>SQL&gt; show parameter parallel
NAME                      TYPE        VALUE
------------------------------------ -----------
parallel_max_servers      integer    160
…
parallel_min_servers      integer    16
…
parallel_servers_target   integer    64
parallel_threads_per_cpu  integer    2
</programlisting>

        <para>For testing purposes you can disable parallel execution in a
        particular environment by setting parallel_max_servers to a value of
        zero. An additional parameter that can provide significant benefit to
        the performance of parallel query workloads is
        optimizer_dynamic_sampling. By default this value is set to 2.
        Increasing this value to 4 has been shown to benefit query performance
        however testing the impact of changing this parameter should always be
        done during pre-testing as it may change between Oracle
        releases.</para>

        <programlisting>SQL&gt; alter system set optimizer_dynamic_sampling=4;

System altered.

SQL&gt; show parameter optimizer_dynamic

NAME                       TYPE       VALUE
------------------------------------ ----------- 
optimizer_dynamic_sampling integer    4


</programlisting>

        <para>If using the In-Memory option ensure that the parameter
        inmemory_size has been configured and the database restarted.</para>

        <programlisting>SQL&gt; show parameter inmemory

NAME                             TYPE        VALUE
-------------------------------------------- --------
inmemory_clause_defaultstring
inmemory_force                   string      DEFAULT
inmemory_max_populate_servers    integer     2
inmemory_query                   string      ENABLE
inmemory_size                    big integer 1500M
inmemory_trickle_repopulate      integer     1
_servers_percent    
optimizer_inmemory_aware         boolean     TRUE
</programlisting>

        <para>Then alter the new tablespace containing the schema to be
        in-memory.</para>

        <programlisting>SQL&gt; alter tablespace TPCHTAB default inmemory;
Tablespace altered.
</programlisting>

        <para>As shown the objects created within the tablespace will now be
        configured to be in-memory.</para>

        <para><programlisting>SQL&gt; select tablespace_name, def_inmemory from dba_tablespaces;

TABLESPACE_NAME           DEF_INME
------------------------- --------
SYSTEM                    DISABLED
SYSAUX                    DISABLED
TEMP                      DISABLED
USER                      DISABLED
TPCCTAB                   DISABLED
TPCHTAB                   ENABLED
</programlisting></para>

        <para>For larger schemas both partitioning and compression settings
        (both standard and in-memory) should be considered for query
        tuning.</para>
      </section>

      <section>
        <title>SQL Server</title>

        <para>For SQL Server ensure that the Max Degree of Parallelism is set
        to the maximum number of cores that you wish to process the
        queries.</para>

        <figure>
          <title>Max Degree of Parallelism</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch13-1.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>

      <section>
        <title>Db2</title>

        <para>If using DB2 BLU use db2set to set the parameter DB2_WORKLOAD to
        ANALYTICS.</para>

        <programlisting>[db2inst1 ~]$ db2set DB2_WORKLOAD=ANALYTICS
[db2inst1 ~]$ db2stop force
25/05/2016 10:08:22     0   0   SQL1064N  DB2STOP processing was successful.
SQL1064N  DB2STOP processing was successful.
[db2inst1 ~]$ db2start
05/25/2016 10:08:27     0   0   SQL1063N  DB2START processing was successful.
SQL1063N  DB2START processing was successful.
</programlisting>

        <para>In your db2cli.ini set the following parameter for each of the
        databases that you plan to create to test the TPC-H workload, this
        will prevent failure due to SQLSTATE 01003 “Null values were
        eliminated from the argument of a column function” when running the
        workload thereby preventing the query set from completing.</para>

        <programlisting>[db2inst1 cfg]$ more db2cli.ini 
[TPCH]
IgnoreWarnList="'01003'"
[TPCH1]
IgnoreWarnList="'01003'"
</programlisting>

        <para>When your database server is installed you should create a
        database into which the test data will be installed, for TPC-H
        workloads a large pagesize is recommended.</para>

        <programlisting>[db2inst1 ~]$ db2 create database tpch pagesize 32 k
DB20000I  The CREATE DATABASE command completed successfully.
</programlisting>
      </section>

      <section>
        <title>PostgreSQL</title>

        <para>The PostgreSQL Server should be at a minimum level of 9.6 that
        supports Parallel Query. The postgresql.conf file should include
        parallelism specific parameters such as follows:</para>

        <programlisting>work_mem = 1000MB                              
max_worker_processes = 16
max_parallel_workers_per_gather = 16
force_parallel_mode = on

</programlisting>
      </section>

      <section>
        <title>MySQL / MariaDB</title>

        <para>To test analytic workloads on a MySQL compatible databases
        MariaDB columnstore is the solution that should be used. Note that
        this is a separate installation from a MySQL or MariaDB installation
        rather than a pluggable storage engine. As MariaDB columnstore is
        based on a columnstore solution called InfiniDB the relevant
        parameters start with infinidb, for example:</para>

        <programlisting># Enable compression by default on create, set to 0 to turn off
infinidb_compression_type=2
# Default for string table threshhold
infinidb_stringtable_threshold=20
# infinidb local query flag
# infinidb_local_query=1
infinidb_diskjoin_smallsidelimit=0
infinidb_diskjoin_largesidelimit=0
infinidb_diskjoin_bucketsize=100
infinidb_um_mem_limit=64000
infinidb_use_import_for_batchinsert=1
infinidb_import_for_batchinsert_delimiter=7
</programlisting>
      </section>
    </section>

    <section>
      <title>Configuring Schema Build Options</title>

      <para>To create the analytic test schema based on the TPC-H
      specification you will need to select which benchmark and database you
      wish to use by choosing select benchmark from under the Options menu or
      under the benchmark tree-view. The initial settings are determined by
      the values in your XML configuration files. The following example shows
      the selection of SQL Server however the process is the same for all
      databases.</para>

      <para><figure>
          <title>Benchmark Options</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch13-2.PNG"/>
            </imageobject>
          </mediaobject>
        </figure></para>

      <para>To create the TPC-H schema select the TPC-H schema options menu
      tab from the benchmark tree-view or the options menu. This menu will
      change dynamically according to your chosen database.</para>

      <para><figure>
          <title>TPC-H Schema Build Options</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch13-3.PNG"/>
            </imageobject>
          </mediaobject>
        </figure></para>

      <para>If selected from the Options menu the schema options window is
      divided into two sections. The “Build Options” section details the
      general login information and where the schema will be built and the
      “Driver Options” for the Driver Script to run after the schema is built.
      If selected from the benchmark tree-view only the “Build Options” are
      shown and these are the only options of importance at this stage. Note
      that in any circumstance you do not have to rebuild the schema every
      time you change the “Driver Options”, once the schema has been built
      only the “Driver Options” may need to be modified. For the “Build
      Options” fill in the values according to the database where the schema
      will be built as follows.</para>

      <section>
        <title>Oracle Schema Build Options</title>

        <figure>
          <title>Oracle TPC-H Build Options</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch13-4.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para><table>
            <title>Oracle Build Options</title>

            <tgroup cols="2">
              <thead>
                <row>
                  <entry align="center">Option</entry>

                  <entry align="center">Description</entry>
                </row>
              </thead>

              <tbody>
                <row>
                  <entry>Oracle Service Name</entry>

                  <entry>The Oracle Service Name is the service name that your
                  load generation server will use to connect to the database
                  running on the SUT database server.</entry>
                </row>

                <row>
                  <entry>System User</entry>

                  <entry>The “system” user or a user with system level
                  privileges</entry>
                </row>

                <row>
                  <entry>System User Password</entry>

                  <entry>The system user password is the password for the
                  “system” user you entered during database creation. The
                  system user already exists in all Oracle databases and has
                  the necessary permissions to create the TPC-H user.</entry>
                </row>

                <row>
                  <entry>TPC-H User</entry>

                  <entry>The TPC-H user is the name of a user to be created
                  that will own the TPC-H schema. This user can have any name
                  you choose but must not already exist and adhere to the
                  standard rules for naming Oracle users. You may if you wish
                  run the schema creation multiple times and have multiple
                  TPC-H schemas created with ownership under a different user
                  you create each time.</entry>
                </row>

                <row>
                  <entry>TPC-H User Password</entry>

                  <entry>The TPC-H user password is the password to be used
                  for the TPC-H user you create and must adhere to the
                  standard rules for Oracle user password. You will need to
                  remember the TPC-H user name and password for running the
                  TPC-H driver script after the schema is built.</entry>
                </row>

                <row>
                  <entry>TPC-H Default Tablespace</entry>

                  <entry>The TPC-H default tablespace is the tablespace that
                  will be the default for the TPC-H user and therefore the
                  tablespace to be used for the schema creation. The
                  tablespace must have sufficient free space for the schema to
                  be created.</entry>
                </row>

                <row>
                  <entry>TPC-H Temporary Tablespace</entry>

                  <entry>The TPC-H temporary tablespace is the temporary
                  tablespace that already exists in the database to be used by
                  the TPC-H User.</entry>
                </row>

                <row>
                  <entry>TimesTen Database Compatible</entry>

                  <entry>When selected this option means that the Oracle
                  Service Name should be a TimesTen Data Source Name and will
                  grey out non-compatible options.</entry>
                </row>

                <row>
                  <entry>Scale Factor</entry>

                  <entry>The Scale Factor is selected by a radio button with a
                  choice of scale factors of 1,10,30,100,300 and 1000
                  corresponding to 1GB, 10GB, 30GB,100GB and 1000GB
                  respectively, larger schema sizes can also be created with
                  the datagen option. Note that the required space will be
                  larger than these values due to the indexes
                  required.</entry>
                </row>

                <row>
                  <entry>Virtual Users to Build Schema</entry>

                  <entry>The Virtual Users to Build Schema is the number of
                  Virtual Users to be created on the Load Generation Server
                  that will complete your multi-threaded schema build. You
                  should set this value to the number of cores on your Load
                  Generation Server or SUT if HammerDB is running
                  there.</entry>
                </row>
              </tbody>
            </tgroup>
          </table></para>
      </section>

      <section>
        <title>SQL Server Schema Build Options</title>

        <figure>
          <title>SQL Server Build Options</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch13-5.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para><table>
            <title>SQL Server Build Options</title>

            <tgroup cols="2">
              <thead>
                <row>
                  <entry align="center">Option</entry>

                  <entry align="center">Description</entry>
                </row>
              </thead>

              <tbody>
                <row>
                  <entry>SQL Server</entry>

                  <entry>The Microsoft SQL Server is the host name or host
                  name and instance where the TPC-H database will be
                  created.</entry>
                </row>

                <row>
                  <entry>TCP</entry>

                  <entry>Use the TCP Protocol</entry>
                </row>

                <row>
                  <entry>SQL Server Port</entry>

                  <entry>When TCP is enabled, the SQL Server Port is the
                  network port that your load generation server will use to
                  connect to the database running on the SUT database server.
                  In most cases this will be the default port of 1433 and will
                  not need to be changed.</entry>
                </row>

                <row>
                  <entry>Azure</entry>

                  <entry>Include the Database name in the connect string
                  typical of Azure connections. To successfully build the
                  schema this database must be created and empty.</entry>
                </row>

                <row>
                  <entry>SQL Server ODBC Driver</entry>

                  <entry>The Microsoft SQL ODBC Driver is the ODBC driver you
                  will use to connect to the SQL Server database. To view
                  which drivers are available on Windows view the ODBC Data
                  Source Administrator Tool.</entry>
                </row>

                <row>
                  <entry>Authentication</entry>

                  <entry>When installing SQL Server on Windows you will have
                  configured SQL Server for Windows or Windows and SQL Server
                  Authentication. On Linux you will be using SQL Server
                  Authentication. If you specify Windows Authentication then
                  SQL Server will use a trusted connection to your SQL Server
                  using your Windows credentials without requiring a username
                  and password. If SQL Server Authentication is specified and
                  SQL Authentication is enabled on your SQL Server then you
                  will be able connect by specifying a username and password
                  that you have already configured on your SQL Server.</entry>
                </row>

                <row>
                  <entry>SQL Server User ID</entry>

                  <entry>The SQL Server User ID is the User ID of a user that
                  you have already created on your SQL Server.</entry>
                </row>

                <row>
                  <entry>SQL Server User Password</entry>

                  <entry>The SQL Server User Password is the Password
                  configured on the SQL Server for the User ID you have
                  specified. Note that when configuring the password on the
                  SQL Server there is a checkbox that when selected enforces
                  more complex rules for passwords or if unchecked enables a
                  simple password such as “admin”.</entry>
                </row>

                <row>
                  <entry>SQL Server TPCH Database</entry>

                  <entry>The SQL Server Database is the name of the Database
                  to be created on the SQL Server to contain the schema. If
                  this database does not already exist then HammerDB will
                  create it, if the database does already exist and the
                  database is empty then HammerDB will use this existing
                  database. Therefore if you wish to create a particular
                  layout or schema then pre-creating the database and using
                  this database is an advanced method to use this
                  configuration.</entry>
                </row>

                <row>
                  <entry>MAXDOP</entry>

                  <entry>The MAXDOP setting defines the maximum degree of
                  parallelism to be set as a default on the schema
                  objects.</entry>
                </row>

                <row>
                  <entry>Clustered Columnstore</entry>

                  <entry>This option selects the database to be created with
                  in-memory clustered columnstore indexes.</entry>
                </row>

                <row>
                  <entry>Scale Factor</entry>

                  <entry>The Scale Factor is selected by a radio button with a
                  choice of scale factors of 1,10,30,100,300 and 1000
                  corresponding to 1GB, 10GB, 30GB,100GB and 1000GB
                  respectively, larger schema sizes can also be created with
                  the datagen option. Note that the required space will be
                  larger than these values due to the indexes
                  required.</entry>
                </row>

                <row>
                  <entry>Virtual Users to Build Schema</entry>

                  <entry>The Virtual Users to Build Schema is the number of
                  Virtual Users to be created on the Load Generation Server
                  that will complete your multi-threaded schema build. You
                  should set this value to the number of cores on your Load
                  Generation Server or SUT if HammerDB is running
                  there.</entry>
                </row>
              </tbody>
            </tgroup>
          </table></para>
      </section>

      <section>
        <title>Db2 Schema Build Options</title>

        <figure>
          <title>Db2 Build Options</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch13-6.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para><table>
            <title>Db2 Build Options</title>

            <tgroup cols="2">
              <thead>
                <row>
                  <entry align="center">Option</entry>

                  <entry align="center">Description</entry>
                </row>
              </thead>

              <tbody>
                <row>
                  <entry>Db2 User</entry>

                  <entry>The name of the operating system user to connect to
                  the DB2 database for example db2inst1.</entry>
                </row>

                <row>
                  <entry>Db2 Password</entry>

                  <entry>The password for the operating system DB2 user by
                  default “ibmdb2”</entry>
                </row>

                <row>
                  <entry>Db2 Database</entry>

                  <entry>The name of the Db2 database that you have already
                  created, for example “tpcc”</entry>
                </row>

                <row>
                  <entry>Db2 Default Tablespace</entry>

                  <entry>The name of the existing tablespace where tables
                  should be located if a specific tablespace has not been
                  defined for that table in the tablespace list. The default
                  is “USERSPACE1”.</entry>
                </row>

                <row>
                  <entry>Db2 Organize By</entry>

                  <entry>The Organize by option is selected by a radio button
                  and determines an optional organize by clause to be
                  specified when creating the tables. The database version
                  must be able to accept the option chosen and therefore the
                  recommended choice is NONE to accept the defaults. When the
                  setting DB2_WORKLOAD is set to analytics for example the
                  default is configuration is for columnar storage. If for
                  example this parameter is set you can then choose ROW
                  configuration even when DB2_WORKLOAD is set to analytics to
                  create row organized tables. The DATE option is mutually
                  exclusive to the column store option however creates a ROW
                  organized table that is organized by date which can
                  accelerate some queries when row organized.</entry>
                </row>

                <row>
                  <entry>Scale Factor</entry>

                  <entry>The Scale Factor is selected by a radio button with a
                  choice of scale factors of 1,10,30,100,300 and 1000
                  corresponding to 1GB, 10GB, 30GB,100GB and 1000GB
                  respectively, larger schema sizes can also be created with
                  the datagen option. Note that the required space will be
                  larger than these values due to the indexes
                  required.</entry>
                </row>

                <row>
                  <entry>Virtual Users to Build Schema</entry>

                  <entry>The Virtual Users to Build Schema is the number of
                  Virtual Users to be created on the Load Generation Server
                  that will complete your multi-threaded schema build. You
                  should set this value to the number of cores on your Load
                  Generation Server or SUT if HammerDB is running
                  there.</entry>
                </row>
              </tbody>
            </tgroup>
          </table></para>
      </section>

      <section>
        <title>MySQL Schema Build Options</title>

        <para><figure>
            <title>MySQL Build Options</title>

            <mediaobject>
              <imageobject>
                <imagedata fileref="docs/images/ch13-8.PNG"/>
              </imageobject>
            </mediaobject>
          </figure></para>

        <para><table>
            <title>MySQL Build Options</title>

            <tgroup cols="2">
              <thead>
                <row>
                  <entry align="center">Option</entry>

                  <entry align="center">Description</entry>
                </row>
              </thead>

              <tbody>
                <row>
                  <entry>MySQL Host</entry>

                  <entry>The MySQL Host Name is the host name of the SUT
                  database server.</entry>
                </row>

                <row>
                  <entry>MySQL Port</entry>

                  <entry>The MySQL Port is the network port on the SUT
                  database server. In most cases this will be the default port
                  of 3306.</entry>
                </row>

                <row>
                  <entry>MySQL User</entry>

                  <entry>The MySQL User is the user which has permission to
                  create a database and you previously granted access to from
                  the load generation server. The root user already exists in
                  all MySQL databases and has the necessary permissions to
                  create the TPC-H database.</entry>
                </row>

                <row>
                  <entry>MySQL User Password</entry>

                  <entry>The MySQL user password is the password for the user
                  defined as the MySQL User. You will need to remember the
                  MySQL user name and password for running the TPC-H driver
                  script after the database is built.</entry>
                </row>

                <row>
                  <entry>MySQL Database</entry>

                  <entry>The MySQL Database is the database that will be
                  created containing the TPC-H schema creation. There must
                  have sufficient free space for the database to be
                  created.</entry>
                </row>

                <row>
                  <entry>Data Warehouse Storage Engine</entry>

                  <entry>Use the "show engine" command to display available
                  storage engines and select a storage engine that supports
                  analytics. For MariaDB columnstore specify.
                  "Columnstore"</entry>
                </row>

                <row>
                  <entry>Scale Factor</entry>

                  <entry>The Scale Factor is selected by a radio button with a
                  choice of scale factors of 1,10,30,100,300 and 1000
                  corresponding to 1GB, 10GB, 30GB,100GB and 1000GB
                  respectively, larger schema sizes can also be created with
                  the datagen option. Note that the required space will be
                  larger than these values due to the indexes
                  required.</entry>
                </row>

                <row>
                  <entry>Virtual Users to Build Schema</entry>

                  <entry>The Virtual Users to Build Schema is the number of
                  Virtual Users to be created on the Load Generation Server
                  that will complete your multi-threaded schema build. You
                  should set this value to the number of cores on your Load
                  Generation Server or SUT if HammerDB is running
                  there.</entry>
                </row>
              </tbody>
            </tgroup>
          </table></para>
      </section>

      <section>
        <title>PostgreSQL Schema Build Options</title>

        <figure>
          <title>PostgreSQL Build Options</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch13-7.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para><table>
            <title>PostgreSQL Build Options</title>

            <tgroup cols="2">
              <thead>
                <row>
                  <entry align="center">Option</entry>

                  <entry align="center">Description</entry>
                </row>
              </thead>

              <tbody>
                <row>
                  <entry>PostgreSQL Host</entry>

                  <entry>The host name of the SUT running PostgreSQL.</entry>
                </row>

                <row>
                  <entry>PostgreSQL Port</entry>

                  <entry>The port of the PostgreSQL service. By default this
                  will be 5432 for a standard PostgreSQL installation or 5444
                  for EnterpriseDB.</entry>
                </row>

                <row>
                  <entry>PostgreSQL Superuser</entry>

                  <entry>The PostgreSQL Superuser is a user with sufficient
                  privileges to create both new users (roles) and databases to
                  enable the creation of the test schema.</entry>
                </row>

                <row>
                  <entry>PostgreSQL Superuser Password</entry>

                  <entry>The PostgreSQL Superuser Password is the password for
                  the PostgreSQL superuser which will have been defined during
                  installation. If you have forgotten the password it can be
                  reset from a psql prompt that has logged in from a trusted
                  connection therefore requiring no password using postgres=#
                  alter role postgres password ‘postgres’;</entry>
                </row>

                <row>
                  <entry>PostgreSQL Default Database</entry>

                  <entry>The PostgreSQL default databases is the database to
                  specify for the superuser connection. Typically this will be
                  postgres for a standard PostgreSQL installation or edb for
                  EnterpriseDB.</entry>
                </row>

                <row>
                  <entry>PostgreSQL User</entry>

                  <entry>The PostgreSQL User is the user (role) that will be
                  created that owns the database containing the TPC-H
                  schema.</entry>
                </row>

                <row>
                  <entry>PostgreSQL User Password</entry>

                  <entry>The PostgreSQL User Password is the password that
                  will be specified for the PostgreSQL user when it is
                  created.</entry>
                </row>

                <row>
                  <entry>PostgreSQL Database</entry>

                  <entry>The PostgreSQL Database is the database that will be
                  created and owned by the PostgreSQL User that contains the
                  TPC-H schema.</entry>
                </row>

                <row>
                  <entry>Greenplum Database Compatible</entry>

                  <entry>Choosing Greenplum Database Compatible creates a
                  schema with Greenplum Database Options. Building the schema
                  by inserting into Greenplum is not recommended and instead a
                  bulk load of data created with the datagen option should be
                  used.</entry>
                </row>

                <row>
                  <entry>Greenplum Compressed Columns</entry>

                  <entry>Becomes active when Greenplum Database Compatible is
                  selected and configures the columns in a compressed
                  format.</entry>
                </row>

                <row>
                  <entry>Scale Factor</entry>

                  <entry>The Scale Factor is selected by a radio button with a
                  choice of scale factors of 1,10,30,100,300 and 1000
                  corresponding to 1GB, 10GB, 30GB,100GB and 1000GB
                  respectively, larger schema sizes can also be created with
                  the datagen option. Note that the required space will be
                  larger than these values due to the indexes
                  required.</entry>
                </row>

                <row>
                  <entry>Virtual Users to Build Schema</entry>

                  <entry>The Virtual Users to Build Schema is the number of
                  Virtual Users to be created on the Load Generation Server
                  that will complete your multi-threaded schema build. You
                  should set this value to the number of cores on your Load
                  Generation Server or SUT if HammerDB is running
                  there.</entry>
                </row>
              </tbody>
            </tgroup>
          </table></para>
      </section>
    </section>

    <section>
      <title>Creating the Schema</title>

      <para>To begin the schema creation select the Build Option from the
      tree-view.</para>

      <figure>
        <title>Build TPC-H Schema</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch13-9.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>A dialog box is shown to confirm the options selected.</para>

      <figure>
        <title>Create Schema</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch13-10.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>When you click Yes HammerDB will login to your chosen service name
      with a monitor thread as the system user and create the user with the
      password you have chosen. It will then log out and log in again as your
      chosen user, create the tables and then load the region and nation table
      data before waiting and monitoring the other threads. The worker threads
      will wait for the monitor thread to complete its initial work.
      Subsequently the worker threads will create and insert the data for
      their assigned warehouses. There are no intermediate data files or
      manual builds required, HammerDB will both create and load your
      requested data dynamically. Data is inserted in a batch format for
      optimal performance, however for larger schemas doing a bulk load of
      data created with the datagen feature will be a faster way to create the
      schema as it bypasses the logging and consistency checks of the insert
      based load.</para>

      <figure>
        <title>Schema Build Start</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch13-11.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>When the workers are complete the monitor thread will create the
      indexes and gather the statistics. When complete Virtual User 1 will
      display the message TPCH SCHEMA COMPLETE and all virtual users will show
      that they completed their action successfully. Pressing the red button
      will destroy the Virtual Users.</para>

      <figure>
        <title>Schema Build Complete</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch13-12.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <section>
        <title>Verifying and Backing-Up the Oracle Schema</title>

        <para>The schema build is now complete with the following tables
        created and populated. Note that in example below the tables have
        inherited the tablespaces’s in-memory configuration without additional
        settings. If required the inmemory_priority can also be set at this
        point in time.</para>

        <programlisting>  1* select table_name, num_rows, inmemory, inmemory_priority from user_tables
SQL&gt; /

TABLE_NAME                       NUM_ROWS INMEMORY INMEMORY_PRIORITY
------------------------------ ---------- -------- -----------------
ORDERS                            1500000 ENABLED  NONE
PARTSUPP                           800000 ENABLED  NONE
CUSTOMER                           150000 ENABLED  NONE
PART                               200000 ENABLED  NONE
SUPPLIER                            10000 ENABLED  NONE
NATION                                 25 ENABLED  NONE
REGION5                                 5 ENABLED  NONE
LINEITEM                          6003632 ENABLED  NONE

</programlisting>

        <para>You can verify the contents with SQL*PLUS as the newly created
        user.</para>

        <programlisting>SQL&gt; select tname, tabtype from tab;

TNAME                          TABTYPE
------------------------------ -------
CUSTOMER                       TABLE
LINEITEM                       TABLE
NATION                         TABLE
ORDERS                         TABLE
PART                           TABLE
PARTSUPP                       TABLE
REGION                         TABLE
SUPPLIER                       TABLE

8 rows selected.

SQL&gt; select * from customer where rownum = 1;

 C_CUSTKEY C_MKTSEGME C_NATIONKEY C_NAME
---------- ---------- ----------- -------------------------
C_ADDRESS C_PHONE  C_ACCTBAL
---------------------------------------- --------------- ----------
C_COMMENT
--------------------------------------------------------------------------------
    112098 AUTOMOBILE       22 Customer#000112098
v,QXkbT8YhhyQYXjX4Ag3iFPQq0gbfZNo7 776-160-1375    5010.19
carefully pending instructions detect slyly-- pending deposits acco

SQL&gt; select index_name, index_type from ind;

INDEX_NAME       INDEX_TYPE
------------------------------
REGION_PK        NORMAL
NATION_PK        NORMAL
SUPPLIER_PK      NORMAL
PARTSUPP_PK      NORMAL
PART_PK          NORMAL
ORDERS_PK        NORMAL
LINEITEM_PK      NORMAL
CUSTOMER_PK      NORMAL

8 rows selected.
</programlisting>

        <para>At this point the data creation is complete and you are ready to
        start running a performance test. Before doing so note that as this is
        a query based workload you have the potential to run multiple tests
        and it will return the same results as the data is not modified during
        tests, however there is one exception, under the Driver Options the
        option to choose a Refresh Function. Further details will be given on
        the refresh function in the next section however at this point it is
        sufficient to note that the refresh function when enabled will modify
        data and no two same refresh functions can be run on the same data
        set. This means if you choose to include a refresh function and then
        attempt to re-run the test you will receive an error as the data has
        been modified. This means you have a number of options. Firstly (and
        recommended) you can use datapump to backup and restore your schema.
        To do this create a directory as follows to reference a file system
        folder you have already created (or use the pre-existing
        DATA_PUMP_DIR)</para>

        <programlisting>SQL&gt; create directory dump_dir1 as '/u02/app/oracle/dumpdir';

Directory created.

Then use datapump to export your schema to this directory before you have run any workloads with a refresh function:
[oracle@MERLIN oracle]$ expdp \"sys/oracle@pdb1 as sysdba\" schemas=tpch content=all directory=DUMP_DIR1 logfile=dp.log

Export: Release 12.1.0.2.0 - Production on Wed Sep 17 11:23:32 2014

Copyright (c) 1982, 2014, Oracle and/or its affiliates.  All rights reserved.

Connected to: Oracle Database 12c Enterprise Edition Release 12.1.0.2.0 - 64bit Production
With the Partitioning, OLAP, Advanced Analytics and Real Application Testing options
Starting "SYS"."SYS_EXPORT_SCHEMA_01":  "sys/********@pdb1 AS SYSDBA" schemas=tpch content=all directory=DUMP_DIR1 logfile=dp.log 
Estimate in progress using BLOCKS method...
Processing object type SCHEMA_EXPORT/TABLE/TABLE_DATA
Total estimation using BLOCKS method: 1.159 GB
Processing object type SCHEMA_EXPORT/USER
Processing object type SCHEMA_EXPORT/SYSTEM_GRANT
Processing object type SCHEMA_EXPORT/ROLE_GRANT
Processing object type SCHEMA_EXPORT/DEFAULT_ROLE
Processing object type SCHEMA_EXPORT/TABLESPACE_QUOTA
Processing object type SCHEMA_EXPORT/PRE_SCHEMA/PROCACT_SCHEMA
Processing object type SCHEMA_EXPORT/TABLE/TABLE
Processing object type SCHEMA_EXPORT/TABLE/INDEX/INDEX
Processing object type SCHEMA_EXPORT/TABLE/CONSTRAINT/CONSTRAINT
Processing object type SCHEMA_EXPORT/TABLE/INDEX/STATISTICS/INDEX_STATISTICS
Processing object type SCHEMA_EXPORT/TABLE/CONSTRAINT/REF_CONSTRAINT
Processing object type SCHEMA_EXPORT/TABLE/STATISTICS/TABLE_STATISTICS
Processing object type SCHEMA_EXPORT/STATISTICS/MARKER
. . exported "TPCH"."LINEITEM"                           746.2 MB 6003632 rows
. . exported "TPCH"."ORDERS"                             165.4 MB 1500000 rows
. . exported "TPCH"."PARTSUPP"                           112.7 MB  800000 rows
. . exported "TPCH"."PART"                               25.99 MB  200000 rows
. . exported "TPCH"."CUSTOMER"                           23.45 MB  150000 rows
. . exported "TPCH"."SUPPLIER"                           1.430 MB   10000 rows
. . exported "TPCH"."NATION"                             9.125 KB      25 rows
. . exported "TPCH"."REGION"                             6.476 KB       5 rows
Master table "SYS"."SYS_EXPORT_SCHEMA_01" successfully loaded/unloaded
******************************************************************************
Dump file set for SYS.SYS_EXPORT_SCHEMA_01 is:
  /u02/app/oracle/dumpdir/expdat.dmp
Job "SYS"."SYS_EXPORT_SCHEMA_01" successfully completed at Wed Sep 17 11:24:10 2014 elapsed 0 00:00:36
</programlisting>

        <para>After you have run a workload with a refresh function drop the
        TPCH user as follows:</para>

        <programlisting>SQL&gt; drop user tpch cascade;

User dropped.
</programlisting>

        <para>Then re-import the export file you took prior to running the
        refresh function:</para>

        <programlisting>[oracle@MERLIN oracle]$ impdp \"sys/oracle@pdb1 as sysdba\" schemas=tpch content=all directory=DUMP_DIR1 logfile=dp1.log

Import: Release 12.1.0.2.0 - Production on Wed Sep 17 11:37:54 2014

Copyright (c) 1982, 2014, Oracle and/or its affiliates.  All rights reserved.

Connected to: Oracle Database 12c Enterprise Edition Release 12.1.0.2.0 - 64bit Production
With the Partitioning, OLAP, Advanced Analytics and Real Application Testing options
Master table "SYS"."SYS_IMPORT_SCHEMA_04" successfully loaded/unloaded
Starting "SYS"."SYS_IMPORT_SCHEMA_04":  "sys/********@pdb1 AS SYSDBA" schemas=tpch content=all directory=DUMP_DIR1 logfile=dp1.log 
Processing object type SCHEMA_EXPORT/USER
Processing object type SCHEMA_EXPORT/SYSTEM_GRANT
Processing object type SCHEMA_EXPORT/ROLE_GRANT
Processing object type SCHEMA_EXPORT/DEFAULT_ROLE
Processing object type SCHEMA_EXPORT/TABLESPACE_QUOTA
Processing object type SCHEMA_EXPORT/PRE_SCHEMA/PROCACT_SCHEMA
Processing object type SCHEMA_EXPORT/TABLE/TABLE
Processing object type SCHEMA_EXPORT/TABLE/TABLE_DATA
. . imported "TPCH"."LINEITEM"                           746.2 MB 6003632 rows
. . imported "TPCH"."ORDERS"                             165.4 MB 1500000 rows
. . imported "TPCH"."PARTSUPP"                           112.7 MB  800000 rows
. . imported "TPCH"."PART"                               25.99 MB  200000 rows
. . imported "TPCH"."CUSTOMER"                           23.45 MB  150000 rows
. . imported "TPCH"."SUPPLIER"                           1.430 MB   10000 rows
. . imported "TPCH"."NATION"                             9.125 KB      25 rows
. . imported "TPCH"."REGION"                             6.476 KB       5 rows
Processing object type SCHEMA_EXPORT/TABLE/INDEX/INDEX
Processing object type SCHEMA_EXPORT/TABLE/CONSTRAINT/CONSTRAINT
Processing object type SCHEMA_EXPORT/TABLE/INDEX/STATISTICS/INDEX_STATISTICS
Processing object type SCHEMA_EXPORT/TABLE/CONSTRAINT/REF_CONSTRAINT
Processing object type SCHEMA_EXPORT/TABLE/STATISTICS/TABLE_STATISTICS
Processing object type SCHEMA_EXPORT/STATISTICS/MARKER
Job "SYS"."SYS_IMPORT_SCHEMA_04" successfully completed at Wed Sep 17 11:38:40 2014 elapsed 0 00:00:44
</programlisting>

        <para>You only need to export once and can then re-import as many
        times as you wish to run the successfully refresh function.</para>

        <para>Secondly another option you have is to use dbms_metadata to
        capture the table definitions and then use SQL*Loader to export and
        import the data using the datagen created data. Finally if you have
        the flashback table feature enabled you can note the time that you
        start running a test with a refresh function and then flashback the
        LINEITEM and ORDERS table to their previous state before the test, for
        example:</para>

        <programlisting>flashback table lineitem to timestamp TO_TIMESTAMP('17-SEP-14 11.41.00.00 AM')</programlisting>

        <para>Whichever method you use, ensure that if you wish to run the
        refresh function you are prepared to restore your schema to the
        previous state before running subsequent tests.</para>
      </section>

      <section>
        <title>Verifying and Backing Up the SQL Server Schema</title>

        <para>Once created you can verify the schema with SSMS or
        sqlcmd.</para>

        <programlisting>C:\Users&gt;sqlcmd -S (local)\SQLDEVELOP -E -Q "use tpch; select name from sys.tables"
Changed database context to 'tpch'.
name

--------------------------------------------------------------------------------
customer
lineitem
nation
part
partsupp
region
supplier
orders

(8 rows affected)

</programlisting>

        <para>At this point the data creation is complete and you are ready to
        start running a performance test. Before doing so note that as this is
        a query based workload you have the potential to run multiple tests
        and it will return the same results as the data is not modified during
        tests, however there is one exception, under the Driver Options the
        option to choose a Refresh Function. Further details will be given on
        the refresh function in the next section however at this point it is
        sufficient to note that the refresh function when enabled will modify
        data and no two same refresh functions can be run on the same data
        set. This means if you choose to include a refresh function and then
        attempt to re-run the test you will receive an error as the data has
        been modified. Therefore you should backup your schema before running
        a workload.</para>

        <figure>
          <title>Backup SQL Server</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch13-13.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>SQL Server will notify when the backup is successful.</para>

        <figure>
          <title>Backup successful</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch13-14.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>The restore can also be selected from the tasks option.</para>

        <figure>
          <title>Restore SQL Server</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch13-15.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>and notification given when the database is restored.</para>

        <figure>
          <title>Restore successful</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch13-16.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>

      <section>
        <title>Verifying and Backing up the Db2 Schema</title>

        <para>The schema build is now complete with the following tables
        created and populated with the example showing a scale factor 10
        schema.</para>

        <programlisting>db2 =&gt; select tabname, card from syscat.tables where tabschema = 'DB2INST1'

TABNAME                                            CARD  
---------------------------------- --------------------
CUSTOMER                                        1500000
LINEITEM                                       60001587
NATION                                               25
ORDERS                                         15000000
PART                                            2000000
PARTSUPP                                        8000000
REGION                                                5
SUPPLIER                                         100000

8 record(s) selected.

db2 =&gt; select * from customer fetch first row only

C_CUSTKEY   C_NAME                    C_ADDRESS     C_NATIONKEY C_PHONE         C_ACCTBAL             C_MKTSEGMENT C_COMMENT                                                                                                             
----------- ------------------------- ---------------------------------------- ----------- --------------- ------------------------ ------------ -------------------------------------------------------
    1128378 Customer#001128378        OzpJsusYMT              6 651-964-1273    +9.57891000000000E+003 HOUSEHOLD    ironic requests above the furiously special foxes wake                                                                

  1 record(s) selected.

db2 =&gt; 

</programlisting>

        <para>At this point the data creation is complete and you are ready to
        start running a performance test. Before doing so note that as this is
        a query based workload you have the potential to run multiple tests
        and it will return the same results as the data is not modified during
        tests, however there is one exception, under the Driver Options the
        option to choose a Refresh Function. Further details will be given on
        the refresh function in the next section however at this point it is
        sufficient to note that the refresh function when enabled will modify
        data and no two same refresh functions can be run on the same data
        set. This means if you choose to include a refresh function and then
        attempt to re-run the test you will receive an error as the data has
        been modified. This means that it is strongly recommended to backup or
        export your data before running a refresh function to ensure that if
        you wish to run the refresh function multiple times you are prepared
        to restore your schema to the previous state before running subsequent
        tests.</para>

        <programlisting>$ db2 backup db tpch to /opt/db2/backup

Backup successful. The timestamp for this backup image is : 20180417181222

$ db2 restore db tpch from /opt/db2/backup replace existing
SQL2539W  The specified name of the backup image to restore is the same as the
name of the target database.  Restoring to an existing database that is the
same as the backup image database will cause the current database to be
overwritten by the backup version.
DB20000I  The RESTORE DATABASE command completed successfully.

</programlisting>

        <para>It is useful to reiterate that before running a query test for
        all configurations you must make the following setting in the
        sb2cli.ini file.</para>

        <programlisting>[db2inst1 cfg]$ more db2cli.ini 
[TPCH]
IgnoreWarnList="'01003'"

</programlisting>

        <para>Failure to add this setting for each virtual user will result in
        the query set failing with the following error.</para>

        <programlisting>Error in Virtual User 3: [IBM][CLI Driver][DB2/NT64] SQLSTATE 01003: Null values were eliminated from the argument of a column function. </programlisting>
      </section>

      <section>
        <title>Verifying and Backing up the MySQL Schema</title>

        <para>The schema build is now complete with the following tables
        created and populated with the example showing a scale factor 1
        schema.</para>

        <programlisting>MariaDB [(none)]&gt; show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| mysql              |
| performance_schema |
| tpch               |
+--------------------+
4 rows in set (0.00 sec)

MariaDB [(none)]&gt; use tpch;
Database changed
MariaDB [tpch]&gt; show tables;
+----------------+
| Tables_in_tpch |
+----------------+
| CUSTOMER       |
| LINEITEM       |
| NATION         |
| ORDERS         |
| PART           |
| PARTSUPP       |
| REGION         |
| SUPPLIER       |
+----------------+
8 rows in set (0.00 sec)</programlisting>

        <para>At this point the data creation is complete and you are ready to
        start running a performance test. Before doing so note that as this is
        a query based workload you have the potential to run multiple tests
        and it will return the same results as the data is not modified during
        tests, however there is one exception, under the Driver Options the
        option to choose a Refresh Function. Further details will be given on
        the refresh function in the next section however at this point it is
        sufficient to note that the refresh function when enabled will modify
        data and no two same refresh functions can be run on the same data
        set. This means if you choose to include a refresh function and then
        attempt to re-run the test you will receive an error as the data has
        been modified. This means that it is strongly recommended to backup or
        export your data before running a refresh function to ensure that if
        you wish to run the refresh function multiple times you are prepared
        to restore your schema to the previous state before running subsequent
        tests.</para>

        <programlisting>./client/mysqldump -u root -pmysql tpch &gt; backup-tpch.sql

./client/mysql -u root -pmysql
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 16
Server version: 10.1.25-MariaDB Source distribution

Copyright (c) 2000, 2017, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]&gt; create database tpch;
Query OK, 1 row affected (0.00 sec)

MariaDB [(none)]&gt; Bye
./client/mysql -u root -pmysql tpch &lt; backup-tpch.sql 

./client/mysql -u root -pmysql
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 18
Server version: 10.1.25-MariaDB Source distribution

Copyright (c) 2000, 2017, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]&gt; use tpch
Database changed
MariaDB [tpch]&gt; show tables;
+----------------+
| Tables_in_tpch |
+----------------+
| CUSTOMER       |
| LINEITEM       |
| NATION         |
| ORDERS         |
| PART           |
| PARTSUPP       |
| REGION         |
| SUPPLIER       |
+----------------+
8 rows in set (0.00 sec)

MariaDB [tpch]&gt; select count(*) from LINEITEM;
+----------+
| count(*) |
+----------+
|  6000385 |
+----------+
1 row in set (0.00 sec)</programlisting>
      </section>

      <section>
        <title>Verifying and Backing up the PostgreSQL Schema</title>

        <para>The schema build is now complete with the following tables
        created and populated with the example showing a scale factor 1
        schema.</para>

        <programlisting>$ ./bin/psql -d tpch
psql (10.1)
Type "help" for help.
tpch=# select relname, n_tup_ins - n_tup_del as rowcount from pg_stat_user_tables;
 relname  | rowcount 
----------+----------
 nation   |       25
 lineitem |  6000773
 orders   |  1497000
 customer |   150000
 region   |        5
 supplier |    10000
 part     |   200000
 partsupp |   800000
(8 rows)

tpch=# 
</programlisting>

        <para>At this point the data creation is complete and you are ready to
        start running a performance test. Before doing so note that as this is
        a query based workload you have the potential to run multiple tests
        and it will return the same results as the data is not modified during
        tests, however there is one exception, under the Driver Options the
        option to choose a Refresh Function. Further details will be given on
        the refresh function in the next section however at this point it is
        sufficient to note that the refresh function when enabled will modify
        data and no two same refresh functions can be run on the same data
        set. This means if you choose to include a refresh function and then
        attempt to re-run the test you will receive an error as the data has
        been modified. This means that it is strongly recommended to backup or
        export your data before running a refresh function to ensure that if
        you wish to run the refresh function multiple times you are prepared
        to restore your schema to the previous state before running subsequent
        tests.</para>

        <programlisting>pgsql$ ./bin/pg_dump tpch &gt; pgdumpfile

pgsql$ ./bin/psql 
psql (10.1)
Type "help" for help.

# drop database tpch;
DROP DATABASE
# drop role tpch;
DROP ROLE

# create user tpch password 'tpch';
CREATE ROLE
# create database tpch owner tpch;
CREATE DATABASE


pgsql$ cat pgdumpfile | ./bin/psql tpch
SET
SET
SET
SET
....</programlisting>
      </section>
    </section>

    <section>
      <title>Configuring Driver Script Options</title>

      <para>To select the driver script options select Options from under the
      Driver Script heading in the tree-view.</para>

      <figure>
        <title>Driver Script Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch13-17.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>This displays the Driver Script Options dialog. The connection
      options are common to the Schema Build Dialog in addition to new Driver
      Options.</para>

      <figure>
        <title>TPC-H Driver Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch13-18.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <table>
        <title>Driver Script Options</title>

        <tgroup cols="2">
          <thead>
            <row>
              <entry align="center">Option</entry>

              <entry align="center">Description</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>Scale Factor</entry>

              <entry>Although not visible under the Driver Options the Scale
              Factor value is also inherited from the Build Options and must
              be the set to the same value for running the Driver Script as
              was used for the Build. This option is required in the Driver
              Script when the refresh function is run to ensure that the data
              is created to insert in the correction location without errors.
              The Scale Factor is also used as input to some queries. This is
              especially important if you have restarted HammerDB as you may
              need to set the Scale Factor in the Build Options again.</entry>
            </row>

            <row>
              <entry>Total Query Sets per User</entry>

              <entry>A Query Set is a sequence of 22 queries. The Total number
              of query sets is the number of times after logging on that the
              virtual user completes an entire sequence of queries before
              logging off again. The difference between this and using
              iterations value in the Virtual User options is that the virtual
              user only logs on and off once and completes all of the query
              sets in between whereas with the iterations value the entire
              script is run multiple times.</entry>
            </row>

            <row>
              <entry>Exit on Database Error</entry>

              <entry>Exit on Database Error is shown as the parameter
              RAISEERROR in the Driver Script. RAISEERROR impacts the
              behaviour of an individual virtual user on detecting a database
              error. If set to TRUE on detecting an error the user will report
              the error into the HammerDB console and then terminate
              execution. If set to FALSE the virtual user will ignore the
              error and proceed with executing the next transaction. It is
              therefore important to be aware that if set to FALSE firstly if
              there has been a configuration error resulting in repeated
              errors then the workload might not be reported accurately and
              secondly you may not be aware of any occasional errors being
              reported as they are silently ignored.</entry>
            </row>

            <row>
              <entry>Verbose Output</entry>

              <entry>Verbose Output is shown as VERBOSE in the Driver Script.
              Setting this value to TRUE will print both the Queries and their
              results for each virtual user however will add to the Query time
              by the time required to print the results.</entry>
            </row>

            <row>
              <entry>Refresh Function</entry>

              <entry>The refresh function checkbox corresponds to refresh_on
              in the Driver Script. When this checkbox is enabled the first
              virtual user will run the refresh function as opposed to running
              a query set. Note that if you choose only one virtual user and
              select the refresh function checkbox then your virtual user will
              run a power test as detailed further in this document. The
              refresh function as the name implies inserts and deletes rows
              from the ORDERS and LINEITEM tables and the times of this
              function are required as input to calculating the QphH.</entry>
            </row>

            <row>
              <entry>Number of Update Sets/Trickle Refresh Delay(ms)/Refresh
              Verbose</entry>

              <entry>If you have enabled the refresh function then the values
              for Number of Update Sets/Trickle Refresh Delay(ms)/Refresh
              Verbose become active and these correspond to update_sets
              trickle_refresh and REFRESH_VERBOSE in the driver script
              respectively. The update sets determines how many times the
              virtual users will cycle through the refresh functions whilst
              noting that the function always starts at 1 and therefore cannot
              be restarted against the same schema until the schema has been
              refreshed. The Trickle Refresh Delay value sets the delay
              between each insert and delete with a default of 1 second
              ensuring that the refresh function does not place a significant
              load on the system, The Refresh Verbose value means that the
              virtual user running the refresh function reports on its
              activities.</entry>
            </row>

            <row>
              <entry>Cloud Analytic Queries (Oracle MySQL PostgreSQL
              Only)</entry>

              <entry>When selected this option loads a driver script that runs
              the sequence of 13 Oracle Cloud Analytic Queries.</entry>
            </row>

            <row>
              <entry>Degree of Parallelism (Oracle, Db2 and PostgreSQL
              Only)</entry>

              <entry>For Oracle Degree of Parallelism defines the number of
              Parallel Execution Server processes that the Queries will be
              executed with. The Degree of Parallelism is defined as the
              degree of parallel in the driver script. You should consult a
              good reference on Parallel Execution as the actual execution
              environment is more complex including both Producer and Consumer
              Parallel Execution Servers. This value will be determined by
              your available hardware resources and may be different for both
              the Power and Throughput tests. HammerDB will ensure that the
              test will run at your chosen degree of parallelism (also
              dependant on your settings of parallel_min and parallel_max
              servers). For Db2 The Degree of Parallelism is the value used
              for the command “SET CURRENT DEGREE” in the driver script and
              determines the level of parallelism used in executing the
              queries. For PostgreSQL the Degree of Parallelism sets the
              max_parallel_workers_per_gather parameter for the sessions
              executing the queries.</entry>
            </row>

            <row>
              <entry>MAXDOP (SQL Server Only)</entry>

              <entry>The MAXDOP setting defines the Maximum degree of
              parallelism to be set as a default on the schema
              objects.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>

    <section>
      <title>Loading the Driver Script</title>

      <para>After selected the Driver Script Options the Driver Script is
      loaded. The configured options can be seen in the Driver Script window
      and also modified directly there. The Load option can also be used to
      refresh the script to the configured Options. Pay particular attention
      to the Scale Factor value shown as "scale_factor" if different from the
      schema that you have loaded.</para>

      <figure>
        <title>Driver Script Loaded</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch13-19.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>

    <section>
      <title>Configure Virtual Users</title>

      <para>Select Virtual User Options from the tree-view.</para>

      <figure>
        <title>Virtual User Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch13-20.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>This displays the Virtual User Options dialog. For Analytic
      workloads it is recommended that only one Virtual User is selected for
      initial testing. If you wish to see the times for individual queries
      rather than just the query set you will also need to write to the
      log.</para>

      <figure>
        <title>Virtual User Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch13-21.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The values have the following meaning.</para>

      <para><table>
          <title>Virtual User Options</title>

          <tgroup cols="2">
            <thead>
              <row>
                <entry align="center">Option</entry>

                <entry align="center">Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>Virtual Users</entry>

                <entry>The number of Virtual Users to create. Note that when
                running a Timed Workload HammerDB will automatically create an
                additional Virtual User to monitor the workload.</entry>
              </row>

              <row>
                <entry>User Delay(ms)</entry>

                <entry>User Delay(ms) defines the time to wait a Virtual User
                will wait behind the previous Virtual User before starting its
                test, this is to prevent a login storm with all Virtual Users
                attempting to login at the same time.</entry>
              </row>

              <row>
                <entry>Repeat Delay(ms)</entry>

                <entry>Repeat Delay(ms) is the time that each Virtual User
                will wait before running its next Iteration of the Driver
                Script. For TPC-H this is an external loop before running
                another query set, however should not be more than 1 when the
                refresh function is enabled.</entry>
              </row>

              <row>
                <entry>Iterations</entry>

                <entry>Iterations is the number of times that the Driver
                Script is run in its entirety.</entry>
              </row>

              <row>
                <entry>Show Output</entry>

                <entry>Show Output will report Virtual User Output to the
                Virtual User Output Window, For TPC-H tests this should be
                enabled.</entry>
              </row>

              <row>
                <entry>Log Output to Temp</entry>

                <entry>When enabled this appends all Virtual User Output to a
                text file in an available temp directory named
                hammerdb.log</entry>
              </row>

              <row>
                <entry>Use Unique Log Name</entry>

                <entry>Use a unique identifier for the Log Name.</entry>
              </row>

              <row>
                <entry>No Log Buffer</entry>

                <entry>By default text log output is buffered in memory before
                being written, this option writes the log output
                immediately.</entry>
              </row>

              <row>
                <entry>Log Timestamps</entry>

                <entry>Add an additional line of output with a timestamp every
                time that the log is written to.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>Select the Virtual User options, Press OK.</para>
    </section>

    <section>
      <title>Run a Single Virtual User Test</title>

      <para>Check that your scale_factor in the Driver Script is the same as
      the schema you are running the test against. You can also set the Degree
      of Parallelism/MAXDOP directly in the script.</para>

      <figure>
        <title>Modified Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch13-24.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Double-click on create Virtual User followed by Run. This will
      proceed to run a single Virtual User with one Query Set.</para>

      <figure>
        <title>Run a single Virtual User Test</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch13-23.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>When complete the Virtual User will show the query set
      time.</para>

      <figure>
        <title>Single Virtual User Complete</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch13-25.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>And the log will show the Query times. Note how the queries are
      run in a pre-determined random order.</para>

      <programlisting>Hammerdb Log @ Thu Apr 19 15:26:25 BST 2018
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-
Vuser 1:Executing Query 14 (1 of 22)
Vuser 1:query 14 completed in 9.449 seconds
Vuser 1:Executing Query 2 (2 of 22)
Vuser 1:query 2 completed in 0.235 seconds
Vuser 1:Executing Query 9 (3 of 22)
Vuser 1:query 9 completed in 16.28 seconds
Vuser 1:Executing Query 20 (4 of 22)
Vuser 1:query 20 completed in 1.254 seconds
Vuser 1:Executing Query 6 (5 of 22)
Vuser 1:query 6 completed in 1.671 seconds
Vuser 1:Executing Query 17 (6 of 22)
Vuser 1:query 17 completed in 0.63 seconds
Vuser 1:Executing Query 18 (7 of 22)
Vuser 1:query 18 completed in 10.106 seconds
Vuser 1:Executing Query 8 (8 of 22)
Vuser 1:query 8 completed in 13.784 seconds
Vuser 1:Executing Query 21 (9 of 22)
Vuser 1:query 21 completed in 46.308 seconds
Vuser 1:Executing Query 13 (10 of 22)
Vuser 1:query 13 completed in 20.612 seconds
Vuser 1:Executing Query 3 (11 of 22)
Vuser 1:query 3 completed in 1.416 seconds
Vuser 1:Executing Query 22 (12 of 22)
Vuser 1:query 22 completed in 0.808 seconds
Vuser 1:Executing Query 16 (13 of 22)
Vuser 1:query 16 completed in 2.0 seconds
Vuser 1:Executing Query 4 (14 of 22)
Vuser 1:query 4 completed in 18.848 seconds
Vuser 1:Executing Query 11 (15 of 22)
Vuser 1:query 11 completed in 6.146 seconds
Vuser 1:Executing Query 15 (16 of 22)
Vuser 1:query 15 completed in 1.886 seconds
Vuser 1:Executing Query 1 (17 of 22)
Vuser 1:query 1 completed in 12.699 seconds
Vuser 1:Executing Query 10 (18 of 22)
Vuser 1:query 10 completed in 5.707 seconds
Vuser 1:Executing Query 19 (19 of 22)
Vuser 1:query 19 completed in 1.534 seconds
Vuser 1:Executing Query 5 (20 of 22)
Vuser 1:query 5 completed in 15.7 seconds
Vuser 1:Executing Query 7 (21 of 22)
Vuser 1:query 7 completed in 6.19 seconds
Vuser 1:Executing Query 12 (22 of 22)
Vuser 1:query 12 completed in 10.954 seconds
Vuser 1:Completed 1 query set(s) in 204 seconds</programlisting>

      <section>
        <title>Changing the Query Order</title>

        <para>For a single virtual User test you may wish to change the query
        order. This query order is predetermined in the common modules.
        However you can redefine this function by copying and pasting the
        ordered_set function and modifying the order. The following example is
        sufficient for the single Virtual User</para>

        <programlisting>rename ordered_set ordered_set_orig
proc ordered_set { myposition } {
if { $myposition &gt; 40 } { set myposition [ expr $myposition % 40 ] }
        set o_s(0)  { 14 2 9 20 6 17 18 8 21 13 3 22 16 4 11 15 1 10 19 5 7 12 }
        return $o_s($myposition)
}</programlisting>

        <para>and then you can change the query order as follows:</para>

        <programlisting>set o_s(0)  { 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 }</programlisting>

        <para>If your database has issues with particular queries being long
        running you can also remove queries this way that you do not wish to
        run.</para>
      </section>
    </section>

    <section>
      <title>Run a Power Test</title>

      <para>Many test environments are sufficient with running single Virtual
      User tests. With available parallel and column store configurations this
      test is sufficient to stress an entire system. Nevertheless a component
      of the TPC-H test is the refresh function and to adhere as closely as
      possible to a TPC-H test the refresh function should be run either side
      of the Power Test. To enable this functionality HammerDB has a special
      power test mode, whereby if refresh_on is set to true as shown and only
      one virtual user is configured then HammerDB will run a Power Test. Note
      that once you selected refresh_on for a single Virtual User in Power
      Test Mode the value of update_sets will be set to 1 and the value of
      trickle_refresh set to 0 and the value of REFRESH_VERBOSE set to false,
      all these values will be set automatically to ensure optimal running of
      the Power Test.</para>

      <figure>
        <title>Power Test Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch13-26.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>When loaded note that the refresh_on option is set in the script.
      You should also ensure that the scale factor setting matches the setting
      for your schema.</para>

      <figure>
        <title>TPC-H refresh on</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch13-28.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>With these settings run the Virtual User and it will run a New
      Sales Refresh, single Virtual User Query Set and Old Sales Refresh in
      order as required by a Power Test.</para>

      <figure>
        <title>Power Test</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch13-27.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>HammerDB will report when the Power Test is complete.</para>

      <figure>
        <title>Power Test Complete</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch13-29.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>and you can collect the refresh and query times from the
      log.</para>

      <programlisting>Hammerdb Log @ Thu Apr 19 16:08:22 BST 2018
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-
Vuser 1:New Sales refresh
Vuser 1:New Sales refresh complete in 46.282 seconds
Vuser 1:Completed 1 update set(s)
Vuser 1:Executing Query 14 (1 of 22)
Vuser 1:query 14 completed in 0.943 seconds
Vuser 1:Executing Query 2 (2 of 22)
Vuser 1:query 2 completed in 0.332 seconds
Vuser 1:Executing Query 9 (3 of 22)
Vuser 1:query 9 completed in 21.281 seconds
Vuser 1:Executing Query 20 (4 of 22)
Vuser 1:query 20 completed in 1.163 seconds
Vuser 1:Executing Query 6 (5 of 22)
Vuser 1:query 6 completed in 1.93 seconds
Vuser 1:Executing Query 17 (6 of 22)
Vuser 1:query 17 completed in 0.504 seconds
Vuser 1:Executing Query 18 (7 of 22)
Vuser 1:query 18 completed in 13.358 seconds
Vuser 1:Executing Query 8 (8 of 22)
Vuser 1:query 8 completed in 11.419 seconds
Vuser 1:Executing Query 21 (9 of 22)
Vuser 1:query 21 completed in 55.767 seconds
Vuser 1:Executing Query 13 (10 of 22)
Vuser 1:query 13 completed in 20.412 seconds
Vuser 1:Executing Query 3 (11 of 22)
Vuser 1:query 3 completed in 2.435 seconds
Vuser 1:Executing Query 22 (12 of 22)
Vuser 1:query 22 completed in 1.011 seconds
Vuser 1:Executing Query 16 (13 of 22)
Vuser 1:query 16 completed in 2.08 seconds
Vuser 1:Executing Query 4 (14 of 22)
Vuser 1:query 4 completed in 18.064 seconds
Vuser 1:Executing Query 11 (15 of 22)
Vuser 1:query 11 completed in 5.83 seconds
Vuser 1:Executing Query 15 (16 of 22)
Vuser 1:query 15 completed in 2.003 seconds
Vuser 1:Executing Query 1 (17 of 22)
Vuser 1:query 1 completed in 13.503 seconds
Vuser 1:Executing Query 10 (18 of 22)
Vuser 1:query 10 completed in 5.548 seconds
Vuser 1:Executing Query 19 (19 of 22)
Vuser 1:query 19 completed in 1.525 seconds
Vuser 1:Executing Query 5 (20 of 22)
Vuser 1:query 5 completed in 9.765 seconds
Vuser 1:Executing Query 7 (21 of 22)
Vuser 1:query 7 completed in 3.69 seconds
Vuser 1:Executing Query 12 (22 of 22)
Vuser 1:query 12 completed in 3.226 seconds
Vuser 1:Completed 1 query set(s) in 196 seconds
Vuser 1:Old Sales refresh
Vuser 1:Old Sales refresh complete in 23.926 seconds
Vuser 1:Completed 1 update set(s)</programlisting>

      <para>Be aware that some databases are considerably better at running
      the refresh functions than others and also that once the power test has
      been run it is necessary to restore the database from backup before
      running the refresh function again. If you fail to do so you will
      receive a constraint violation error. This is expected behaviour.</para>

      <programlisting>Error in Virtual User 1: 23000 2627 {[Microsoft][ODBC Driver 13 for SQL Server][SQL Server]
Violation of PRIMARY KEY constraint 'orders_pk'. 
Cannot insert duplicate key in object 'dbo.orders'. The duplicate key value is (9).}</programlisting>
    </section>

    <section>
      <title>Run a Throughput Test</title>

      <para>After the power test you should run the throughput test (if the
      refresh function has been run it is necessary to refresh the schema).
      For the throughput test you need to also run the refresh function
      however this time the aim is to trickle the refresh function slowly
      while multiple query streams are run. Configure the options as for the
      Power Test and enable the refresh function, this time the update sets,
      trickle refresh and REFRESH_VERBOSE options will also be enabled when
      refresh_on is set to true. Configure the correct number of Virtual Users
      to enable the first Virtual User to run the Refresh Functions and
      additional Virtual Users to run the Query Streams as defined in the
      specification for the test. For the example below at Scale Factor 10
      there are 3 Virtual Users to run the queries and 1 to run the refresh.
      Note that the Refresh Function will run more slowly as expected and all
      of the Virtual Users run the queries in a different order.</para>

      <figure>
        <title>Throughput Test</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch13-30.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <section>
        <title>SQL Server Snapshot Isolation</title>

        <para>Note that before running a long running query at the same time
        as the inserts of the refresh function you should enable snapshot
        isolation on the database. Failure to do so will mean the Query
        streams will hang under a shared lock (LCK_M_S) whilst the refresh
        function is running.</para>

        <figure>
          <title>Enable Snapshot Isolation</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch13-31.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>Once set the refresh and queries will run as expected.</para>

        <figure>
          <title>SQL Server with Snapshot Isolation</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch13-32.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>When the Virtual Users running the Query sets have completed the
        throughput tests, note the longest (not the shortest) time taken for a
        full query set to complete. You do not need to wait for the trickled
        refresh function to complete, however must have configured enough
        update sets to ensure that the refresh function remains running whilst
        the throughput test completes. In the example the value is 494 seconds
        for the slowest query set to complete.</para>

        <figure>
          <title>Throughput test complete</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch13-33.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>
    </section>

    <section>
      <title>Calculate the Geometric Mean</title>

      <para>For comparison of HammerDB TPC-H workloads between systems it is
      recommended to use the geometric mean of the query times. This can be a
      straightforward comparison between power tests. When comparing
      throughput tests it is recommended to compare the geomean of the slowest
      virtual user when comparing throughput tests of an equal number of
      virtual users across systems.</para>

      <programlisting>0.943
0.332
21.281
1.163
1.93
0.504
13.358
11.419
55.767
20.412
2.435
1.011
2.08
18.064
5.83
2.003
13.503
5.548
1.525
9.765
3.69
3.226
SUM 195.789
GEOMEAN 4.011822724
</programlisting>
    </section>
  </chapter>

  <chapter>
    <title>Remote Modes</title>

    <para>HammerDB allows for multiple instances of the HammerDB program to
    run in Master and Slave modes. Running with multiple modes enables the
    additional instances to be controlled by a single master instance either
    on the same load testing server or across the network. This functionality
    can be particularly applicable when testing Virtualized environments and
    the desire is to test multiple databases running in virtualized guests at
    the same time. Similarly this functionality is useful for clustered
    databases with multiple instances such as Oracle Real Application Clusters
    and wishing to partition a load precisely across servers. HammerDB Remote
    Modes are entirely operating system independent and therefore an instance
    of HammerDB running on Windows can be Master to one or more instances
    running on Linux and vice versa. Additionally there is no requirement for
    the workload to be the same and therefore it would be possible to connect
    multiple instances of HammerDB running on Windows and Linux simultaneously
    testing SQL Server, Oracle, MySQL and PostrgreSQL workloads in a
    virtualized environment. In the bottom right hand corner of the interface
    the status bar shows the mode that HammerDB is running in. By default this
    will be Local Mode.</para>

    <figure>
      <title>Mode</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="docs/images/ch11-1.PNG"/>
        </imageobject>
      </mediaobject>
    </figure>

    <sect1>
      <title>Master Mode</title>

      <para>From the tree-view select Mode Options.</para>

      <figure>
        <title>Mode Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch11-2.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>This displays the Mode Options as shown in Figure 3 confirming
      that the current mode is Local.</para>

      <figure>
        <title>Mode Options Select</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch11-3.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Select Master Mode and click OK.</para>

      <figure>
        <title>Master Mode Select</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch11-6.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Confirm the selection.</para>

      <figure>
        <title>Mode Confirmation</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch11-4.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>This will show that Master Mode is now active and the ID and
      hostname it is running on.</para>

      <para><figure>
          <title>Mode Active</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch11-5.PNG"/>
            </imageobject>
          </mediaobject>
        </figure> Note that this will also be recorded in the console display
      and the current Mode displayed in the status bar at the bottom right of
      the Window.</para>

      <programlisting>Setting Master Mode at id : 14296, hostname : osprey</programlisting>
    </sect1>

    <sect1>
      <title>Slave Mode</title>

      <para>On another instance of HammerDB select Slave Mode, enter the id
      and hostname of the master and select OK.</para>

      <para><figure>
          <title>Slave Mode</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch11-7.PNG"/>
            </imageobject>
          </mediaobject>
        </figure></para>

      <para>Confirm the change and observe the Mode connection on both the
      Slave</para>

      <programlisting>Setting Slave Mode at id : 11264, hostname : osprey
Slave connecting to osprey 14296 : Connection suceeded
Master call back successful</programlisting>

      <para>and the Master</para>

      <programlisting>Received a new slave connection from host fe80::69db:2b4d:edd7:962a%16
New slave joined : {11264 osprey}</programlisting>
    </sect1>

    <sect1>
      <title>Master Distribution</title>

      <para>The Master Distribution button in the edit menu now becomes active
      to distribute scripts across instances.</para>

      <figure>
        <title>Master Distribution</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch11-8.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Pressing this button enables the distribution of the contents of
      the Script Editor to all connected instances.</para>

      <programlisting>Distributing to 11264 osprey ...Succeeded</programlisting>

      <para>The contents however may remain to be individually edited on
      remote instances. Note that in particular the OLTP timed tests account
      for the particular mode that an instance is running.</para>

      <para>If loaded locally a script will show the Mode that the instance of
      HammerDB is running in.</para>

      <programlisting>set mode "Slave" ;# HammerDB operational mode</programlisting>

      <para>If distributed it will inherit the mode and will need to be
      changed manually.</para>

      <programlisting>set mode "Master" ;# HammerDB operational mode</programlisting>

      <para>When running in slave Mode the Monitor Virtual User is created but
      idle and does not monitor the workload.</para>

      <para>Now run your workload as normal on the Master, all of your
      workload choices of creating and running and closing down virtual users
      will be replicated automatically on the connected Slaves enabling
      control and simultaneous timing from a central point. However, note that
      running a schema creation with multiple connected instances is not
      supported.</para>

      <para>To disable Remote Modes select Local Mode on the Master on
      confirmation all connected instances will return to Local Mode.</para>
    </sect1>
  </chapter>

  <chapter>
    <title>Generating and Loading Bulk Datasets</title>

    <para>For all workloads HammerDB can create the schema and generate and
    load the data without requiring a staging area, in many circumstances this
    is the preferred method of loading especially for OLTP workloads.
    Nevertheless in some circumstances it is preferable to create the data
    externally as flat files and then use a special database vendor provided
    bulk loading command to load the data into pre-created tables. This option
    may be preferred for example where the target database to load is located
    in the cloud or where the target database has a column structure meaning
    that load performance using batch inserts is poor. Additionally bulk
    loading can enable more flexibility to modify the schema according to
    preference and reload during testing. This chpater details how to generate
    and load large data sets with HammerDB.</para>

    <section>
      <title>Generate the Dataset</title>

      <para>This example for generating the dataset uses SQL Server on
      Windows, however the process is identical when creating data on Linux.
      Firstly create an empty directory that is writable for the user running
      HammerDB and does not contain any existing generated files. HammerDB
      will not overwrite existing generated files.</para>

      <para><figure>
          <title>Data Directory</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch10-1.png"/>
            </imageobject>
          </mediaobject>
        </figure>This point in time is a good stage to validate how much free
      space is in the directory, HammerDB will not check before generating
      data that enough free space exists in the file system before proceeding.
      From the benchmark menu select your chosen database and workload. Note
      that it is particularly important that you select the correct database
      and workload for your environment before generating the data. The data
      generated is different between different databases and workloads. For
      example for optimization purposes the columns may be ordered differently
      between different databases. The data is generated in column order for
      the way that HammerDB generates the schema and data such as time and
      date formats may be different. Errors will result from loading the data
      generated for one database in another without modification.</para>

      <figure>
        <title>Benchmark Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch10-2.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Under the benchmark or Options menus select Datagen and
      Options</para>

      <figure>
        <title>Datagen Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch10-3.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Depending upon whether you have selected a TPC-C or TPC-H
      benchmark under the benchmark options the dialog will be different. For
      the TPC-H options select the Scale Factor, the directory that you have
      pre-created and the number of Virtual Users to generate the
      schema.</para>

      <figure>
        <title>Data Generation Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch10-4.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The data generation offers linear scalability with the key limit
      on performance being the number of CPU cores/threads on the system. Each
      virtual user will use 100% of an available core/thread and therefore it
      should be clear that creating more virtual users than available
      cores/threads is non-productive. Additionally creating the same number
      of virtual users as cores/threads will drive CPU utilisation to 100% -
      therefore select the number of Virtual Users equivalent to the available
      capacity on the system. Similarly it should also be clear that the time
      to create a data set is dependent on the number of available
      cores/threads – a 4 socket or above server with hundreds of
      cores/threads will be able to generate data considerably more quickly
      than a single socket PC or laptop. Finally bear in mind that each
      virtual user will generate a subsection of the data for tables. For
      example selecting 10 virtual users will generate 10 separate files to
      load each table. This approach enables both flexibility and scalability
      in both generating the data but also uploading generated files to the
      cloud and loading data in parallel.</para>

      <figure>
        <title>Multiple files</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch10-5.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>When the chosen values have been selected choose and click the
      Generate button or Generate menu option.</para>

      <figure>
        <title>Generate</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch10-6.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>This displays the Generate Data confirmation. Click Yes.</para>

      <figure>
        <title>Generate Data Confirmation</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch10-7.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>HammerDB will begin to generate the chosen schema in
      parallel.</para>

      <figure>
        <title>Generating Data</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch10-8.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Observe that the CPU utilisation level is in accordance with your
      Virtual User settings.</para>

      <figure>
        <title>CPU Utilisation 100%</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch10-9.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>HammerDB requires no further intervention to generate the data for
      the required schema.</para>

      <figure>
        <title>Schema Generated</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch10-10.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>You may also view the data with a text editor to see that the
      generated data is delimited by a pip character ie “|” and intended NULL
      values are represented by blank or empty data.</para>

      <figure>
        <title>Pipe Delimited Data</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch10-11.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Data is now generated and you are ready to proceed to creating a
      schema and loading the data. It should be clear that once you have
      created a chosen data set then you are able to reuse and reload that
      dataset multiple times and therefore HammerDB enables a quick way to
      generate all of the test data that you require. It is also important to
      note that in some cases databases will support compressing data with
      tools such as zip or gzip prior to loading and therefore if you have a
      large dataset to upload then investigating whether this is an option for
      your database is a worthwhile task.</para>
    </section>

    <section>
      <title>Generate the Dataset with the CLI</title>

      <para>Data Generation can also be run from the command line. As shown
      the dbset command is used to specify the database and benchmark to
      generate data for.</para>

      <programlisting>./hammerdbcli 
HammerDB CLI v3.0
Copyright (C) 2003-2018 Steve Shaw
Type "help" for a list of commands
The xml is well-formed, applying configuration
hammerdb&gt;print datagen
Data Generation set to build a TPC-C schema for Oracle with 1 warehouses with 1 virtual users in /tmp

hammerdb&gt;dbset bm TPC-H
Benchmark set to TPC-H for Oracle

hammerdb&gt;print datagen
Data Generation set to build a TPC-H schema for Oracle with 1 scale factor with 1 virtual users in /tmp

hammerdb&gt;dbset bm TPC-C
Benchmark set to TPC-C for Oracle

hammerdb&gt;print datagen
Data Generation set to build a TPC-C schema for Oracle with 1 warehouses with 1 virtual users in /tmp</programlisting>

      <para>and the Data Generation options set with the command dgset:</para>

      <programlisting>hammerdb&gt;dgset warehouse 10

hammerdb&gt;dgset vu 8
Set virtual users to 8 for data generation

hammerdb&gt;dgset directory "/tmp/TPCCDATA"

hammerdb&gt;print datagen
Data Generation set to build a TPC-C schema for Oracle with 10 warehouses with 8 virtual users in /tmp/TPCCDATA</programlisting>

      <para>The data is then created using the command datagenrun.</para>

      <programlisting>hammerdb&gt;datagenrun
Ready to generate the data for a 10 Warehouse Oracle TPC-C schema
in directory /tmp/TPCCDATA ?
Enter yes or no: replied yes
Vuser 1 created - WAIT IDLE
Vuser 2 created - WAIT IDLE
Vuser 3 created - WAIT IDLE
Vuser 4 created - WAIT IDLE
Vuser 5 created - WAIT IDLE
Vuser 6 created - WAIT IDLE
Vuser 7 created - WAIT IDLE
Vuser 8 created - WAIT IDLE
Vuser 9 created - WAIT IDLE
RUNNING - TPC-C generation
Vuser 1:RUNNING
Vuser 1:Monitor Thread
Vuser 1:Opened File /tmp/TPCCDATA/item_1.tbl
Vuser 1:Generating Item
Vuser 2:RUNNING
Vuser 2:Worker Thread
Vuser 2:Waiting for Monitor Thread...
Vuser 2:Generating 2 Warehouses start:1 end:2
Vuser 2:Start:Mon Apr 09 16:21:36 BST 2018
Vuser 2:Opened File /tmp/TPCCDATA/warehouse_1.tbl
Vuser 2:Opened File /tmp/TPCCDATA/stock_1.tbl
Vuser 2:Opened File /tmp/TPCCDATA/district_1.tbl
Vuser 2:Opened File /tmp/TPCCDATA/customer_1.tbl
Vuser 2:Opened File /tmp/TPCCDATA/history_1.tbl
Vuser 2:Opened File /tmp/TPCCDATA/orders_1.tbl
Vuser 2:Opened File /tmp/TPCCDATA/new_order_1.tbl
Vuser 2:Opened File /tmp/TPCCDATA/order_line_1.tbl
Vuser 2:Generating Warehouse
Vuser 2:Generating Stock Wid=1

...</programlisting>
    </section>

    <section>
      <title>Generate the template database</title>

      <para>Generating a template database is exceptionally straightforward.
      From the HammerDB documentation follow the steps for Build a Schema and
      create the smallest size database such as Scale Factor 1 for TPC-H. This
      database can then be used as a template to capture the DDL. Note that if
      you stop the database creation after the tables are created but before
      all of the data is loaded objects such as indexes will not have been
      created and will not therefore be included in generated DDL, this may or
      may not be an issue for the type of schema you are intending to build,
      for example for a column store such as Amazon Redshift, indexes are not
      a requirement.</para>

      <section>
        <title>Capture and run the table creation DDL</title>

        <para>All of the mainstream databases supported with HammerDB enable
        DDL capture. This can be done as follows for each database. Note that
        at this stage you have the option to modify the DDL for your needs
        such as for partitioning or column orientation.</para>

        <section>
          <title>Oracle</title>

          <para>As the user owning the template database at a sqlplus prompt
          run a GET_DDL SQL statement as follows, noting that you need to set
          the long and pagesize values correctly to view all of the
          output.</para>

          <programlisting>SQL&gt;select DBMS_METADATA.GET_DDL('TABLE','ORDERS') from dual;</programlisting>

          <para>This produces a Create Table statement such as follows:</para>

          <programlisting>CREATE TABLE "TPCH"."ORDERS"                                                 
("O_ORDERDATE" DATE,                                         
"O_ORDERKEY" NUMBER NOT NULL ENABLE,                            
"O_CUSTKEY" NUMBER NOT NULL ENABLE,                            
"O_ORDERPRIORITY" CHAR(15),                                   
"O_SHIPPRIORITY" NUMBER,                                      
"O_CLERK" CHAR(15),                                           
"O_ORDERSTATUS" CHAR(1),                                       
"O_TOTALPRICE" NUMBER,              
"O_COMMENT" VARCHAR2(79),
 CONSTRAINT "ORDERS_PK" PRIMARY KEY ("O_ORDERKEY")
);
</programlisting>

          <para>Joining these files together can then be run against the
          database to create the schema of empty tables:</para>

          <programlisting>sqlplus tpch/tpch
SQL*Plus: Release 12.1.0.2.0 Production
Copyright (c) 1982, 2014, Oracle.  All rights reserved.
Connected to:
Oracle Database 12c Enterprise Edition Release 12.1.0.2.0
SQL&gt; @tpch_tables.sql
Table created.
Table created.
Table created.
Table created.
Table created.
Table created.
Table created.
Table created.
SQL&gt;
</programlisting>
        </section>

        <section>
          <title>SQL Server</title>

          <para>Within SQL Server Management Studio right click your chosen
          table, select “Script Table as” followed by “CREATE To” and choose
          your destination for the DDL. This produces DDL to create your table
          such as follows. Create a single file containing all of your DDL
          statements and click on Execute (F5) under SQL Server Management
          Studio.</para>

          <figure>
            <title>SQL Server Create Table</title>

            <mediaobject>
              <imageobject>
                <imagedata fileref="docs/images/ch10-12.png"/>
              </imageobject>
            </mediaobject>
          </figure>
        </section>

        <section>
          <title>Db2</title>

          <para>For Db2 use the db2look command, this can generate the DDL for
          all objects within a schema with one command.</para>

          <programlisting>db2look -d TPCH -a -e -x -o tpchcreate.sql
-- Generate statistics for all creators 
-- Creating DDL for table(s)
-- Output is sent to file: tpchcreate.sql
-- Binding package automatically ... 
-- Bind is successful
-- Binding package automatically ... 
-- Bind is successful
------------------------------------------------
-- DDL Statements for Table "DB2INST1"."ORDERS"
------------------------------------------------
</programlisting>

          <para>The output file will contain output as follows:</para>

          <programlisting>
CREATE TABLE "DB2INST1"."ORDERS"  (
  "O_ORDERKEY" INTEGER NOT NULL , 
  "O_CUSTKEY" INTEGER NOT NULL , 
  "O_ORDERSTATUS" CHAR(1 OCTETS) NOT NULL , 
  "O_TOTALPRICE" DOUBLE NOT NULL , 
  "O_ORDERDATE" DATE NOT NULL , 
  "O_ORDERPRIORITY" CHAR(15 OCTETS) NOT NULL , 
  "O_CLERK" CHAR(15 OCTETS) NOT NULL , 
  "O_SHIPPRIORITY" INTEGER , 
  "O_COMMENT" VARCHAR(79 OCTETS) NOT NULL )   
 IN "USERSPACE1"  
 ORGANIZE BY ROW; 
</programlisting>

          <para>Run the file as follows:</para>

          <programlisting>db2 -tvf tpchcreate.sql
CONNECT TO TPCH

   Database Connection Information

 Database server        = DB2/LINUXX8664 11.1.0
 SQL authorization ID   = DB2INST1
 Local database alias   = TPCH2

CREATE SCHEMA "DB2INST1"
DB20000I  The SQL command completed successfully.

CREATE TABLE "DB2INST1"."ORDERS"  ( "O_ORDERKEY" INTEGER NOT NULL , "O_CUSTKEY" INTEGER NOT NULL , "O_ORDERSTATUS" CHAR(1 OCTETS) NOT NULL , "O_TOTALPRICE" DOUBLE NOT NULL , "O_ORDERDATE" DATE NOT NULL , "O_ORDERPRIORITY" CHAR(15 OCTETS) NOT NULL , "O_CLERK" CHAR(15 OCTETS) NOT NULL , "O_SHIPPRIORITY" INTEGER , "O_COMMENT" VARCHAR(79 OCTETS) NOT NULL ) IN "USERSPACE1" ORGANIZE BY ROW
DB20000I  The SQL command completed successfully.
</programlisting>
        </section>

        <section>
          <title>MySQL</title>

          <para>For MySQL use the show create table command. Be aware that if
          foreign keys are defined at this stage they will significantly
          impact load performance.</para>

          <programlisting>mysql&gt; use tpch;
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Database changed
mysql&gt; show tables;
+----------------+
| Tables_in_tpch |
+----------------+
| CUSTOMER       |
| LINEITEM       |
| NATION         |
| ORDERS         |
| PART           |
| PARTSUPP       |
| REGION         |
| SUPPLIER       |
+----------------+
8 rows in set (0.00 sec)

mysql&gt; show create table SUPPLIER;

CREATE TABLE `SUPPLIER` (
  `S_SUPPKEY` int(11) NOT NULL,
  `S_NATIONKEY` int(11) DEFAULT NULL,
  `S_COMMENT` varchar(102) CHARACTER SET latin1 COLLATE latin1_bin DEFAULT NULL,
  `S_NAME` char(25) CHARACTER SET latin1 COLLATE latin1_bin DEFAULT NULL,
  `S_ADDRESS` varchar(40) CHARACTER SET latin1 COLLATE latin1_bin DEFAULT NULL,
  `S_PHONE` char(15) CHARACTER SET latin1 COLLATE latin1_bin DEFAULT NULL,
  `S_ACCTBAL` decimal(10,2) DEFAULT NULL,
  PRIMARY KEY (`S_SUPPKEY`),
  KEY `SUPPLIER_FK1` (`S_NATIONKEY`)
) ENGINE=MyISAM DEFAULT CHARSET=latin1;
</programlisting>

          <para>Create a file containing all of the table creation statements
          and run as follows:</para>

          <programlisting>sql&gt; use tpch
Database changed
mysql&gt; source /home/mysql/TPCHDATA/createtpch.sql
Query OK, 0 rows affected (0.08 sec)
Query OK, 0 rows affected (0.07 sec)
Query OK, 0 rows affected (0.05 sec)
Query OK, 0 rows affected (0.05 sec)
Query OK, 0 rows affected (0.05 sec)
Query OK, 0 rows affected (0.06 sec)
Query OK, 0 rows affected (0.05 sec)
Query OK, 0 rows affected (0.05 sec)
mysql&gt;
</programlisting>
        </section>

        <section>
          <title>PostgreSQL/Amazon Redshift</title>

          <para>To create the DLL for PostgreSQL or Amazon Redshift (note you
          can create a template local PostgreSQL database and the DDL is 100%
          compatible to create a database in Redshift) use the pg_dump command
          as follows:</para>

          <programlisting>pg_dump -U postgres -h localhost tpch -t table_name --schema-only -f table.sql</programlisting>

          <para>On Linux systems you can use the bash shell to generate the
          DDL for all tables with one command, for example:</para>

          <programlisting>for sys in customer lineitem nation orders part partsupp region supplier; do pg_dump -U postgres -h localhost tpch -t $sys --schema-only -f $sys.sql; done</programlisting>

          <para>This generates a series of files containing the required DDL
          as follows:</para>

          <programlisting>CREATE TABLE customer (
    c_custkey numeric NOT NULL,
    c_mktsegment character(10),
    c_nationkey numeric,
    c_name character varying(25),
    c_address character varying(40),
    c_phone character(15),
    c_acctbal numeric,
    c_comment character varying(118)
);
</programlisting>

          <para>Run the files as follows under PostgreSQL or Redshift to
          create the desired tables. The following example create the schema
          on PostgreSQL</para>

          <programlisting>psql -d tpch -f pgtpchtables.sql</programlisting>

          <para>and the following on Redshift</para>

          <programlisting>bash-4.2$ psql -h tpch-instance.xxxxxxxxxxxx.eu-west-1.redshift.amazonaws.com -U postgres -d tpch -p 5439 -f pgtpchtables.sql
Password for user postgres: 
CREATE TABLE
CREATE TABLE
CREATE TABLE
CREATE TABLE
CREATE TABLE
CREATE TABLE
CREATE TABLE
CREATE TABLE
-bash-4.2$ psql -h tpch-instance.xxxxxxxxxxxx.eu-west-1.redshift.amazonaws.com -U postgres -d tpch -p 5439
Password for user postgres: 
psql (9.2.15, server 8.0.2)
WARNING: psql version 9.2, server version 8.0.
         Some psql features might not work.
SSL connection (cipher: ECDHE-RSA-AES256-GCM-SHA384, bits: 256)
Type "help" for help.

tpch=# \d
          List of relations
 schema |   name   | type  |  owner   
--------+----------+-------+----------
 public | customer | table | postgres
 public | lineitem | table | postgres
 public | nation   | table | postgres
 public | orders   | table | postgres
 public | part     | table | postgres
 public | partsupp | table | postgres
 public | region   | table | postgres
 public | supplier | table | postgres
(8 rows)
</programlisting>
        </section>
      </section>
    </section>

    <section>
      <title>Run the bulk data load</title>

      <para>With the schemas created you can proceed to bulk load the data you
      have created without the overhead of features such as logging associated
      with inserts. This section details example methods by which data can be
      bulk loaded. Note that some of these databases support multiple
      different methods to bulk load data and this section gives examples
      using either the most straightforward or widely available tools,
      therefore additional methods may exists to load your data.</para>

      <section>
        <title>Oracle</title>

        <para>SQL*Loader is the default method for loading Oracle with
        external data. SQL*Loader has the advantage of flexibility in being
        adaptable to loading data in many different formats. To use SQL*Loader
        firstly create a control file. The following example shows a control
        file for the ORDERS table from the TPCH schema. Firstly note that the
        control can accept wildcard characters and therefore multiple files
        can be loaded with one command. Also note how the DATE format has been
        specified.</para>

        <programlisting>more sqlldr-orders.ctl
load data
infile '/home/oracle/TPCCDATA/orders_*.tbl'
into table orders
fields terminated by "|"
(O_ID,O_W_ID,O_D_ID,O_C_ID,O_CARRIER_ID,O_OL_CNT,O_ALL_LOCAL,O_ENTRY_D DATE "YYY
YMMDDHH24MISS")
</programlisting>

        <para>A further different date formatting example can be seen for the
        LINEITEM table in the TPCH schema.</para>

        <programlisting>more sqlldr-lineitem.ctl
load data
infile '/home/oracle/TPCHDATA/lineitem_*.tbl'
into table lineitem
fields terminated by "|"
(L_SHIPDATE DATE "yyyy-mon-dd",L_ORDERKEY,L_DISCOUNT ,L_EXTENDEDPRICE,L_SUPPKEY,L_QUANTITY,L_RETURNFLAG,L_PARTKEY,L_LINESTATUS,L_TAX,L_COMMITDATE DATE "yyyy-mon-dd", L_RECEIPTDATE DATE "yyyy-mon-dd",L_SHIPMODE, L_LINENUMBER, L_SHIPINSTRUCT, L_COMMENT)
</programlisting>

        <para>Now run SQL*Loader specifying the control file and username and
        password.</para>

        <programlisting>sqlldr tpch/tpch control=/home/oracle/sqlldr-orders.ctl direct=true</programlisting>
      </section>

      <section>
        <title>SQL Server</title>

        <para>For SQL Server use a bulk insert state as follows. SQL Server
        does not recognize wildcard characters for bulk insert however is
        adaptable in recognizing both NULLS and various date formats by
        default.</para>

        <programlisting>BULK INSERT customer FROM 'C:\TEMP\TPCHDATA\customer_1.tbl' WITH (TABLOCK, DATAFILETYPE='char', CODEPAGE='raw', FIELDTERMINATOR = '|')</programlisting>

        <para>Run the bulk insert commands via SQL Server Management
        studio.</para>

        <figure>
          <title>SQL Server Bulk Insert</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch10-13.png"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>

      <section>
        <title>Db2</title>

        <para>For DB2 firstly connect to your database and then use the db2
        load command. The delimiter is specified by 0x7c as the ASCII
        character for vertical bar as the delimiter used.</para>

        <programlisting>$ db2 connect to tpch

Database Connection Information

Database server        = DB2/LINUXX8664 11.1.0
SQL authorization ID   = DB2INST1
Local database alias   = TPCH


db2 load from /home/db2inst1/TPCHDATA/nation_1.tbl of DEL MODIFIED BY DELPRIORITYCHAR COLDEL0x7c insert INTO nation
</programlisting>

        <para>This command is easy to script in Linux environments to load all
        available files:</para>

        <programlisting>$ for sys in `ls -1 customer_*.tbl`; do db2 load from /home/db2inst1/TPCHDATA/$sys of DEL MODIFIED BY DELPRIORITYCHAR COLDEL0x7c insert INTO customer; done

</programlisting>

        <para>Where a date is specified the dateformat must be given as
        follows:</para>

        <programlisting>db2 load from /home/db2inst1/TPCHDATA/lineitem_1.tbl of DEL MODIFIED BY DELPRIORITYCHAR COLDEL0x7c DATEFORMAT=\"YYYY-MMM-DD\" insert INTO lineitem</programlisting>

        <para>As with Oracle and SQL Server Db2 automatically recognises NULL
        values in the data.</para>
      </section>

      <section>
        <title>MySQL</title>

        <para>Bulk loading is done in MySQL using the load data infile
        command. This can be done as follows:</para>

        <programlisting>mysql&gt; load data infile '/home/mysql/TPCHDATA/supplier_1.tbl' INTO table SUPPLIER fields terminated by '|';</programlisting>

        <para>Load data infile does not offer enable a method by which
        different date formats can be specified however this can be achieved
        by specifying an additional SET command as shown for the LINEITEM
        table:</para>

        <programlisting>load data infile '/home/mysql/TPCHDATA/lineitem_1.tbl' INTO table LINEITEM fields terminated by '|'
(@dt1, L_ORDERKEY, L_DISCOUNT, L_EXTENDEDPRICE, L_SUPPKEY, L_QUANTITY, L_RETURNFLAG, L_PARTKEY, L_LINESTATUS, L_TAX, @dt2, @dt3, L_SHIPMODE, L_LINENUMBER, L_SHIPINSTRUCT, L_COMMENT)
set L_SHIPDATE = STR_TO_DATE(@dt1, '%Y-%b-%d'),L_COMMITDATE = STR_TO_DATE(@dt2, '%Y-%b-%d'),L_RECEIPTDATE = STR_TO_DATE(@dt3, '%Y-%b-%d');
</programlisting>

        <para>and a SET command as follows shown for the ORDERS table:</para>

        <programlisting>load data infile '/home/mysql/TPCHDATA/orders_1.tbl' INTO table ORDERS fields terminated by '|'
(@dt1, O_ORDERKEY, O_CUSTKEY, O_ORDERPRIORITY, O_SHIPPRIORITY, O_CLERK, O_ORDERSTATUS, O_TOTALPRICE, O_COMMENT)
set O_ORDERDATE = STR_TO_DATE(@dt1, '%Y-%b-%d');
</programlisting>

        <para>Add these commands to a file:</para>

        <programlisting>load data infile '/home/mysql/TPCHDATA/customer_1.tbl' INTO table CUSTOMER fields terminated by '|';
load data infile '/home/mysql/TPCHDATA/customer_2.tbl' INTO table CUSTOMER fields terminated by '|';
load data infile '/home/mysql/TPCHDATA/customer_3.tbl' INTO table CUSTOMER fields terminated by '|';
load data infile '/home/mysql/TPCHDATA/customer_4.tbl' INTO table CUSTOMER fields terminated by '|';
load data infile '/home/mysql/TPCHDATA/customer_5.tbl' INTO table CUSTOMER fields terminated by '|';
load data infile '/home/mysql/TPCHDATA/customer_6.tbl' INTO table CUSTOMER fields terminated by '|';
</programlisting>

        <para>and run as follows:</para>

        <para><programlisting>mysql&gt; source /home/mysql/TPCHDATA/loadfiles.sql
Query OK, 150000 rows affected (0.74 sec)
Records: 150000  Deleted: 0  Skipped: 0  Warnings: 0

Query OK, 150000 rows affected (1.56 sec)
Records: 150000  Deleted: 0  Skipped: 0  Warnings: 0
</programlisting>For the MySQL TPC-C schema NULLS are not automatically
        recognised and a SET command is required as follows for the ORDER_LINE
        and ORDERS table:</para>

        <programlisting>load data infile '/home/mysql/TPCCDATA/order_line_1.tbl' INTO table order_line fields terminated by '|'
(ol_w_id, ol_d_id, ol_o_id, ol_number, ol_i_id, @dt1, ol_amount, ol_supply_w_id, ol_quantity, ol_dist_info)
set ol_delivery_d = nullif(@dt1,'');

load data infile '/home/mysql/TPCCDATA/orders_1.tbl' INTO table orders fields terminated by '|'
(o_id, o_w_id, o_d_id, o_c_id, @id1, o_ol_cnt, o_all_local, o_entry_d)
set o_carrier_id = nullif(@id1,'');
</programlisting>
      </section>

      <section>
        <title>PostgreSQL/Amazon Redshift</title>

        <para>Both PostgreSQL and Amazon Redshift use the copy command to bulk
        load data, however Redshift has additional requirements to load the
        data into the cloud. For PostgreSQL make a file with the copy commands
        for all tables for example:</para>

        <programlisting>\copy customer from '/home/postgres/TPCHDATA/customer_1.tbl' WITH DELIMITER AS '|';
\copy customer from '/home/postgres/TPCHDATA/customer_2.tbl' WITH DELIMITER AS '|';
\copy customer from '/home/postgres/TPCHDATA/customer_3.tbl' WITH DELIMITER AS '|';
\copy customer from '/home/postgres/TPCHDATA/customer_4.tbl' WITH DELIMITER AS '|';
\copy customer from '/home/postgres/TPCHDATA/customer_5.tbl' WITH DELIMITER AS '|';
\copy customer from '/home/postgres/TPCHDATA/customer_6.tbl' WITH DELIMITER AS '|';
\copy customer from '/home/postgres/TPCHDATA/customer_7.tbl' WITH DELIMITER AS '|';
\copy customer from '/home/postgres/TPCHDATA/customer_8.tbl' WITH DELIMITER AS '|';

</programlisting>

        <para>And run the script to copy the files</para>

        <programlisting>psql -U postgres -d tpch -f TPCHCOPY.sql</programlisting>

        <para>With PostgreSQL additional lines are required to handle NULL
        value for the TPC-C schema as follows:</para>

        <programlisting>\copy order_line from '/home/postgres/TPCCDATA/order_line_1.tbl' WITH NULL AS '' DELIMITER AS '|';
\copy orders from '/home/postgres/TPCCDATA/orders_1.tbl' WITH NULL AS '' DELIMITER AS '|';
</programlisting>

        <para>For Amazon Redshift firstly upload the generated files to an
        Amazon S3 bucket. As noted previously Amazon S3 is one of the
        databases that supports loading from a compressed file and therefore
        you may wish to convert the files to a compressed format such as gzip
        before uploading.</para>

        <figure>
          <title>Upload to S3</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch10-14.png"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>Under the AWS IAM Console create a user for uploading and under
        security credentials create and download an access key. Note that the
        access keys have been removed from the image.</para>

        <figure>
          <title>Postgres User Access Keys</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch10-15.png"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>Finally give the postgres user permission to access Amazon
        S3.</para>

        <figure>
          <title>S3 Permissions</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch10-16.png"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>Now connect to Redshift using the PostgreSQL command line tool
        and run the copy command specifying the location of the S3 bucket and
        the CREDENTIALs option where the XXX characters are replaced by the
        access key and secure access key you created previously.</para>

        <programlisting>-bash-4.2$ psql -h tpch-instance.xxxxxxxxxxxx.eu-west-1.redshift.amazonaws.com -U postgres -d tpch -p 5439
Password for user postgres: 
psql (9.2.15, server 8.0.2)
WARNING: psql version 9.2, server version 8.0.
         Some psql features might not work.
SSL connection (cipher: ECDHE-RSA-AES256-GCM-SHA384, bits: 256)
Type "help" for help.

tpch=# copy region from 's3://s3bucket/load/region_1.tbl' CREDENTIALS 'aws_access_key_id=XXXXXXXXXXXXXXXXXXXX;aws_secret_access_key=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX' delimiter '|';
INFO:  Load into table 'region' completed, 5 record(s) loaded successfully.
COPY
</programlisting>

        <para>Note that if you specify part of the filename Redshift will
        upload all of the files with the same prefix.</para>

        <programlisting>copy customer from 's3://s3bucket/load/customer_' CREDENTIALS 'aws_access_key_id=XXXXXXXXXXXXXXXXXXXX;aws_secret_access_key=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX' delimiter '|';
INFO:  Load into table 'customer' completed, 150000 record(s) loaded successfully.
</programlisting>

        <para>For NULL values and date and time formats you can specify the
        formats for load as follows:</para>

        <programlisting>tpch=# copy orders from 's3://s3bucket/load/orders_' CREDENTIALS 'aws_access_key_id=XXXXXXXXXXXXXXXXXXXX;aws_secret_access_key=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX' emptyasnull blanksasnull delimiter '|' timeformat 'YYYY-MON-DD HH:MI:SS';
INFO:  Load into table 'orders' completed, 1500000 record(s) loaded successfully.
COPY
</programlisting>

        <para>Note that as a column store Redshift does not require the
        additional indexes or constraints of a traditional row store
        format.</para>
      </section>
    </section>
  </chapter>

  <chapter>
    <title>Oracle Trace File Replay</title>

    <para>Oracle Trace Files can be converted and replayed against an Oracle
    database. Within HammerDB this trace file replay functionality is only
    available with Oracle as the other supported databases do not provide an
    interface to generating a detailed trace of workloads.</para>

    <sect1>
      <title>Generating Trace Files</title>

      <para>To begin converting Oracle trace file workloads with HammerDB the
      first step is to load an Oracle trace file under the File:Open menu
      option. Before doing this therefore an Oracle trace file must be
      generated using Oracle Event 10046. (There are numerous methods to
      generate Oracle trace files of which only event 10046, level 4 to
      capture bind variables is covered here). As an example the simplest
      method to start and stop the trace of a session interactively is as
      follows:</para>

      <programlisting>SQL&gt; connect / as sysdba
Connected.
SQL&gt; alter session set events '10046 trace name context forever, level 4'; 

Session altered.

SQL&gt; select sysdate from dual;

SYSDATE
---------
10-APR-18

SQL&gt; alter session set events '10046 trace name context off';

Session altered.

SQL&gt; 
</programlisting>

      <para>For more advanced use the creation of a logon trigger is
      recommended. This trigger can then be enabled or disabled to capture the
      trace information for a particular use. The example uses the user TPCC
      created for an Oracle HammerDB OLTP test.</para>

      <programlisting>create or replace trigger logon_trigger
after logon on database
begin
if (user = 'TPCC') then
execute immediate
'alter session set events ''10046 trace name context forever, level 4''';
end if;
end;</programlisting>

      <para>This will then create a tracefile automatically at logon.</para>

      <programlisting>SQL&gt; connect tpcc/tpcc@RVDB1
Connected.
SQL&gt; select sysdate from dual;

SYSDATE
---------
10-APR-18
</programlisting>

      <para>The trigger must be created as SYS with SYSDBA privileges, if
      created by system the trigger will create successfully but fail on the
      user login. This event will produce a trace file in the diagnostic area
      specified for the database server. By default the file will be
      identifiable by ora_SPID.trc, however there are also methods that can be
      used to set the name of the trace file. Note that in a Shared Server
      environment (previously MTS) one users’ session may be distributed
      across numerous trace files as the user processes share multiple server
      processes. Therefore in this environment it is necessary to reassemble
      the trace file data before converting with the Oracle utility ‘trcsess’.
      A a raw trace file is shown below:</para>

      <programlisting>race file /home/oracle/app/oracle/diag/rdbms/rvdb1/RVDB1/trace/RVDB1_ora_13783.trc
Oracle Database 12c Enterprise Edition Release 12.1.0.2.0 - 64bit Production
With the Partitioning, OLAP, Advanced Analytics and Real Application Testing options
ORACLE_HOME = /home/oracle/app/oracle/product/12.1.0/dbhome_1
System name:Linux
Node name:raven
Release:4.1.12-61.1.23.el7uek.x86_64
Version:#2 SMP Tue Dec 20 16:59:23 PST 2016
Machine:x86_64
Instance name: RVDB1
Redo thread mounted by this instance: 1
Oracle process number: 72
Unix process pid: 13783, image: oracle@raven (TNS V1-V3)


*** 2018-04-10 14:35:39.821
*** SESSION ID:(4.48241) 2018-04-10 14:35:39.821
*** CLIENT ID:() 2018-04-10 14:35:39.821
*** SERVICE NAME:(SYS$USERS) 2018-04-10 14:35:39.821
*** MODULE NAME:(sqlplus@raven (TNS V1-V3)) 2018-04-10 14:35:39.821
*** CLIENT DRIVER:(SQL*PLUS) 2018-04-10 14:35:39.821
*** ACTION NAME:() 2018-04-10 14:35:39.821
 
CLOSE #140169830640712:c=0,e=6,dep=0,type=1,tim=530154237
=====================
PARSING IN CURSOR #140169830722080 len=24 dep=0 uid=0 oct=3 lid=0 tim=530194337 hv=2343063137 ad='97b9c9e8' sqlid='7h35uxf5uhmm1'
select sysdate from dual
END OF STMT
PARSE #140169830722080:c=3149,e=39623,p=0,cr=0,cu=0,mis=1,r=0,dep=0,og=1,plh=1388734953,tim=530194336
EXEC #140169830722080:c=0,e=21,p=0,cr=0,cu=0,mis=0,r=0,dep=0,og=1,plh=1388734953,tim=530194497
FETCH #140169830722080:c=0,e=16,p=0,cr=0,cu=0,mis=0,r=1,dep=0,og=1,plh=1388734953,tim=530194568
STAT #140169830722080 id=1 cnt=1 pid=0 pos=1 obj=0 op='FAST DUAL  (cr=0 pr=0 pw=0 time=0 us cost=2 size=0 card=1)'
FETCH #140169830722080:c=0,e=1,p=0,cr=0,cu=0,mis=0,r=0,dep=0,og=0,plh=1388734953,tim=530213687

*** 2018-04-10 14:36:07.973
CLOSE #140169830722080:c=539,e=34,dep=0,type=0,tim=558307025
=====================
PARSING IN CURSOR #140169830722080 len=55 dep=0 uid=0 oct=42 lid=0 tim=558307224 hv=2217940283 ad='0' sqlid='06nvwn223659v'
alter session set events '10046 trace name context off'
END OF STMT
PARSE #140169830722080:c=0,e=112,p=0,cr=0,cu=0,mis=0,r=0,dep=0,og=0,plh=0,tim=558307223
EXEC #140169830722080:c=0,e=214,p=0,cr=0,cu=0,mis=0,r=0,dep=0,og=0,plh=0,tim=558307510</programlisting>

      <para>For more information on the trace file format the document
      Note:39817.1 Subject “Interpreting Raw SQL_TRACE output “ available from
      My Oracle Support a, however this knowledge is not essential as HammerDB
      can convert this raw format into a form that can be replayed.</para>

      <figure>
        <title>Doc 39817.1</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch12-1.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>
    </sect1>

    <sect1>
      <title>Converting Oracle Trace Files</title>

      <para>It is important to note that to convert Oracle trace files you
      must have selected Oracle from the treeview. If another database is
      selected the button to convert Oracle trace files is disabled Copy the
      trace file to the client machine or location where HammerDB is running
      and use the File:Open menu option or the “Open an existing file button”
      under the Edit Menu to display the Open File dialogue</para>

      <figure>
        <title>Open File</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch12-2.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The Open File dialog allows the specifying of the Directory and a
      filter for the file type, by default this is *.tcl. Change the file
      extension to ‘trc’ and change directory to the location of your files,
      select the trace file you previously generated and select OK.</para>

      <figure>
        <title>Trace Loaded</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch12-3.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Select the Trace Conversion button at the bottom of the Edit
      menu</para>

      <figure>
        <title>Convert Trace</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch12-4.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>and the trace file is converted into a format that can replayed
      against the database.</para>

      <figure>
        <title>Trace Converted</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch12-5.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Note that the trace file never records a users’ authentication
      details. Therefore the connect string must always be modified manually
      after conversion. Remove the comment before the “set connect” line on
      line 4 and enter the correct username and password. The SID will be set
      by default to the SID that the trace file was taken from and therefore
      if using a pluggable database then the correct container must also be
      set to the correct identifier. Once the connect string is updated the
      generated script is ready for running against the database and contains
      the original statements that were traced. The save menu option or “Save
      current file” button can be used to save the generated script for
      reloading at a later point in time.</para>
    </sect1>

    <sect1>
      <title>Replaying Oracle Trace Files</title>

      <para>Next to the “Convert” button there is a “Test current code”
      button. Click on this to test the code in an individual Virtual User
      environment. Once tested the window can be closed manually or by
      clicking the same button now containing a stop image.</para>

      <para><figure>
          <title>Run Trace</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch12-8.png"/>
            </imageobject>
          </mediaobject>
        </figure></para>

      <para>Once the script has been tested and is ready for running increase
      the number of Virtual Users and run as for an OLTP test.</para>

      <figure>
        <title>Multiuser Test</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch12-6.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The tracefile has now been replayed with multiple Virtual Users
      illustrating the basic building blocks for creating a bespoke Oracle
      workload from a captured and converted trace file. As a further examples
      using the same logon trigger for the TPCC user the HammerDB OLTP
      workload has now been loaded and for example purposes the number of
      transactions set to 10. Once converted HammerDB will correctly format,
      extract and insert bind variables for workload replay.</para>

      <figure>
        <title>Bind Variables</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch12-7.png"/>
          </imageobject>
        </mediaobject>
      </figure>
    </sect1>

    <sect1>
      <title>Capturing Errors from Trace File Workloads</title>

      <para>It must be remembered that although the previous workloads shown
      are suitable for directly replaying, with some applications replaying
      data from a previous transaction may result in constraint violations or
      updates of data that no longer exists that results in errors during
      replay. One of the most common error messages received is is the
      standard PL/SQL failure reported by Oratcl oraplexec: SQL execution
      failed. To get the actual Oracle error message underlying this error you
      can use the TCL "catch" command to capture the error and print it out
      inline. For example if you wanted to see the error in the neword
      procedure of the TPC-C script change the oraplexec line to look like the
      following :</para>

      <programlisting>if {[ catch {oraplexec $curn1 $sql5 :no_w_id $no_w_id :no_max_w_id $w_id_input :no_d_id $no_d_id :no_c_id $no_c_id :no_o_ol_cnt $ol_cnt :no_c_discount {NULL} :no_c_last {NULL} :no_c_credit {NULL} :no_d_tax {NULL} :no_w_tax {NULL} :no_d_next_o_id {0} :timestamp $date} message]} {
puts $message
puts [ oramsg $curn1 all ]
}
</programlisting>

      <para>So you are adding the statements as below to the oraplexec
      statement</para>

      <programlisting>if {[ catch { ... } message] } {
puts $message
puts [ oramsg $curn1 all ]
}
</programlisting>

      <para>Note that the cursor variable $curn1 used in oramsg is the same
      variable used in the previous oraplexec. You can then run the script by
      pressing the test current code button ( or running one user thread with
      an output window ) The $message variable will contain and print out (
      using the "puts" command ) any TCL error and the [ oramsg $curn1 all ]
      command will then print out the oracle error from the cursor $curn1. An
      example is if the procedure neword has not yet been created: This then
      gives the full output from Oracle as shown here instead of just the
      standard message :</para>

      <programlisting>new order
{6550 {ORA-06550: line 1, column 7:
PLS-00201: identifier 'NEWORD' must be declared
ORA-06550: line 1, column 7:
PL/SQL: Statement ignored} 0 0 4 8}
</programlisting>
    </sect1>
  </chapter>

  <chapter>
    <title>Copyright</title>

    <para>Copyright © 2019 by Steve Shaw All rights reserved.</para>

    <para>This documentation or any portion thereof may not be reproduced or
    used in any manner whatsoever without permission.</para>

    <para><link
    xlink:href="http://www.hammerdb.com">http://www.hammerdb.com</link></para>
  </chapter>
</book>
