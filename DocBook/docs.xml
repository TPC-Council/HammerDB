<?xml version="1.0" encoding="UTF-8"?>
<book version="5.1" xmlns="http://docbook.org/ns/docbook"
      xmlns:xlink="http://www.w3.org/1999/xlink"
      xmlns:xila="http://www.w3.org/2001/XInclude/local-attributes"
      xmlns:xi="http://www.w3.org/2001/XInclude"
      xmlns:trans="http://docbook.org/ns/transclusion"
      xmlns:svg="http://www.w3.org/2000/svg"
      xmlns:m="http://www.w3.org/1998/Math/MathML"
      xmlns:html="http://www.w3.org/1999/xhtml"
      xmlns:db="http://docbook.org/ns/docbook">
  <info>
    <title><inlinemediaobject>
        <imageobject>
          <imagedata fileref="docs/images/hammerDB-H-Logo-FB.png"/>
        </imageobject>
      </inlinemediaobject> HammerDB Documentation</title>
  </info>

  <chapter>
    <title>Installation and Configuration</title>

    <section>
      <title>Release Notes</title>

      <para>The following are the release notes for HammerDB v4.10.</para>

      <section>
        <title>Nomenclature Change</title>

        <para>From version 4.0 In the database.xml file and the User Interface
        the workload names have changed to TPROC-C and TPROC-H. This is a
        nomenclature change only to represent that the workloads are fair use
        implementations derived from the TPC specifications and the
        nomenclature does not change the functionality of the workload
        compared to prior versions using the TPC-C and TPC-H
        terminology.</para>
      </section>

      <section>
        <title>Stored Procedure Refactoring and Performance</title>

        <para>From version 4.0 the stored procedures for the Oracle and
        PostgreSQL TPROC-C workloads have been refactored. From version 4.5
        the stored procedures for the SQL Server TPROC-C have been refactored
        and MySQL, MariaDB and Db2 modified. This increases the expected
        performance between versions and consequently the performance from
        HammerDB v4.5 &amp; HammerDB v4.0 cannot be compared directly to the
        performance of v3.3 or previous releases dependent on the database
        being tested. Additionally for some workloads HammerDB v45/v4.0
        changed the relationship between the NOPM and TPM metrics compared to
        previous versions. As a result of the stored procedure refactoring
        using bulk operations more work is processed per commit and therefore
        in these cases the NOPM has increased whilst the TPM remains the same.
        This indicates a real measure of increased throughput by doing more
        work per database transaction and consequently NOPM is now listed
        first as the primary metric in reporting output. However as raised in
        <link
        xlink:href="https://github.com/TPC-Council/HammerDB/issues/111">HammerDB
        GitHub Issue #111</link> there may be cases where there is a
        dependency on the wording of the HammerDB log. For this reason a
        configuration option in the generic.xml file of first_result is given.
        If this option is set to NOPM then the v4.0 format is used if set to
        TPM then the output is compatible with v3.3.</para>

        <programlisting>&lt;benchmark&gt;
&lt;rdbms&gt;Oracle&lt;/rdbms&gt;
&lt;bm&gt;TPC-C&lt;/bm&gt;
<emphasis role="bold">&lt;first_result&gt;NOPM&lt;/first_result&gt;</emphasis>
&lt;/benchmark&gt;</programlisting>
      </section>

      <section>
        <title>Storing, Retrieval and Visualization of Results</title>

        <para>From v4.8 the HammerDB Web Service has been enhanced to provide
        a read only interface to the jobs output from both the GUI and CLI
        stored in a SQLite database using a web browser. This enables the
        viewing of workload configuration as well as automatically generated
        charts from the job output. These charts use Apache e-charts and
        therefore to access the charts it is necessary for the viewing browser
        to have an internet connection to download the the e-chart
        functionality. If the browser cannot access the e-charts URL the chart
        pages will remain blank. Chapter 10 provides further details on
        viewing Jobs with the Web Service.</para>
      </section>

      <section>
        <title>Schema and Consistency Checks</title>

        <para>From v4.10 HammerDB has added schema and data consistency checks
        for all databases. These checks can be run after a schema build and
        after running the workload to verify that the schema and data has
        remained consistent.</para>
      </section>

      <section>
        <title>Docker CloudTK GUI</title>

        <para>HammerDB has added an additional Docker image to enable running
        the GUI through a web browser.</para>
      </section>

      <section>
        <title>Quotemeta to escape special characters in passwords</title>

        <para>From v4.10 HammerDB has added an optional quotemeta function to
        escape special characters in passwords. By default this functionality
        is enabled and can be controlled with the quote_passwords setting in
        the configuration.</para>

        <programlisting> &lt;code_highlight&gt;
	&lt;highlight&gt;true&lt;/highlight&gt;
<emphasis role="bold">	&lt;quote_passwords&gt;true&lt;/quote_passwords&gt;</emphasis>
   &lt;/code_highlight&gt;</programlisting>
      </section>

      <section>
        <title>Microsoft SQL Server 2022 Entra Authentication</title>

        <para>From v4.10 HammerDB has been enhanced to permit Microsoft Entra
        authentication where configured for SQL Server 2022 with the ODBC
        driver version 18.</para>
      </section>

      <section>
        <title>MariaDB TPROC-C purge and write back option</title>

        <para>From v4.10 HammerDB has added an option to the TPROC-C workload
        for MariaDB to enable accelerated purge and writeback between runs.
        This option is compatible with MariaDB versions 10.7.0 upwards.</para>
      </section>

      <section>
        <title>Giset command to dynamically set generic dictionary
        settings</title>

        <para>From v4.10 HammerDB has added a giset command that can be used
        to modify generic dictionary settings such as timeouts.</para>
      </section>

      <section>
        <title>Known Third-Party Driver Issues</title>

        <para>HammerDB has a dependency on 3rd party driver libraries to
        connect to the target databases. The following are known issues with
        some of the 3rd party drivers that HammerDB uses.</para>

        <section>
          <title>Oracle on Windows: Oracle Bug 12733000 OCIStmtRelease crashes
          or hangs if called after freeing the service context handle</title>

          <para>If you are running HammerDB against Oracle on Windows there is
          long established bug in Oracle that can cause application crashes
          for multi-threaded applications on Windows.This bug can be
          investigated on the My Oracle Support website with the following
          reference. Bug 12733000 OCIStmtRelease crashes or hangs if called
          after freeing the service context handle. To resolve this Oracle
          issue add the following entry to the SQLNET.ORA file on your
          HammerDB client.</para>

          <programlisting>SQLNET.AUTHENTICATION_SERVICES = (NTS)
DIAG_ADR_ENABLED=OFF 
DIAG_SIGHANDLER_ENABLED=FALSE
DIAG_DDE_ENABLED=FALSE</programlisting>
        </section>

        <section>
          <title>SQL Server on Linux: unixODBC's handle validation may become
          a performance bottleneck</title>

          <para>Using the HammerDB client for SQL Server on Linux can be
          slower than the same client on Windows when using the default
          installed unixODBC drivers on many Linux distributions. As described
          in the <link
          xlink:href="https://docs.microsoft.com/en-us/sql/connect/odbc/linux-mac/programming-guidelines?view=sql-server-ver15">SQL
          Server Programming Guidelines</link> "<emphasis>When using the
          driver with highly multithreaded applications, unixODBC's handle
          validation may become a performance bottleneck. In such scenarios,
          significantly more performance may be obtained by compiling unixODBC
          with the --enable-fastvalidate option. However, beware that this may
          cause applications which pass invalid handles to ODBC APIs to crash
          instead of returning SQL_INVALID_HANDLE errors.</emphasis>"
          Recompiling unixODBC with the --enable-fastvalidate option has been
          measured to improve client performance by 2X. Example configure
          options used to build unixODBC are shown as follows:</para>

          <programlisting>./configure --prefix=/usr/local/unixODBC --enable-gui=no --enable-drivers=no --enable-iconv 
--with-iconv-char-enc=UTF8 --with-iconv-ucode-enc=UTF16LE --enable-threads=yes <emphasis
              role="bold">--enable-fastvalidate</emphasis></programlisting>
        </section>

        <section>
          <title>Db2 driver is not included in the HammerDB Docker image or
          build</title>

          <para>When running the HammerDB Docker image, drivers for all
          databases are pre-installed except for Db2. This is because the
          required redistributable IBM® Data Server Driver for ODBC and CLI is
          not available to be pulled automatically within the build and the
          IBM documentation lacks clarity on the conditions for whether a copy
          of the redistributable package can be made available by HammerDB
          "royalty-free". Therefore any user of Db2 must download and install
          their own Db2 client after installing Docker. This issue is tracked
          in <link
          xlink:href="https://github.com/TPC-Council/HammerDB/issues/404">Issue
          #404</link>.</para>
        </section>
      </section>

      <section>
        <title>Linux Xft Font and Cairo Graphics Pre-Installation
        Requirements</title>

        <para>On Linux the HammerDB GUI requires the <link
        xlink:href="https://www.freedesktop.org/wiki/Software/Xft/">Xft
        FreeType-based font drawing library for X</link> installed as
        follows:</para>

        <para>Ubuntu:</para>

        <para><programlisting>$ sudo apt-get install libxft-dev</programlisting>Red
        Hat:</para>

        <programlisting>$ yum install libXft</programlisting>

        <para>On Linux HammerDB GUI requires <link
        xlink:href="https://www.cairographics.org/">Cairo Graphics</link>
        installed to support the graphical transaction counter and can be
        installed as follows:</para>

        <para>Ubuntu:</para>

        <programlisting>sudo apt-get install libcairo2-dev</programlisting>

        <para>Red Hat:</para>

        <programlisting>ymu install cairo-devel</programlisting>
      </section>
    </section>

    <section>
      <title>Documentation License and Copyright</title>

      <para>Copyright (C) 2024 Steve Shaw.</para>

      <para>Permission is granted to copy, distribute and/or modify this
      document under the terms of the GNU Free Documentation License, Version
      1.3 or any later version published by the Free Software Foundation; with
      no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts. A
      copy of the license is included in the section entitled "GNU Free
      Documentation License".</para>
    </section>

    <section>
      <title>HammerDB v4.10 New Features</title>

      <para>HammerDB v4.10 New Features are all referenced to <link
      xlink:href="https://github.com/TPC-Council/HammerDB/pulls?q=is%3Apr+is%3Aclosed">GitHub
      Pull Requests</link>, where more details for each new feature and
      related pull requests can be found here:</para>

      <para>[TPC-Council#668] Increase default CLI timeouts</para>

      <para>[TPC-Council#665] Add support for Microsoft Entra authentication
      for SQL Server</para>

      <para>[TPC-Council#664] Add schema and consistency checks for all
      databases for TPROC-C and TPROC-H</para>

      <para>[TPC-Council#658] Add quotemeta function to escape special
      characters in passwords</para>

      <para>[TPC-Council#653] Add bcp -u option for SQL Server to trust the
      server certificate</para>

      <para>[TPC-Council#652] MSSQL TPCH Create Advanced Statistics</para>

      <para>[TPC-Council#647] MSSQLS TPCH Add Flag to Partition Orders and
      Lineitems</para>

      <para>[TPC-Council#646] Add giset command to set generic dictionary
      settings</para>

      <para>[TPC-Council#644] Add check to vurun for pre-existing
      threads</para>

      <para>[TPC-Council#642] Add optional purge and write back for MariaDB
      TPROC-C database[TPC-Council#641] Docker cloudtk docker gui</para>

      <para>[TPC-Council#638] Fix call ash_fetch,error on Windows client
      running PostgreSQL metrics</para>

      <para>[TPC-Council#634] Alter MSSQLS TPCH Schema to use Decimal not
      Money</para>
    </section>

    <section>
      <title>Test Matrix</title>

      <para>The following test matrix is provided as a guide based on the
      operating system and database releases that HammerDB has been built and
      tested against. The matrix is not an exclusive support or configuration
      matrix and HammerDB has been designed to be compatible with the
      supported databases running on different architectures and operating
      systems. The HammerDB release files are built for the x86-64
      architecture on Linux and Windows. To support other database versions
      HammerDB supports building from source as described further in this
      documentation. Where the database is not running on either Linux or
      Windows on x86-64 HammerDB can be run on Linux or Windows and connect to
      the target database on another architecture over a network.</para>

      <para>HammerDB has been built and tested on the following x86 64-bit
      Linux and Windows releases.</para>

      <para><table>
          <title>OS Test Matrix</title>

          <tgroup cols="2">
            <thead>
              <row>
                <entry align="center">Operating System</entry>

                <entry align="center">Release</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>Linux</entry>

                <entry>Ubuntu 17.X 18.X 19.X 20.X 21.X 22.X / RHEL 7.X RHEL
                8.X RHEL 9.X</entry>
              </row>

              <row>
                <entry>Windows</entry>

                <entry>Windows 10 / 11</entry>
              </row>
            </tbody>
          </tgroup>
        </table></para>

      <para>HammerDB has been built and testing on the following x86-64 64-bit
      Databases.</para>

      <table>
        <title>Database Test Matrix</title>

        <tgroup cols="2">
          <thead>
            <row>
              <entry align="center">Database (Compatible)</entry>

              <entry align="center">Release</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>Oracle (TimesTen)</entry>

              <entry>12c / 18c / 19c / 21c</entry>
            </row>

            <row>
              <entry>Microsoft SQL Server</entry>

              <entry>2017 / 2019 / 2022</entry>
            </row>

            <row>
              <entry>Db2</entry>

              <entry>11.1</entry>
            </row>

            <row>
              <entry>MySQL (Amazon Aurora)</entry>

              <entry>5.7 / 8.0</entry>
            </row>

            <row>
              <entry>MariaDB</entry>

              <entry>10.2 / 10.3 / 10.4 / 10.5 / 10.6 / 10.8 / 10.10 /10.11 /
              11.1 / 11.2</entry>
            </row>

            <row>
              <entry>PostgreSQL (EnterpriseDB) (Amazon Redshift)
              (Greenplum)</entry>

              <entry>10.2 / 10.3 / 11 / 12 / 13 / 14 / 15</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>

    <section>
      <title>Downloading HammerDB</title>

      <para>Pre-built binary installation packages for HammerDB can be
      downloaded from the <link
      xlink:href="https://www.hammerdb.com/download.html">Download</link> page
      at www.hammerdb.com.</para>
    </section>

    <section>
      <title>Checksum Verification</title>

      <para>Checksums for the installation files are shown alongside the
      download files in <link
      xlink:href="https://github.com/TPC-Council/HammerDB/releases">GitHub
      Releases</link>. The integrity of the HammerDB installation files can be
      verified on Windows with the Microsoft File Checksum Integrity Verifier
      which can be downloaded at no cost from Microsoft and run as
      follows:</para>

      <programlisting>fciv -both HammerDB-4.10-Win-x86-64-Setup.exe</programlisting>

      <para>and on Linux with md5sum and sha1sum as shown:</para>

      <programlisting>md5sum HammerDB-4.10-Linux.tar.gz 
sha1sum HammerDB-4.10-Linux.tar.gz</programlisting>
    </section>

    <section>
      <title>Installing and Starting HammerDB on Windows</title>

      <para>To install HammerDB on Windows you have the option of using the
      self-extracting installer or zipfile. The self-extracting installer will
      create an uninstall executable for you. A zipfile installation can be
      deleted manually. In both cases the install is entirely self-contained
      within the installation directory.</para>

      <section>
        <title>Self Extracting Installer</title>

        <para>Double click on the Setup file the installer will confirm the
        Verified Publisher as the TRANSACTION PROCESSING PERFORMANCE COUNCIL.
        Click Yes to start the installation.</para>

        <figure>
          <title>Verified Publisher</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/CH1-4a.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>Click continue to begin the installation.</para>

        <figure>
          <title>HammerDB Version</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch1-4.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>Read and Accept the GPL License Agreement.</para>

        <figure>
          <title>GPL v3 License</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch1-5.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>Choose the installation directory.</para>

        <figure>
          <title>Choose the Installation Directory</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch1-6.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>Press Next to begin the install.</para>

        <figure>
          <title>Files copying</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch1-7.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>The installer will extract the files into the chosen
        directory.</para>

        <figure>
          <title>Complete</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch1-8.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>Complete the Install by viewing the Readme File and running
        HammerDB. If both options are chosen HammerDB will run after the
        Readme is closed.</para>

        <figure>
          <title>Complete the Setup Wizard</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch1-8a.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>HammerDB will start ready for you to use. The default order of
        the databases in the treeview is in the order that they were added to
        HammerDB, however this is user configurable and from v4.5 settings
        will persist across restarts of HammerDB. To change the default order
        and understand how settings are persisted see section 9 XML
        Configuration further in this Chapter.</para>

        <para>If it is the first time that HammerDB has run it will create a
        new Jobs repository database called hammer.DB, in the configured
        location in the config/generic.xml file which by default is set to TMP
        &lt;commandline&gt;&lt;sqlite_db&gt;TMP&lt;/sqlite_db&gt;&lt;/commandline&gt;
        to find a suitable temp location. Subsequent use will use the same
        file for storing job related data. This file can moved if another
        location is preferred or if deleted a new one will be
        recreated.</para>

        <figure>
          <title>HammerDB Started</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch1-9.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>

      <section>
        <title>Zip File</title>

        <para>As an alternative to the self-extracting installer you can
        download and extract the zipfile into a directory of your
        choice.</para>

        <figure>
          <title>Zip File</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch1-10.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>

      <section>
        <title>Starting HammerDB</title>

        <para>After installation double-click on the "Windows Batch File"
        hammerdb to start hammerdb.</para>

        <figure>
          <title>hammerdb batch file</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch1-11.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>

      <section>
        <title>Uninstalling HammerDB</title>

        <para>For a zipfile installation, delete the hammerDB directory. For
        an installer based installation double-click on uninstall and follow
        the on-screen prompts.</para>

        <para><figure>
            <title>Uninstall</title>

            <mediaobject>
              <imageobject>
                <imagedata fileref="docs/images/ch1-12.PNG"/>
              </imageobject>
            </mediaobject>

            <para>In addition to the HammerDB software deletion there is also
            a number of SQLite database files by default stored in the
            temporary area. These can also be safely removed if
            preferred.</para>
          </figure></para>

        <figure>
          <title>Uninstall database files</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch1-12aa.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>
    </section>

    <section>
      <title>Installing and Starting HammerDB on Linux</title>

      <para>To install HammerDB on Linux you have the option of using the
      self-extracting installer or a tar.gz file. The self-extracting
      installer will create an uninstall executable for you. A tar.gz
      installation can be deleted manually. In both cases the install is
      entirely self-contained within the installation directory.</para>

      <section>
        <title>Self Extracting Installer</title>

        <para>To install from the self-extracting installer using a graphical
        environment refer to the previous section on the self-extracting
        installer for Windows, the installation method is the same.</para>
      </section>

      <section>
        <title>Tar.gz File</title>

        <para>To install from the tar.gz run the command</para>

        <programlisting>tar -zxvf HammerDB-4.10.tar.gz </programlisting>

        <para>This will extract HammerDB into a directory named
        HammerDB-4.10.</para>
      </section>

      <section>
        <title>Starting HammerDB</title>

        <para>To start HammerDB change to the HammerDB directory and run
        locally as follows.</para>

        <programlisting>./hammerdb</programlisting>
      </section>

      <section>
        <title>Uninstalling HammerDB</title>

        <para>To uninstall HammerDB on Linux run the uninstall executable for
        the self-extracting installer or remove the directory for the tar.gz
        install. You can also remove the .db and .DB persistent configuration
        database files that are by default stored in the /tmp
        directory.</para>
      </section>
    </section>

    <section>
      <title>HammerDB Docker Container Build &amp; Run</title>

      <para>Containers are lightweight prebuilt environments that are easy to
      distribute and deploy. Version 4.10 includes both a pre-built Docker
      Image and a Dockerfile that can be used to create a HammerDB Docker
      containers. This container is based on an Ubuntu 20.04 base image and
      provides a ready to use environment to run HammerDB. HammerDB has a
      dependency on 3rd party driver libraries to connect to the target
      databases. This docker container installs the required client libraries
      and sets up the environment that enables connections to the target
      databases.</para>

      <section>
        <title>Docker Image</title>

        <para>A pre-built Docker image is provided on the official TPC site on
        Docker Hub at <link
        xlink:href="https://hub.docker.com/u/tpcorg">https://hub.docker.com/u/tpcorg</link>
        than be downloaded with the docker pull command either for a complete
        image for all databases but also database specific images.</para>

        <programlisting>docker pull tpcorg/hammerdb
docker pull tpcorg/hammerdb:mysql
docker pull tpcorg/hammerdb:maria
docker pull tpcorg/hammerdb:oracle
docker pull tpcorg/hammerdb:postgres
docker pull tpcorg/hammerdb:mssqls
</programlisting>

        <para>The official HammerDB Docker images are hosted by the
        TPC-Council and recognised by the Docker Sponsored OSS program.</para>

        <figure>
          <title>Docker Sponsored OSS</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="../DocBook/docs/images/ch1-12aaa.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>It is only necessary to follow the Dockerfile build instructions
        if you wish to build your own Dockerfile from source.</para>
      </section>

      <section>
        <title>CloudTK Web Application Docker Image</title>

        <para>HammerDB provides an additional Docker image for running the
        HammerDB GUI as a Web Application accessed through a web browser. This
        is particularly useful running in a cloud environment where a local
        Desktop environment is not readily available. f you want access to
        functionality such as CPU metrics then you should use the –net-host
        option.</para>

        <programlisting>$ sudo docker run --net=host --name hammerdb-cloudtk tpcorg/hammerdb:latest-cloudtk
can't find package limit
Running with default file descriptor limit
/debug user "debug" password "sycu4xfc.gcw"
httpd started on port 8081
secure httpd started on SSL port 8082</programlisting>

        <para>Otherwise you can only expose the ports as needed. </para>

        <programlisting>docker run -p 8081:8081 -p 8082:8082 -p 8080:8080 --name hammerdb-cloudtk hammerdb:cloudtk</programlisting>

        <para>You can connect to port 8081 for an unencrypted connection.
        Alternatively you can use an encrypted connection on port 8082. By
        default sample self-signed certificates are installed and these should
        be replaced by your own. </para>

        <figure>
          <title>CloudTK Launcher</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="../DocBook/docs/images/ch1-12aaaa.png"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>Click Submit Query on the Launcher.</para>

        <figure>
          <title>Launch HammerDB</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="../DocBook/docs/images/ch1-12aaaaa.png"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>HammerDB will then run fully contained within a browser, giving
        access to the GUI from cloud environments where the Desktop may not be
        readily available.</para>

        <figure>
          <title>HammerDB GUI within a browser</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="../DocBook/docs/images/ch1-12aaaaaa.png"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>

      <section>
        <title>Dockerfile Download and installation</title>

        <para>A Dockerfile is a shell script that automates the docker
        container build. It makes it easier to spawn multiple containers with
        ease. HammerDBv-4.6 source on GitHub is packaged with a folder
        “Docker”, that contains all the Docker related files and folders. This
        folder contains a Dockerfile for building a HammerDB client container
        that supports all the databases that HammerDB supports, i.e. Oracle,
        Microsoft SQL Server, IBM Db2, MySQL, PostgreSQL and MariaDB.
        Dockerfile is easy to customize each one’s requirement.</para>

        <para>The HammerDB Dockerfile is stored within the <link
        xlink:href="https://github.com/TPC-Council/HammerDB">HammerDB GitHub
        repository</link>. To download the Dockerfile either Clone or Download
        the ZIP of the the HammerDB source and navigate to the Docker
        directory.</para>

        <para><figure>
            <title>Docker File Download</title>

            <mediaobject>
              <imageobject>
                <imagedata fileref="docs/images/ch1-12a.PNG"/>
              </imageobject>
            </mediaobject>
          </figure></para>
      </section>

      <section>
        <title>Building the HammerDB Container</title>

        <para>The Docker container built from this Dockerfile installs the
        required 3rd party driver libraries for all databases supported in
        HammerDB except for IBM Db2. As IBM Db2 client libraries are not free
        to redistribute they must be independently downloaded and added if
        required. For the build to include IBM Db2, IBM Data Server Driver for
        <link xlink:href="https://www.ibm.com/support/pages/node/6465915">ODBC
        and CLI version 11.5.6</link> can be downloaded locally and placed in
        the same location as Dockerfile.</para>

        <para>To create an image: Go to the folder containing the Dockerfile
        and run the docker build command.</para>

        <programlisting>docker build -t hammerdb-v4.10 . </programlisting>

        <figure>
          <title>docker build</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch1-12b.png"/>
            </imageobject>
          </mediaobject>
        </figure>

        <figure>
          <title>Successfully built</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch1-12c.png"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>

      <section>
        <title>Running the Container</title>

        <para>To create and start a container named "hammerdb" with the image
        built in previous section run the following command. The container
        build is successful but the client libraries for IBM are not
        installed.</para>

        <programlisting>	docker run -it --name hammerdb hammerdb-v4.10 bash</programlisting>

        <figure>
          <title>docker run</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch1-12d.png"/>
            </imageobject>
          </mediaobject>

          <para>If IBM client libraries are manually downloaded and placed
          alongside Dockerfile, container builds successfully and output looks
          as follows.</para>
        </figure>

        <section>
          <title>IBM Client install</title>

          <para>If IBM client libraries are manually downloaded and placed
          alongside Dockerfile, container builds successfully and output looks
          as follows.</para>

          <figure>
            <title>IBM client</title>

            <mediaobject>
              <imageobject>
                <imagedata fileref="../DocBook/docs/images/ch1-12e.png"/>
              </imageobject>
            </mediaobject>
          </figure>
        </section>
      </section>

      <section>
        <title>Connecting the Container to the Network</title>

        <para>The docker container needs to be connected to network to
        communicate with remote databases. The following is an example of
        adding host network to the container.</para>

        <programlisting> docker run --network=host -it --name hammerdb hammerdb-v4.10 bash</programlisting>
      </section>
    </section>

    <section>
      <title>Verifying the Installation of Database Client Libraries</title>

      <para>For all of the databases that HammerDB supports it is necessary to
      have a third-party client library installed that HammerDB can use to
      connect and interact with the database. Note that if you use the
      official HammerDB Docker images this step has already been done for you
      and all of the third-party client libraries have been preiinstalled.
      This client library will also typically be installed with database
      server software. HammerDB does not statically link the 3rd party
      libraries to minimise executable size and provide flexibility in the
      third-party libraries used. For example if a bug is detected in a
      particular library then this can be upgraded without requiring the
      HammerDB libraries to be rebuilt. However as the client libraries are
      dynamically linked it is essential that the correct client libraries are
      already installed and environment variables set to ensure that HammerDB
      can find the correct libraries. Note that it is only necessary to load
      the libraries for the database that your are testing.</para>

      <para>The HammerDB command line tool can be used to check the status of
      library availability for all databases.</para>

      <para>To run this utility run the following command</para>

      <programlisting>./hammerdbcli</programlisting>

      <para>and type librarycheck.</para>

      <programlisting>HammerDB CLI v4.10
Copyright (C) 2003-2024 Steve Shaw
Type "help" for a list of commands
The xml is well-formed, applying configuration
hammerdb&gt;librarycheck
Checking database library for Oracle
Success ... loaded library Oratcl for Oracle
Checking database library for MSSQLServer
Success ... loaded library tdbc::odbc for MSSQLServer
Checking database library for Db2
Success ... loaded library db2tcl for Db2
Checking database library for MySQL
Success ... loaded library mysqltcl for MySQL
Checking database library for PostgreSQL
Success ... loaded library Pgtcl for PostgreSQL
Checking database library for MariaDB
Success ... loaded library mariatcl for MariaDB

hammerdb&gt;
</programlisting>

      <para>In the examples it can be seen that the libraries for all
      databases were found and loaded. The following table illustrates the
      first level library that HammerDB requires however there may be
      additional dependencies. Refer to the Test Matrix to determine which
      database versions HammerDB was built against. On Windows the <link
      xlink:href="https://github.com/lucasg/Dependencies/">Dependencies
      Utility</link> can be used to determine the dependencies and on Linux
      the command ldd.</para>

      <para>For example on Windows use the dependencies utility to open the
      HammerDB library for your chosen database. In the following example
      libmysqltcl.dll is opened for MySQL. This shows that the key dependency
      is on the 64-bit libmysql.dll.</para>

      <figure>
        <title>Dependencies MySQL</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch1-13.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Right-clicking on this library shows the properties including
      where it was found.</para>

      <figure>
        <title>LIBMYSQL.DLL Properties</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch1-14.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>This location was set in the Environment variables under the Path
      option.</para>

      <figure>
        <title>Environment Variables</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch1-15.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>As shown below hammerDB found the correct MySQL 8.0 library
      because the path to the 64-bit MySQL 8.0 library was set correctly in
      the environment variables.</para>

      <figure>
        <title>Path environment variable</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch1-16.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>On Linux we run a similar test with librarycheck, however in this
      instance the library file is not found, although note that it identifies
      the file that is missing as libmysqlclient.so.21.</para>

      <programlisting>Checking database library for MySQL
Error: failed to load mysqltcl - couldn't load file "/home/steve/HammerDB-4.1/lib/mysqltcl-3.052/libmysqltcl3.052.so": libmysqlclient.so.21: cannot open shared object file: No such file or directory
Ensure that MySQL client libraries are installed and the location in the LD_LIBRARY_PATH environment variable
</programlisting>

      <para>We can investigate further using the ldd command in an equivalent
      way to dependency walker on Windows. This also identifies the file that
      is missing.</para>

      <programlisting>$ ldd libmysqltcl3.052.so 
linux-vdso.so.1 (0x00007ffc44f7d000)
<emphasis role="bold">libmysqlclient.so.21 =&gt; not found</emphasis>
libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007f33e73e7000)
/lib64/ld-linux-x86-64.so.2 (0x00007f33e79e2000)
</programlisting>

      <para>Checking in our MySQL installation we can find the file
      libmysqlclient.so.21.</para>

      <programlisting>$ pwd
/opt/mysql-8.0.18-linux-glibc2.12-x86_64/lib
$ ls libmysqlclient*
libmysqlclient.a  libmysqlclient.so  libmysqlclient.so.21  libmysqlclient.so.21.1.18
</programlisting>

      <para>Therefore we know that the file is installed, however we need to
      tell HammerDB where to find it. This is done by adding the MySQL library
      to the LD_LIBRARY_PATH.</para>

      <programlisting>$ export LD_LIBRARY_PATH=/opt/mysql-8.0.18-linux-glibc2.12-x86_64/lib:$LD_LIBRARY_PATH</programlisting>

      <para>Reversing our steps we can see that the library is now
      found.</para>

      <programlisting>$ ldd libmysqltcl3.052.so 
linux-vdso.so.1 (0x00007fff7f7e6000)
libmysqlclient.so.21 =&gt; /opt/mysql-8.0.18-linux-glibc2.12-x86_64/lib/libmysqlclient.so.21 (0x00007f92b0153000)
libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007f92afd62000)
libpthread.so.0 =&gt; /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f92afb43000)
libdl.so.2 =&gt; /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f92af93f000)
libssl.so.1.1 =&gt; /opt/mysql-8.0.18-linux-glibc2.12-x86_64/lib/libssl.so.1.1 (0x00007f92af6b5000)
libcrypto.so.1.1 =&gt; /opt/mysql-8.0.18-linux-glibc2.12-x86_64/lib/libcrypto.so.1.1 (0x00007f92af270000)
librt.so.1 =&gt; /lib/x86_64-linux-gnu/librt.so.1 (0x00007f92af068000)
libstdc++.so.6 =&gt; /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f92aecdf000)
libm.so.6 =&gt; /lib/x86_64-linux-gnu/libm.so.6 (0x00007f92ae941000)
libgcc_s.so.1 =&gt; /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f92ae729000)
/lib64/ld-linux-x86-64.so.2 (0x00007f92b0c34000)</programlisting>

      <para>and library check confirms that it can be loaded.</para>

      <programlisting>Checking database library for MySQL
Success ... loaded library mysqltcl for MySQL</programlisting>

      <para>Add the export command to the .bash_profile ensures that it will
      be found each time HammerDB is launched from a new shell.</para>

      <para>The following table shows the libraries that are required for each
      database version. All libraries are 64-bit. Note that some databases are
      considerably more flexible in library versions and therefore the
      following section is important to ensure that you install the correct
      library for your needs.</para>

      <table>
        <title>3rd party libraries</title>

        <tgroup cols="2">
          <thead>
            <row>
              <entry align="center">Database / OS</entry>

              <entry align="center">Library</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>Oracle Linux</entry>

              <entry>libclntsh.so</entry>
            </row>

            <row>
              <entry>Oracle Windows</entry>

              <entry>OCI.DLL</entry>
            </row>

            <row>
              <entry>SQL Server Linux</entry>

              <entry>libodbc.so</entry>
            </row>

            <row>
              <entry>SQL Server Windows</entry>

              <entry>ODBC32.DLL</entry>
            </row>

            <row>
              <entry>Db2 Linux</entry>

              <entry>libdb2.so</entry>
            </row>

            <row>
              <entry>Db2 Windows</entry>

              <entry>DB2CLI64.DLL</entry>
            </row>

            <row>
              <entry>MySQL Linux</entry>

              <entry>libmysqlclient.so</entry>
            </row>

            <row>
              <entry>MySQL Windows</entry>

              <entry>LIBMYSQL.DLL</entry>
            </row>

            <row>
              <entry>MariaDB Linux</entry>

              <entry>libmariadb.so</entry>
            </row>

            <row>
              <entry>MariaDB Windows</entry>

              <entry>LIBMARIADB.DLL</entry>
            </row>

            <row>
              <entry>PostgreSQL Linux</entry>

              <entry>libpq.so</entry>
            </row>

            <row>
              <entry>PostgreSQL Windows</entry>

              <entry>LIBPQ.DLL</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <section>
        <title>Oracle Client</title>

        <para>When using the Oracle instant client Oratcl uses the additional
        environment variable ORACLE_LIBRARY to identify the Oracle client
        library. On the Windows the Oracle client library is called oci.dll in
        a location such as: C:\oraclexe\app\oracle\product\11.2.0\server\bin.
        On Linux the library is called libclntsh.so where this is typically a
        symbolic link to a product specific name such as libclntsh.so.12.1 for
        Oracle 12c. An example .bash_profile file is shown for a typical
        Oracle environment.</para>

        <programlisting>oracle@server1  oracle]$ cat ~/.bash_profile
# .bash_profile

if [ -t 0 ]; then
stty intr ^C
fi

# Get the aliases and functions
if [ -f ~/.bashrc ]; then
        . ~/.bashrc
fi
# User specific environment and startup programs
umask 022
export ORACLE_BASE=/u01/app/oracle
export ORACLE_HOME=$ORACLE_BASE/product/12.1.0/dbhome_1
export LD_LIBRARY_PATH=$ORACLE_HOME/lib
export ORACLE_LIBRARY=$ORACLE_HOME/lib/libclntsh.so
export ORACLE_SID=PROD1
export PATH=$ORACLE_HOME/bin:$PATH
</programlisting>
      </section>

      <section>
        <title>SQL Server</title>

        <para>On SQL Server on Windows the client libraries and necessary
        environment variables are set automatically during the SQL Server
        installation. Note that on 64-bit Windows the 64-bit ODBC client
        library is named ODBC32.DLL in the following location.
        C:\Windows\System32\odbc32.dll. On Linux follow the SQL Server on
        Linux installation guide to install 'mssql-tools' with the unixODBC
        developer package. The command database drivers will show the
        installed ODBC Driver. (Note that this command is not installed on
        Windows as unixODBC is only installed on Linux).</para>

        <programlisting>hammerdb&gt;database drivers
{{ODBC Driver 17 for SQL Server} {{Description=Microsoft ODBC Driver 17 for SQL Server} 
Driver=/opt/microsoft/msodbcsql/lib64/libmsodbcsql-17.0.so.1.1 UsageCount=1}}</programlisting>
      </section>

      <section>
        <title>Db2</title>

        <para>For DB2 on Linux the client library libdb2.so.1 is required
        either in the lib64 directory for 32. Similarly on Windows the
        db2cli64.dll is required. These libraries are included with a standard
        DB2 installation or also with a standalone DB2 client install.</para>
      </section>

      <section>
        <title>MySQL</title>

        <para>HammerDB version 4.9 (4.X and version 3.3) has been built and
        tested against a MySQL 8.0 client installation, hammerDB version
        3.0-3.2 has been built against MySQL 5.7. On Linux this means that
        HammerDB will require a MySQL client library called
        libmysqlclient.so.21 for HammerDB version 4.X and 3.3 and
        libmysqlclient.so.20 for version 3.2 and earlier. This client library
        needs to be referenced in the LD_LIBRARY_PATH as shown previously in
        this section. On Windows libmysqll.dll is required and should be
        referenced in the PATH environment variable.</para>
      </section>

      <section>
        <title>MariaDB</title>

        <para>HammerDB version 4.9 has been built and tested against MariaDB
        10.6 client installation. On Linux this means that HammerDB will
        require a MariaDB client library called libmariadb.so.3. This client
        library needs to be referenced in the LD_LIBRARY_PATH as shown
        previously in this section. On Windows the client library is called
        libmariadb.dll and should be referenced in the PATH environment
        variable.</para>
      </section>

      <section>
        <title>PostgreSQL</title>

        <para>For PostgreSQL the client library is called libpq.dll on Windows
        and libpq.so on Linux however note that additional libraries are also
        required. For Windows this means setting your PATH environment
        variable such as the following: D:\PostgreSQL\pgsql\bin; On Linux it
        is required to set the LD_LIBRARY_PATH environment variable in the
        same way described for Oracle previously in this section to the
        location of the PostgreSQL lib directory. Alternatively for
        installations of EnterpriseDB the client directory also contains the
        necessary files for a HammerDB installation.</para>
      </section>
    </section>

    <section>
      <title>XML &amp; Persistent Configuration</title>

      <para>HammerDB configuration and settings are defined in a number of XML
      files in the config directory.</para>

      <figure>
        <title>XML Configuration Files</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch1-17.PNG"/>
          </imageobject>
        </mediaobject>

        <para>From v4.7 it is recommended not to edit these files apart from
        the sqlitedb_dir parameter in the generic.xml file. The configuration
        is now stored and persisted across restarts through a series of SQLite
        database files located in the sqlitedb_dir directory. By default this
        directory is set to a temporary location and it is therefore
        recommended to set this to a preferred permanent storage location if
        desired. In the listing below the Data Base File corresponds to the
        XML file with the same name. Note that after the first start and
        creation of the Data Base File the XML configuration file will not be
        re-read again. If you wish to reset the configuration for a particular
        database if you the Data Base File for a particular database it will
        be recreated from the XML configuration at the next HammerDB start.
        The hammer.DB file contains the data related to jobs storage.
        Similarly if this file is deleted it will be recreated on the next
        HammerDB start.</para>

        <programlisting>   &lt;sqlitedb&gt;
        &lt;sqlitedb_dir&gt;TMP&lt;/sqlitedb_dir&gt;
   &lt;/sqlitedb&gt;</programlisting>
      </figure>

      <figure>
        <title>Database Files</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch1-17aa.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>With this persistent storage after closing and restarting HammerDB
      it will continue with the settings previously used. For example, on
      startup SQL Server is shown first and already selected meaning it is not
      required to switch to a preferred database from the default
      choice.</para>

      <figure>
        <title>Modified Default Database</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch1-17a.PNG"/>
          </imageobject>
        </mediaobject>

        <para>Also note that the configuration is interchangeable between GUI,
        CLI and WS and therefore the settings made for one interface will be
        persisted for use for another interface on the same system. Therefore,
        for example settings made with the GUI can be stored and then run with
        the CLI. If it is wished to reset the settings for a particular
        database the corresponding SQLite database can be deleted and it will
        be recreated at next use.</para>

        <para>Note that for the potential of an incompatible modified
        configuration the SQLite databases will be cleared if a different
        version of HammerDB is detected from the one that created the existing
        database files, reporting the following message at startup.</para>

        <programlisting>The existing SQLite DBs are from version v4.9. SQLite DBs will be reset to v4.10.</programlisting>

        <para>For this reason before running another version any existing
        configuration that is wished to be saved should be exported using
        SQLite tools and imported into the newly created database
        files.</para>
      </figure>
    </section>

    <section>
      <title>Themes and Scalable Graphics</title>

      <para>HammerDB v4.0 onwards includes an updated graphical interface that
      adapts to scale to UHD displays such as Microsoft pixelsense displays.
      The behaviour of the display is set in the theme section of
      generic.xml.</para>

      <programlisting>&lt;theme&gt;
&lt;scaling&gt;auto&lt;/scaling&gt;
&lt;scaletheme&gt;auto&lt;/scaletheme&gt;
&lt;pixelsperpoint&gt;auto&lt;/pixelsperpoint&gt;
&lt;/theme&gt;</programlisting>

      <para>By default scaling, the scaletheme and pixelsperpoint are all set
      to auto. This means that HammerDB will detect the display settings and
      scale the interface accordingly. For example the image shows HammerDB
      v3.3 and v4.7 on the same UHD display.</para>

      <figure>
        <title>Scaling Graphics</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch1-18.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>However some displays or third-party X Windows servers may not be
      updated to support scalable graphics. In this case the scaling value can
      be set to fixed and a standard 96 dpi display will be used with the
      fixed themes from HammerDB v3.3.</para>

      <programlisting>&lt;theme&gt;
&lt;scaling&gt;fixed&lt;/scaling&gt;
&lt;scaletheme&gt;auto&lt;/scaletheme&gt;
&lt;pixelsperpoint&gt;auto&lt;/pixelsperpoint&gt;
&lt;/theme&gt;</programlisting>

      <para>The scaletheme value will accept settings of "auto", "awlight",
      "arc" or "breeze". If set to the default of "auto", "awlight" will be
      used on Linux.</para>

      <figure>
        <title>Linux Theme Awlight</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch1-19.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>and "breeze" on Windows.</para>

      <figure>
        <title>Windows Theme Breeze</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch1-9.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The scale factor can be fine-tuned by setting the pixelsperpoint
      value. By running the command puts [ tk scaling ] in the console you can
      determine the current value. By setting this value slightly larger or
      smaller than the default you can adjust the scaling to your system. This
      value is not intended for large scale changes from the default as
      settings have been adjusted to the detected value.</para>

      <programlisting>(HammerDB-4.0) 49 % puts [ tk scaling ]
1.3333333333333333</programlisting>
    </section>

    <section>
      <title>Building HammerDB from Source</title>

      <para>Before building from source be aware that HammerDB already
      provides pre-compiled packages from the download page and therefore
      building from source is not required to run HammerDB. Instead building
      from source is for people who want to compile all of their own
      dependencies from the C code that HammerDB is built in.</para>

      <para>Building from source also enables building a distribution with
      commits ahead of the current HammerDB release.</para>

      <para>When building from source all of the dependent packages will be
      downloaded from www.hammerdb.com.</para>

      <para>HammerDB build automation uses an adapted version of the BAWT
      package by Paul Obermeier . BAWT is copyrighted by Paul Obermeier and
      distributed under the 3-clause BSD license.</para>

      <section>
        <title>Compiler &amp; Dependency Install on Linux</title>

        <para>Install a development environment using the gcc compiler for
        Linux. You will also need the p7zip tool as well as the Xft font
        library and Cairo graphics development packages.  The build has been
        tested on x64 Red Hat 8.X and Ubuntu 20.04.X Linux, with the following
        example from Red Hat Linux.</para>

        <programlisting>yum install p7zip-16.02-20.el8.x86_64.rpm
sudo yum install libXft-devel
yum group install "Development Tools"</programlisting>

        <para>The HammerDB Oracle and ODBC for SQL Server client libraries
        will build without the respective Oracle and SQL Server client
        libraries, but will need them at runtime. However, the client or
        server must be installed for all of Db2, MariaDB, PostgreSQL and MySQL
        for the HammerDB build to be successful. The database installation
        must include both the include and lib directories. For Db2 either the
        server or client can be installed, for example
        v11.5.7_linuxx64_server_dec.tar.gz. HammerDB build automation will
        look for the Db2 installation in the location of the environment
        variable IBM_DB_DIR set using db2profile tool. Verify that this has
        been set as follows:</para>

        <programlisting>$ echo $IBM_DB_DIR
/home/ibm/sqllib</programlisting>

        <para>On Linux MariaDB, PostgreSQL and MySQL include a config command
        in the bin directory that returns details of the configuration.
        HammerDB uses these commands to find the headers and libraries needed
        for the build. Before running the build, environment variables
        MARIADB_CONFIG, PG_CONFIG and MYSQL_CONFIG must be set to the location
        of the respective config commands for each database in the terminal
        running the build.</para>

        <programlisting>$ export MARIADB_CONFIG=/opt/mariadb/mariadb-10.6.7-linux-systemd-x86_64/bin
$ export PG_CONFIG=/opt/postgresql/bin
$ export MYSQL_CONFIG=/opt/mysql/mysql-8.0.28-linux-glibc2.12-x86_64/bin</programlisting>

        <para>In addition the source build will expect to find the python3 and
        python3-config executables that would typically already be installed
        on a Linux system with for example python3 being installed by the
        python3-minimal package and python3-config by the python3-dev package
        on Ubuntu.</para>
      </section>

      <section>
        <title>Compiler &amp; Dependency Install on Windows</title>

        <para>On Windows, download and install <link
        xlink:href="https://visualstudio.microsoft.com/vs/community/">Visual
        Studio 2022</link>, Visual Studio is free for open source developers.
        An additional gcc compiler will be downloaded and installed locally
        during the build. The build has been tested on x64 Windows 10 and 11.
        As with Linux it is also mandatory to install a database server or
        client including the development environment of headers and libraries
        for MariaDB, Db2, MySQL and PostgreSQL. For Db2 on Windows, there is
        no db2profile that sets the environment therefore the IBM_DB_DIR
        environment variable must be set to the location of the Db2 install.
        Similarly, the MariaDB and MySQL config commands are not available on
        Windows either and should also be set to the database or client
        installation directory rather than the bin directory. PostgreSQL for
        Windows does include the config command and therefore the environment
        configuration is the same as Linux. The source build will also require
        a Python installation from <link
        xlink:href="https://www.python.org/">https://www.python.org/</link>,
        this installation will need the include and libs directory for
        compilation and the Python DLL which is in the Python home directory
        at runtime. In the following example the PYTHONHOME environment
        variable should be set to
        C:\Users\username\AppData\Local\Programs\Python\Python310.</para>

        <programlisting>C:\Users\username\AppData\Local\Programs\Python\Python310
C:\Users\username\AppData\Local\Programs\Python\Python310\include
C:\Users\username\AppData\Local\Programs\Python\Python310\libs</programlisting>

        <para>With the following settings for all environments variables for
        the build.</para>

        <programlisting>set MARIADB_CONFIG=C:\Program Files\MariaDB\MariaDB Connector C 64-bit
set MYSQL_CONFIG=C:\Program Files\MySQL\MySQL Server 8.0
set PG_CONFIG=C:\Program Files\PostgreSQL\pgsql\bin
set IBM_DB_DIR=C:\Program Files\IBM\SQLLIB
set PYTHONHOME=C:\Users\username\AppData\Local\Programs\Python\Python310</programlisting>

        <para>For all database installations on Windows whether client or
        server verify that the installation has the include, bin and lib
        directories. On Windows in particular, some installations may not
        include all the required files for development.</para>
      </section>

      <section>
        <title>Download HammerDB Source</title>

        <para>At this stage you will have installed the compiler you need and
        database client/server installations for MariaDB, Db2, MySQL and
        PostgreSQL. To reiterate, HammerDB will not build correctly unless you
        have installed ALL the required database environments. Next download
        HammerDB from gitHub by either cloning or downloading. From the main
        HammerDB GitHub page use the clone URL or the Download Zip link from
        the master branch.</para>

        <figure>
          <title>Download Source</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch1-12a.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <programlisting>$ git clone https://github.com/TPC-Council/HammerDB.git
Cloning into 'HammerDB'...
remote: Enumerating objects: 8099, done.
remote: Total 8099 (delta 0), reused 0 (delta 0), pack-reused 8099
Receiving objects: 100% (8099/8099), 40.41 MiB | 8.39 MiB/s, done.
Resolving deltas: 100% (2564/2564), done.
</programlisting>

        <para>If cloning you will have a directory called “HammerDB” or
        extracting the zipfile a directory called “HammerDB-master”.</para>
      </section>

      <section>
        <title>Running the Build</title>

        <para>Navigate to the Build\Bawt-2.1.0 directory with the command line
        for both Linux and Windows. Note that during the build an InputLibs
        directory will be created and all HammerDB libraries downloaded and
        under the Build directory a BawtBuild directory will be created where
        the installation will take place. Therefore, sufficient disk space and
        permissions must be available for the build to take place. Within the
        command line you are running the build make sure that you have
        correctly set all the MARIADB_CONFIG, MYSQL_CONFIG, PG_CONFIG, and
        IBM_DB_DIR environment variables used during the build and run the
        Build-Linux.sh command for Linux.</para>

        <programlisting>./Build-Linux.sh x64 Setup/HammerDB-Linux.bawt update</programlisting>

        <para>and make sure PYTHONHOME is set to the correct directory of the
        Python installation and run the Build-Windows.bat commands for
        Windows</para>

        <programlisting>./Build-Windows.bat x64 vs2022+gcc Setup/HammerDB-Windows.bawt update</programlisting>

        <para>the command sequence will look similar to the following on
        Linux:</para>

        <programlisting>$ export MYSQL_CONFIG=/opt/mysql-8.0.20-linux-glibc2.12-x86_64/bin
$ export MARIADB_CONFIG=/opt/mariadb-10.8.1-linux-x86_64/bin/
$ export PG_CONFIG=/opt/postgresql-14.1/bin
$ echo $IBM_DB_DIR
/opt/ibm/sqllib
$ ./Build-Linux.sh x64 Setup/HammerDB-Linux.bawt update</programlisting>

        <para>and the following on Windows:</para>

        <programlisting>set MARIADB_CONFIG=C:\Program Files\MariaDB\MariaDB Connector C 64-bit
set MYSQL_CONFIG=C:\Program Files\MySQL\MySQL Server 8.0
set PG_CONFIG=C:\Program Files\PostgreSQL\pgsql\bin
set IBM_DB_DIR=C:\Program Files\IBM\SQLLIB
set PYTHONHOME=C:\Users\username\AppData\Local\Programs\Python\Python310
Build-Windows.bat x64 vs2022+gcc Setup\HammerDB-Windows.bawt update</programlisting>

        <para>The first step the build will take is to download the required
        packages and build instructions from www.hammerdb.com, On Windows the
        MYSYS/MinGW package will also be downloaded. These will be stored in
        the Bawt-2.1.0/InputLibs directory. Both checksums and modification
        times are verified with the remote packages. If a package is already
        present with the same checksum and modification time, it will not be
        downloaded again if already present. Also some packages such as Tcl
        have been modified from the original and therefore only the packages
        from www.hammerdb.com should be used. In some cases different packages
        are used on Linux and Windows and therefore the package listing
        differs between the OS environments, this is expected and the
        following listing shows a Linux example.</para>

        <para><programlisting>awthemes-9.3.1.7z  libressl-2.6.4.7z  pgtcl-2.1.1.7z  tcltls-1.7.22.7z
awthemes.bawt      libressl.bawt      pgtcl.bawt      tcltls.bawt
clearlooks-1.0.7z  mariatcl-0.1.7z    redis-0.1.7z    Tk-8.6.12.7z
clearlooks.bawt    mariatcl.bawt      redis.bawt      Tk.bawt
db2tcl-2.0.1.7z    mysqltcl-3.052.7z  Tcl-8.6.12.7z   tkblt-3.2.23.7z
db2tcl.bawt        mysqltcl.bawt      Tcl.bawt        tkblt.bawt
expect-5.45.4.7z   oratcl-4.6.7z      tclpy-0.4.7z    tksvg-0.5.7z
expect.bawt        oratcl.bawt        tclpy.bawt      tksvg.bawt</programlisting>Allow
        the build to complete. A summary will be given of the packages built
        and the location of the build given, for example for Linux as
        follows:</para>

        <programlisting>17:31:43 &gt; Creating Distribution tar.gz in /opt/HammerDB-master/Build/BawtBuild/Linux/x64/Release/Distribution
17:31:43 &gt;   TarGzip
               Source directory: /opt/HammerDB-master/Build/BawtBuild/Linux/x64/Release/Distribution/HammerDB-4.6
               Tar file        : /opt/HammerDB-master/Build/BawtBuild/Linux/x64/Release/Distribution/HammerDB-4.6-Linux.tar.gz
17:31:43 &gt; End FinalizeStage

17:31:43 &gt; Summary
           Setup file     : /opt/HammerDB-master/Build/Bawt-2.1.0/Setup/HammerDB-Linux.bawt
           Build directory: /opt/HammerDB-master/Build/BawtBuild/Linux/x64/Release/Build
           Architecture   : x64
           Compilers      : gcc
           Global stages  : Finalize
           #  : Library Name         Version    Build time      Stages
           ----------------------------------------------------------------------
             1: Tcl                  8.6.12     1.54 minutes    Clean Extract Configure Compile Distribute
             2: Tk                   8.6.12     0.36 minutes    Clean Extract Configure Compile Distribute
             3: awthemes             9.3.1      0.00 minutes    Clean Extract Configure Compile Distribute
             4: clearlooks           1.0        0.00 minutes    Clean Extract Configure Compile Distribute
             5: db2tcl               2.0.1      0.10 minutes    Clean Extract Configure Compile Distribute
             6: expect               5.45.4     0.12 minutes    Clean Extract Configure Compile Distribute
             7: libressl             2.6.4      0.94 minutes    Clean Extract Configure Compile Distribute
             8: mariatcl             0.1        0.04 minutes    Clean Extract Configure Compile Distribute
             9: mysqltcl             3.052      0.04 minutes    Clean Extract Configure Compile Distribute
            10: oratcl               4.6        0.05 minutes    Clean Extract Configure Compile Distribute
            11: pgtcl                2.1.1      0.05 minutes    Clean Extract Configure Compile Distribute
            12: redis                0.1        0.00 minutes    Clean Extract Configure Compile Distribute
            13: tclpy                0.4        0.01 minutes    Clean Extract Configure Compile Distribute
            14: tcltls               1.7.22     0.18 minutes    Clean Extract Configure Compile Distribute
            15: tkblt                3.2.23     0.21 minutes    Clean Extract Configure Compile Distribute
            16: tksvg                0.5        0.05 minutes    Clean Extract Configure Compile Distribute
           ----------------------------------------------------------------------
           Total: 3.70 minutes</programlisting>

        <para>On Windows a graphical viewer monitors the progress of the
        build. Note that on Windows with the Windows Defender virus real-time
        protection enabled the build can be slower as more time is taken as
        each source file is scanned. If preferred disabling real-time
        protection for the duration of the build will enable it to complete in
        approximately half of the time of a build with virus scanning
        enabled.</para>

        <figure>
          <title>Windows build</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch1-9a.png"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>On Windows both Visual Studio and gcc compilers are used with
        Visual Studio for performance critical packages (i.e. Tcl, Tk and
        databases interfaces) and gcc for other packages. For this reason Tcl
        and Tk are compiled twice with both Visual Studio and gcc for the
        respective dependent packages to be compiled against, however the
        final distribution only includes the Tcl and Tk builds compiled with
        Visual Studio. For this reason the Windows build takes more time than
        the comparative Linux build.</para>

        <para>You now have your own distribution of HammerDB with the latest
        source code.</para>

        <para>This will be in the following directory on Linux:</para>

        <programlisting>HammerDB/Build/BawtBuild/Linux/x64/Release/Distribution</programlisting>

        <para>and the following directory on Windows:</para>

        <programlisting>HammerDB-master\Build\BawtBuild\vs2022\x64\Release\Distribution</programlisting>

        <para>Note that the entire distribution including the HammerDB source
        is built and installed in this directory and therefore you should run
        or copy the distribution from this location. If trying to run HammerDB
        from the HammerDB source root directory it will not run correctly
        instead copy or run the built distribution from the locations
        above.</para>

        <para>Alongside the HammerDB distribution directory is also a tar.gz
        file on Linux and zip file on Windows of the directory.</para>

        <para>You can run the hammerdbcli librarycheck command to verify that
        the libraries built correctly.</para>

        <programlisting>/opt/HammerDB/Build/BawtBuild/Linux/x64/Release/Distribution$ ls
HammerDB-4.6  HammerDB-4.6-Linux.tar.gz
$ cd HammerDB-4.6
/opt/HammerDB/Build/BawtBuild/Linux/x64/Release/Distribution/HammerDB-4.6$ ./hammerdbcli
HammerDB CLI v4.6
Copyright (C) 2003-2022 Steve Shaw
Type "help" for a list of commands
hammerdb&gt;librarycheck
Checking database library for Oracle
Success ... loaded library Oratcl for Oracle
Checking database library for MSSQLServer
Success ... loaded library tdbc::odbc for MSSQLServer
Checking database library for Db2
Success ... loaded library db2tcl for Db2
Checking database library for MySQL
Success ... loaded library mysqltcl for MySQL
Checking database library for PostgreSQL
Success ... loaded library Pgtcl for PostgreSQL
Checking database library for MariaDB
Success ... loaded library mariatcl for MariaDB</programlisting>

        <para>You can also browse all of the C source code for the libraries
        you have built in the Build directory for example:</para>

        <programlisting>HammerDB/Build/BawtBuild/Linux/x64/Release/Build</programlisting>
      </section>

      <section>
        <title>HammerDB License</title>

        <para>HammerDB is copyrighted and distributed under the GNU General
        Public License Version 3.0 (GPL v3). When building from source this
        copyright remains and any distributions must be made under GPLv3
        complying to the following requirements.</para>

        <para>1. Include a copy of the full license text</para>

        <para>2. State all significant changes made to the original
        software</para>

        <para>3. Make available the original source code when you distribute
        any binaries based on the licensed work</para>

        <para>4. Include a copy of the original copyright notice</para>

        <para>It is not permitted to modify the license on a build from source
        distribution.</para>
      </section>
    </section>
  </chapter>

  <chapter>
    <title>Quick Start</title>

    <para>Before proceeding you should have your database software installed
    and running and be familiar with basic functionality of connecting to your
    database. After starting HammerDB, firstly you will need to select which
    benchmark and database you wish to use by choosing Benchmark from under
    the Options menu or double on your chosen database in the tree-view. The
    initial settings are determined by the values in your xml configuration
    file. Select your chosen database and TPROC-C and press OK. This Quick
    Start guide uses Microsoft SQL Server, however the process is the same for
    all other supported databases.</para>

    <section>
      <title>Building the Schema</title>

      <para><figure>
          <title>Benchmark Options</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch2-1.PNG"/>
            </imageobject>
          </mediaobject>
        </figure></para>

      <para>Click on the Benchmark tree view, under TPROC-C select TPROC-C
      Schema Build Options to display the TPROC-C Schema Options window.
      Within this window enter the connection details of your database
      software. These options will vary depending on the database chosen.
      Select a number of warehouses, 10 is good for a first test and set the
      Virtual Users to build schema to the number of CPU cores on your system.
      Click OK.</para>

      <figure>
        <title>Build Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch2-2.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>.</para>

      <para>Double-click on Build in the tree view and you will receive a
      prompt on the settings chosen to build the schema. Click Yes.</para>

      <para><figure>
          <title>Create Schema</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch2-3.PNG"/>
            </imageobject>
          </mediaobject>
        </figure></para>

      <para>Observe that HammerDB begins to build the schema with multiple
      users.</para>

      <figure>
        <title>Building Schema</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch2-4.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Wait until the schema build completes and then click on the red
      button, tagged with Destroy Virtual Users.</para>

      <figure>
        <title>Schema build complete</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch2-5.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Within the tree-view Select Driver Script Options. Leave the
      settings at Test Driver Script and click OK</para>
    </section>

    <section>
      <title>Run a Test Workload</title>

      <para><figure>
          <title>Driver Options</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch2-6.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>Observe that the Driver Script is Loaded. Clicking on Load
      will reload the script.</para>

      <figure>
        <title>Driver Script Loaded</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch2-7.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Further down the tree-view under Virtual User select Options and
      select "2" for the number of Virtual Users, click OK.</para>

      <figure>
        <title>Virtual User Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch2-8.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Click on Create and the Virtual Users will be created and waiting
      to run in an idle status. (Clicking run first will run both Create and
      Run Immediately).</para>

      <para><figure>
          <title>Virtual Users Created</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch2-9.PNG"/>
            </imageobject>
          </mediaobject>
        </figure></para>

      <para>Double-click on Run - you can now observe the Virtual Users
      running a workload against the database. When ready press the red stop
      button to stop the workload.</para>

      <para><figure>
          <title>Virtual Users Running</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch2-10.PNG"/>
            </imageobject>
          </mediaobject>
        </figure></para>
    </section>

    <section>
      <title>Run a Timed Workload</title>

      <para>the Test script is to check connectivity and diagnose performance
      and configuration errors. It is the Timed Workload that should be used
      to conduct performance tests. Under Driver Options select Timed Driver
      Script and click OK, the Timed Driver Script is now loaded.</para>

      <para/>

      <figure>
        <title>Driver Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch2-11.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Verify the Virtual User Options.</para>

      <figure>
        <title>Virtual Users</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch2-12.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Click create Virtual User and observe that for Timed workloads an
      additional Virtual User has been created. This Virtual User does not run
      the workload but provides the timing and monitoring
      functionality.</para>

      <figure>
        <title>Virtual User and Monitor Created</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch2-13.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Click Run, the workload will begin but this time without the
      Virtual User output being written to the screen. The Monitor Virtual
      User will provide information on Timing as the workload
      progresses.</para>

      <figure>
        <title>Timed Workload Running</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch2-14.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>On completion observe that the Monitor Virtual User reports a
      value for NOPM and a value for TPM. NOPM stands for New Orders per
      Minute and is extracted from the database schema and is therefore
      database independent meaning it is valid to compare between different
      databases. TPM is the transactions per minute and is a unique value to
      how each database processes transactions. NOPM is the key performance
      metric however TPM is the metric that correlates with database
      performance tools measurement of transactions per second or minute and
      can therefore be used by database engineers for deeper analysis of
      database performance.</para>

      <figure>
        <title>Test Result</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch2-15.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>

    <section>
      <title>Using the Transaction Counter</title>

      <para>During a workload press the Transaction Counter button. This will
      report the TPM value at a timed interval (default 10 seconds) to the
      screen.</para>

      <figure>
        <title>Transaction Counter</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch2-16.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>

    <section>
      <title>Deleting the Schema</title>

      <para>When complete to delete the schema, select the Delete option under
      Schema options.</para>

      <para><figure>
          <title>Delete Schema</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="../DocBook/docs/images/ch2-16a.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>Confirm the Schema delete dialog</para>

      <figure>
        <title>Confirm Deletion</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="../DocBook/docs/images/ch2-16b.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>HammerDB will report that the Schema has been successfully
      deleted.</para>

      <figure>
        <title>Schema deleted</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="../DocBook/docs/images/ch2-16c.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>
  </chapter>

  <chapter>
    <title>Introduction to OLTP Testing (TPROC-C derived from TPC-C)</title>

    <section>
      <title>What is a Transactional Workload</title>

      <para>A transactional or OLTP (online transaction processing) workload
      is a workload typically identified by a database receiving both requests
      for data and multiple changes to this data from a number of users over
      time where these modifications are called transactions. Each database
      transaction has a defined beginning point, manipulates and modifies the
      data within the database and either commits the changes or rollbacks the
      changes to the starting point. A database must adhere to the ACID
      (Atomicity, Consistency, Isolation, Durability) properties to ensure
      that the database remains consistent whilst processing transactions.
      Database systems that process transactional workloads are inherently
      complex in order to manage the user sessions access to same data at the
      same time, processing the transactions in isolation whilst keeping the
      database consistent and recoverable. People will typically interact with
      OLTP systems on a regular basis with examples such as an online grocery
      ordering and delivery system or an airline reservation system.
      Performance and scalability are essential properties of systems designed
      to process transactional workloads. The TPC-C benchmark is a benchmark
      designed by the TPC to measure the performance of the software and
      hardware of a relational database system to process these
      workloads.</para>
    </section>

    <section>
      <title>What is the TPC and the TPROC-C workload derived from
      TPC-C?</title>

      <para>Designing and implementing a database benchmark is a significant
      challenge. Many performance tests and tools experience difficulties in
      comparing system performance especially in the area of scalability, the
      ability of a test conducted on a certain system and schema size to be
      comparable with a test on a larger scale system. When system vendors
      wish to publish validated benchmark information about database
      performance they have needed to access sophisticated test specifications
      and the TPC is the industry body most widely recognized for defining
      benchmarks. TPC specifications are the only published benchmarks in the
      database industry recognized by all of the leading database vendors.
      TPC-C is the benchmark published by the TPC for Online Transaction
      Processing and you can view the published TPC-C results at the TPC
      website.</para>

      <para>The TPC Policies allow for derivations of TPC Benchmark Standards
      that comply with the TPC Fair Use rules. TPROC-C is the OLTP workload
      implemented in HammerDB derived from the TPC-C specification with
      modification to make running HammerDB straightforward and cost-effective
      on any of the supported database environments. The HammerDB TPROC-C
      workload is an open source workload derived from the TPC-C Benchmark
      Standard and as such is not comparable to published TPC-C results, as
      the results comply with a subset rather than the full TPC-C Benchmark
      Standard. The name for the HammerDB workload TPROC-C means "Transaction
      Processing Benchmark derived from the TPC "C" specification".</para>
    </section>

    <section>
      <title>HammerDB TPROC-C workload</title>

      <para>The HammerDB TPROC-C workload is intentionally not fully optimized
      and not biased towards any particular database implementation or system
      hardware, being open source you are free to inspect all of the HammerDB
      source code and to submit pull requests to update or enhance the
      workloads. The intent is to provide an out-of-the-box type experience
      when testing a database without requiring complex configurations or
      additional third-party software in addition to both HammerDB and the
      database you are testing. HammerDB can be run on any environment from a
      simple laptop based express type database install right through to 8, 16
      and 32 CPU socket servers and clusters. The crucial element is to
      reiterate the point made in the previous section that the HammerDB
      workloads are designed to be reliable, scalable and tested to produce
      accurate, repeatable and consistent results. In other words HammerDB is
      designed to measure relative as opposed to absolute database performance
      between systems. What this means is if you run a test against one
      particular configuration of hardware and software and re-run the same
      test against exactly the same configuration you will get exactly the
      same result within the bounds of the random selection of transactions
      which will typically be within 1%. Any differences between results are
      directly as a result of changes you have made to the configuration (or
      management overhead of your system such as database checkpoints or
      user/administrator error). Testing has proven that HammerDB tests re-run
      multiple times unattended (see the autopilot feature) on the same
      reliable configuration produce performance profiles that will overlay
      each other almost identically. The Figure below illustrates an example
      of this consistency and shows the actual results of 2 sequences of tests
      run unattended one after another against one of the supported databases
      with the autopilot feature from 1 to 144 virtual users to test
      modifications to a WAL (Write Ahead Log File). In other words HammerDB
      will give you the same results each time, if your results vary you need
      to focus entirely on your database, OS and hardware
      configuration.</para>

      <para/>

      <figure>
        <title>WAL Test</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch3-1.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Consequently when you begin to make changes you are able to
      quantify the impact of these changes. Examples include modifying
      database parameters, changing database software or operating system or
      upgrading or modifying hardware. Taking a baseline of a database system
      with a HammerDB workload is an ideal method to ensure that optimal
      database configurations are engineered put into production.</para>
    </section>

    <section>
      <title>Comparing HammerDB results</title>

      <para>HammerDB implements a workload called TPROC-C based on the TPC-C
      specification called TPROC-C however does NOT implement a full
      specification TPC-C benchmark and the transaction results from HammerDB
      cannot be compared with the official published TPC-C benchmarks in any
      manner. Official Audited TPC-C benchmarks are extremely costly, time
      consuming and complex to establish and maintain. The HammerDB
      implementation based on the specification of the TPC-C benchmark is
      designed to capture the essence of TPC-C in a form that can be run at
      low cost on any system bringing professional, reliable and predictable
      load testing to all database environments. For this reason HammerDB
      results cannot and should NOT be compared or used with the term tpmC in
      any circumstance. HammerDB workloads produce 2 statistics to compare
      systems called TPM and NOPM respectively. NOPM value is based on a
      metric captured from within the test schema itself. As such NOPM (New
      Orders per minute) as a performance metric independent of any particular
      database implementation is the recommended primary metric to use.</para>
    </section>

    <section>
      <title>Understanding the TPROC-C workload derived from TPC-C</title>

      <para>The TPC-C specification on which TPROC-C is based implements a
      computer system to fulfil orders from customers to supply products from
      a company. The company sells 100,000 items and keeps its stock in
      warehouses. Each warehouse has 10 sales districts and each district
      serves 3000 customers. The customers call the company whose operators
      take the order, each order containing a number of items. Orders are
      usually satisfied from the local warehouse however a small number of
      items are not in stock at a particular point in time and are supplied by
      an alternative warehouse. It is important to note that the size of the
      company is not fixed and can add Warehouses and sales districts as the
      company grows. For this reason your test schema can be as small or large
      as you wish with a larger schema requiring a more powerful computer
      system to process the increased level of transactions. The TPROC-C
      schema is shown below, in particular note how the number of rows in all
      of the tables apart from the ITEM table which is fixed is dependent upon
      the number of warehouses you choose to create your schema.</para>

      <para><figure>
          <title>TPROC-C Schema</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch3-2.png"/>
            </imageobject>
          </mediaobject>
        </figure>For additional clarity please note that the term Warehouse in
      the context of TPROC-C bears no relation to a Data Warehousing workload,
      as you have seen TPROC-C defines a transactional based system and not a
      decision support (DSS) one. In addition to the computer system being
      used to place orders it also enables payment and delivery of orders and
      the ability to query the stock levels of warehouses. Consequently the
      workload is defined by a mix of 5 transactions selected at random
      according to the balance of the percentage value shown as
      follows:</para>

      <itemizedlist>
        <listitem>
          <para>New-order: receive a new order from a customer: 45%</para>
        </listitem>

        <listitem>
          <para>Payment: update the customers balance to record a payment:
          43%</para>
        </listitem>

        <listitem>
          <para>Delivery: deliver orders asynchronously: 4%</para>
        </listitem>

        <listitem>
          <para>Order-status: retrieve the status of customers most recent
          order: 4%</para>
        </listitem>

        <listitem>
          <para>Stock-level: return the status of the warehouses inventory:
          4%</para>
        </listitem>
      </itemizedlist>
    </section>

    <section>
      <title>TPROC-C key similarities and differences from TPC-C</title>

      <para>HammerDB can be seen as a subset of the the full TPC-C
      specification, intentionally modified to make the workload simpler and
      easier to run. The key similarities are the schema definition and data
      and the 5 transactions implemented as stored procedures. The key
      difference is that by default HammerDB will run without keying and
      thinking time enabled (Note enabling event driven scaling will enable
      keying and thinking time to be run with a large number of user
      sessions). This means that HammerDB TPROC-C will run a CPU and memory
      intensive version of the TPC-C workload. In turn this also means that
      the number of virtual users and the required data set will be much
      smaller than a full TPC-C implementation to reach maximum levels of
      performance. HammerDB also does not implement terminals as the full
      specification does. Nevertheless with the HammerDB TPROC-C
      implementation a large number of client systems and third-party
      middleware is not required nor a very large data set to reach maximum
      levels of performance whilst still providing a robust test of relational
      databases capabilities.</para>
    </section>

    <section>
      <title>How many warehouses to create for the TPROC-C test</title>

      <para>This is a very typical FAQ and although detailed in the
      documentation some extra details may help in sizing and configuration.
      For a basic starting point create a schema with 250-500 warehouses per
      server CPU socket for more details size as follows.</para>

      <para>The official TPC-C test has a fixed number of users per warehouse
      and uses keying and thinking time so that the workload generated by each
      user is not intensive. However most people use HammerDB with keying and
      thinking time disabled and therefore each virtual user can approximately
      drive the CPU resources of one CPU core on the database server.
      Therefore the relationship between virtual users, warehouses and cores
      can be seen, you need considerably fewer virtual users and warehouses to
      drive a system to maximum throughput than the official test.</para>

      <para>Additionally it is important to understand the workload. By
      default each virtual user has the concept of a home warehouses where
      approximately 90% of its workload will take place. With HammerDB this
      home warehouse is chosen at random at the start of the test and remains
      fixed. For example it can then be understood that if you configure a
      schema with 1000 warehouses and run a test with 10 virtual users by
      default most of the workload will be concentrated upon 10 warehouses.
      (It is important to note the “by default” clause here as there are
      exceptions to change this behaviour if desired). Also with the home
      warehouse chosen at random it should be clear that number of warehouses
      should be configured so that when the maximum number of virtual users
      that you will run are configured there is a good chance that the
      selection of a home warehouse at random will be evenly distributed
      across the available warehouses with one or possibly 2 virtual users
      selecting the same home warehouse at random but not more.</para>

      <para>As an example configuring a 10 warehouse schema and running 100
      virtual users against this schema would be an error in configuration as
      it would be expected for 10 virtual users or more to select the same
      warehouse. Doing this would mean that the workload would spend
      considerably more time in lock contention and would not produce valid
      results. Typically an option of 4 to 5 warehouses per virtual user would
      be a minimum value to ensure an even distribution of virtual users to
      warehouse. Therefore for the 100 virtual users 400 to 500 warehouses
      should be a minimum to be configured. As noted configuring more should
      not have a major impact on results as depending on the number of virtual
      users used in the test most the warehouses will be idle (and ideally
      most of the warehouses you are using will be cached in memory in your
      buffer cache so the I/O to the data area is minimal).</para>

      <para>As one virtual user can drive most of the capacity of one CPU core
      the actual value for the number of warehouses you choose will depend
      upon the number of cores per socket. Note that if using CPUs with
      Hyper-Threading allow for additional CPU capacity, so size as if there
      were 35% more physical cores. Also depending on your chosen database
      some database software will not scale to fully utilise all cores, see
      the best practice guides for guidance on your chosen database. If CPU
      utilisation is limited then you will need fewer warehouses configured
      and virtual users for the test.</para>

      <para>It should also be clear that there is no completely accurate
      one-size-fits-all type guidance for warehouse sizing as different
      databases will scale differently and some may need more warehouses and
      virtual users than others to reach peak performance. A common error is
      to size many thousands of warehouses and virtual users with the aim of
      reaching high performance but instead resulting in high levels of
      contention and low performance. Even for highly scalable databases on
      large systems upper limits for tests without keying and thinking time
      are in the region of 2000 warehouses for up to 500 virtual users for
      maximum performance.</para>

      <para>The exceptions to these guidelines are given in the section
      Advanced Driver Script Options in the following Chapter namely the Use
      All Warehouses and Event Driven Scaling options. For advanced users
      these options enable increasing the amount of data area I/O and running
      more Virtual Users less intensively with keying and thinking time
      respectively.</para>
    </section>

    <section>
      <title>Publishing database performance results</title>

      <para>The goal of HammerDB is to make database performance data open
      source and enable database professionals a fast and low-cost to compare
      and contrast database systems. HamerDB maintains a list of <link
      xlink:href="https://www.hammerdb.com/benchmarks.html">3rd party
      publications on the HammerDB website</link>.</para>
    </section>
  </chapter>

  <chapter>
    <title>How to Run a TPROC-C Workload</title>

    <para>This Chapter provides a general overview on the HammerDB TPROC-C
    workload and gives you an introduction to conducting OLTP (Online
    Transaction Processing) workloads on all of the supported databases. This
    will equip you with the essentials for assessing the ability of any system
    for processing transactional workloads.</para>

    <section>
      <title>Test Network Configuration</title>

      <para>You require the database server to be tested known as the system
      under test (SUT) installed and configured with the target database
      server. You also require a load generation server to run HammerDB
      installed with the HammerDB software and a database client. Typically
      the load generation server is run on a separate system from the SUT with
      the load generated across the network. It is possible to run HammerDB on
      the same system as the SUT however this will be expected to produce
      different results from a network based load. For example where the
      database software is highly scalable then running HammerDB on the same
      system will result in lower performance as the database software will
      not be able to take advantage of all of the available CPU. Conversely
      where the database software is less scalable and there is more network
      overhead it can take more virtual users to reach the same levels of
      performance using an additional load generation server compared to
      running HammerDB on the SUT. Both the SUT and the load generation server
      may be virtualized or container databases although similarly results may
      differ from a native hardware based installation. In all cases when
      comparing performance results you should ensure that you are comparing
      across the same configurations test network configurations.</para>

      <section>
        <title>SUT Database Server Configuration</title>

        <para>The database server architecture to be tested must meet the
        minimum requirements for your chosen database software, however to
        reach maximum performance it is likely that the specifications will
        considerably exceed these standard. To run a HammerDB transactional
        load test there are minimum requirements in memory and I/O (disk
        performance) to prevent these components being a bottleneck on
        performance. For a configuration requiring the minimal level of memory
        and I/O to maximize CPU utilization keying and thinking time should be
        set to FALSE (keying and thinking time is detailed later in this
        guide). To achieve this you should aim to create a schema with
        approximately 250-500 warehouses per CPU socket. By default each
        Virtual User will select a home warehouse at random and most of its
        work takes place on that home warehouse and therefore the schema
        sizing of 250-500 warehouses per socket should ensure that when the
        Virtual Users login the choice of a home warehouse at random is evenly
        distributed without a large number of Virtual Users selecting the same
        home warehouse. As long as it is not too small resulting in contention
        the schema size should not significantly impact results when testing
        in a default configuration. You should have sufficient memory to cache
        as much of your test schema in memory as possible. If keying and
        thinking time is set to TRUE you will need a significantly larger
        schema and number of virtual users to create a meaningful system load
        and should consider the advanced event-driven scaling option.
        Reductions in memory will place more emphasis on the I/O performance
        of the database containing the schema. If the allocated memory is
        sufficient most of the data will be cached during an OLTP test and I/O
        to the data area will be minimal. As a consequence the key I/O
        dependency will be to the redo/WAL/transaction logs for both bandwidth
        and sequential write latency. Modern PCIe SSDs when correctly
        configured have been shown to provide the capabilities to sustain high
        performance transaction logging.</para>
      </section>

      <section>
        <title>Load Generation Server Configuration</title>

        <para>The most important component of the load generation server is
        the server processor. The overall load generation server capacity
        required depends on the system capabilities of the SUT. It is
        recommend to use an up to date multi-core processor. HammerDB is a
        multi-threaded application and implicitly benefits from a multi-core
        server CPU. To determine whether CPU capacity is sufficient for
        testing you can monitor the CPU utilisation with HammerDB Metrics. CPU
        utilisation reaching 100% is an indication that the CPU on the load
        generation server is limiting performance. Load generation memory
        requirements are dependent on the operating system configuration and
        the number of virtual users created with each virtual user requiring
        its own database client. Typically server sizing guidelines should be
        within the limits expected to support a real user count. Multiple load
        generation servers connected in a “master-slave” configuration are
        enabled within HammerDB to exceed the capacity of a single load
        generation client. The load generation server does not need to be
        running the same version of SQL Server as the SUT.</para>
      </section>

      <section>
        <title>CPU Single-Threaded Performance Calibration</title>

        <para>By far one of the most common configuration errors with database
        performance testing is to have configured the CPUs to run in powersave
        mode. On some Linux operating systems this is the default
        configuration and therefore it is recommended to verify the CPU
        single-threaded performance and operating mode before running database
        workloads. One way to do this is to use the <link
        xlink:href="http://www.juliandyke.com/CPUPerformance/CPUPerformance.php">Julian
        Dyke CPU performance test</link> (referenced by permission of Julian
        Dyke and there are versions shown below to run directly in HammerDB
        and for Oracle PL/SQL and SQL Server T-SQL). Note that the timings are
        not meant to equivalent and it is expected that the HammerDB based
        test is approximately twice as fast as PL/SQL or T-SQL. The reason for
        the faster performance is that the TCL version is compiled into
        bytecode and you can observe this by running a Linux utility such as
        perf to see that the top function is TEBCresume. (Tcl Execute ByteCode
        Resume). During normal HammerDB operations TEBCResume should also be
        the top client function for the same reason.</para>

        <programlisting>Samples: 67K of event 'cycles:ppp', Event count (approx.): 33450114923
Overhead  Shared Object                  Symbol
<emphasis role="bold">  33.56%  libtcl8.6.so                   [.] TEBCresume</emphasis>
   7.68%  libtcl8.6.so                   [.] Tcl_GetDoubleFromObj
   6.28%  libtcl8.6.so                   [.] EvalObjvCore
   6.14%  libtcl8.6.so                   [.] TclNRRunCallbacks
</programlisting>

        <para>The goal of running these tests is to ensure that your CPU runs
        the test at the CPU advertised boost frequency. To do this you can use
        the turbostat utility on Linux and the Task Manager utility on
        Windows. By default the tests run for 10000000 iterations however this
        can be extended if desired to allow sufficient time to monitor the
        boost frequency is operational. For the HammerDB version save the
        script shown and run it using the CLI. A commented out command is
        shown that can be uncommented to observe the bytecode for a particular
        procedure.</para>

        <programlisting>proc runcalc {} {
set n 0
for {set f 1} {$f &lt;= 10000000} {incr f} {
set n [ expr {[::tcl::mathfunc::fmod $n 999999] + sqrt($f)} ] 
}
return $n
}
#puts "bytecode:[::tcl::unsupported::disassemble proc runcalc]"
set start [clock milliseconds]
set output [ runcalc ]
set end [ clock milliseconds]
set duration [expr {($end - $start)}]
puts "Res = [ format %.02f $output ]"
puts "Time elapsed : [ format %.03f [ expr $duration/1000.0 ] ]"</programlisting>

        <para>The expected result is 873729.72 as shown in the example output.
        Depending on the CPU used the default completion time should be up to
        3 seconds, if longer then investigating the CPU configuration is
        recommended.</para>

        <programlisting>hammerdb&gt;source CPUTEST.tcl
Res = 873729.72
Time elapsed : 2.990

hammerdb&gt;source CPUTEST.tcl
Res = 873729.72
Time elapsed : 2.966

hammerdb&gt;source CPUTEST.tcl
Res = 873729.72
Time elapsed : 2.980

hammerdb&gt;source CPUTEST.tcl
Res = 873729.72
Time elapsed : 2.976

hammerdb&gt;source CPUTEST.tcl
Res = 873729.72
Time elapsed : 2.972

hammerdb&gt;source CPUTEST.tcl
Res = 873729.72
Time elapsed : 2.988

hammerdb&gt;source CPUTEST.tcl
Res = 873729.72
Time elapsed : 2.976

</programlisting>

        <para>The following listing shows the original Julian Dyke PL/SQL CPU
        test that can be run in an Oracle instance. Example timings are given
        at the website link above.</para>

        <programlisting>SET SERVEROUTPUT ON
SET TIMING ON
 
DECLARE
  n NUMBER := 0;
BEGIN
  FOR f IN 1..10000000
  LOOP
    n := MOD (n,999999) + SQRT (f);
  END LOOP;
  DBMS_OUTPUT.PUT_LINE ('Res = '||TO_CHAR (n,'999999.99'));
END;
/</programlisting>

        <para>The following listing shows the same routine in T-SQL for SQL
        Server.</para>

        <programlisting>USE [tpcc]
GO
SET ANSI_NULLS ON
GO
CREATE PROCEDURE [dbo].[CPUSIMPLE] 
AS
   BEGIN
      DECLARE
         @n numeric(16,6) = 0,
         @a DATETIME,
         @b DATETIME
      DECLARE
         @f int
      SET @f = 1
      SET @a = CURRENT_TIMESTAMP
      WHILE @f &lt;= 10000000 
         BEGIN
      SET @n = @n % 999999 + sqrt(@f)
            SET @f = @f + 1
         END
         SET @b = CURRENT_TIMESTAMP
         PRINT ‘Timing = ‘ + ISNULL(CAST(DATEDIFF(MS, @a, @b)AS VARCHAR),”)
         PRINT ‘Res = ‘+ ISNULL(CAST(@n AS VARCHAR),”)
   END</programlisting>
      </section>

      <section>
        <title>Administrator PC Configuration</title>

        <para>When using the graphical version of HammerDB the administrator
        PC must have the minimal requirement to display the graphical output
        from the load generation server. The PC should also have the ability
        to connect to the SUT to monitor performance by the installation of an
        appropriate database client. For Linux clients where remote desktop
        displays are used it is recommended to use VNC instead of X Windows
        for better graphics performance in particular when using v4.0 SVG
        based scalable graphics. running X windows over long distances is
        known to impact display refresh rates and is not a HammerDB
        issue.</para>
      </section>
    </section>

    <section>
      <title>Installation and Configuration</title>

      <para>This section details database specific installation and
      configuration requirements.</para>

      <section>
        <title>Oracle</title>

        <para>You should have the Oracle database software installed and a
        test database created and running. During the installation make a note
        of your system user password, you will need it for the test schema
        creation. (Note that the system user is used and not sys). You may at
        your discretion use an existing database however please note that
        HammerDB load testing can drive your system utilization to maximum
        levels and therefore testing an active production system is not
        recommended. After your database server is installed you should create
        a tablespace into which the test data will be installed allowing disk
        space according to the guide previously in this chapter. For example
        the following shows creating the tablespace in the ASM disk group
        DATA:</para>

        <programlisting>SQL&gt; create bigfile tablespace tpcctab datafile '+DATA' size 100g; </programlisting>

        <para>If you are running HammerDB against Oracle on Windows add the
        following entry to your SQLNET.ORA file for the reasons described in
        the HammerDB release notes.</para>

        <programlisting>SQLNET.AUTHENTICATION_SERVICES = (NTS)
DIAG_ADR_ENABLED=OFF DIAG_SIGHANDLER_ENABLED=FALSE
DIAG_DDE_ENABLED=FALSE</programlisting>

        <para>You must be able to connect from your load generation server to
        your SUT database server across the network using Oracle TNS. This
        will involve successful configuration of your listener on the SUT
        database server and the tnsnames.ora file on the load generation
        server. You can troubleshoot connectivity issues using the ping,
        tnsping and sqlplus commands on the load generation client and the
        lsnrctl command on the SUT database server. For example a successful
        tnsping test looks as follows:</para>

        <programlisting>[oracle@MERLIN ~]$ tnsping PDB1

TNS Ping Utility for Linux: Version 12.1.0.1.0 - Production on 21-MAY-2014 05:40:49

Copyright (c) 1997, 2013, Oracle.  All rights reserved.

Used parameter files:
/u01/app/oracle/product/12.1.0/dbhome_1/network/admin/sqlnet.ora

Used TNSNAMES adapter to resolve the alias
Attempting to contact (DESCRIPTION = (ADDRESS = (PROTOCOL = TCP)(HOST = merlin)(PORT = 1521)) (CONNECT_DATA = (SERVER = DEDICATED) (SERVICE_NAME = pdb1)))
OK (30 msec)

</programlisting>

        <para>Note that where the instant client is being used on the load
        generation server you should configure the TNS_ADMIN environment
        variable to the location where the tnsnames.ora and sqlnet.ora files
        are installed. When you have installed the load generation server and
        SUT database and have verified that you can communicate between them
        using Oracle TNS you can proceed to building a test schema.</para>
      </section>

      <section>
        <title>Microsoft SQL Server</title>

        <para>You will have configured Microsoft SQL Server during
        installation to authenticate either with Windows Authentication or
        with SQL Server and Windows Authentication. HammerDB will permit
        either method to be used however you must have the corresponding
        configuration on your SQL Server. Additionally your chosen method of
        authentication is required to be compatible with your chosen ODBC
        driver. To discover the available drivers use the ODBC Data Source
        Administrator tool on Windows and the command database drivers on
        Linux. The driver name should be entered into HammerDB exactly as
        shown in the Data Source Administrator. The default value is “ODBC
        Driver 18 for SQL Server” for both Windows and Linux. From v4.10
        HammerDB will also permit authentication with Microsoft Entra where
        available with your configuration and a minimum version of SQL Server
        2022. </para>

        <figure>
          <title>ODBC Drivers</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch4-1.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>

      <section>
        <title>Db2</title>

        <para>To connect to Db2 requires the IBM CLI interface. Note that CLI
        in this context means "call level interface" and should not be
        confused the with the HammerDB command-line interface. Db2 CLI is the
        'C' language interface that HammerDB uses. ODBC is not used for
        HammerDB connectivity to Db2 however both ODBC and CLI drivers are
        packaged together and therefore for Db2 connectivity it is necessary
        to install the Db2 client software IBM Data Server Driver for ODBC and
        CLI. This is also typically installed with the Db2 database software.
        Configure your db2dsdriver.cfg file with the hostname, port and
        database that you have created on the server.</para>

        <programlisting>db2inst1:~/odbc_cli/clidriver/cfg&gt; more db2dsdriver.cfg
&lt;?xml version="1.0" encoding="UTF-8" standalone="no" ?&gt;
&lt;configuration&gt;
  &lt;dsncollection&gt;
    &lt;dsn alias="TPCC" host="db2v1064bit" name="TPCC" port="50001"/&gt;
  &lt;/dsncollection&gt;

  &lt;databases&gt;
    &lt;database host="db2v1064bit" name="TPCC" port="50001"/&gt;
  &lt;/databases&gt;
&lt;/configuration&gt;
</programlisting>

        <para>Options can be set in the db2cli.ini file.</para>

        <programlisting>[db2inst1@~/sqllib/cfg]$ more db2cli.ini 
[TPCC]
UID=db2inst1
PWD=ibmdb2
SysSchema=SYSIBM
SchemaList=”’SYSIBM’,’TPCC’”
DeferredPrepare=1
ConnectTimeout=10
ReceiveTimeout=120
LockTimeout=-1
AppendForFetchOnly=0
AutoCommit=1
ConnectType=1
CursorHold=OFF
TxnIsolation=1
StmtConcentrator=OFF
</programlisting>

        <para>You should have the Db2 database software installed and ready to
        accept connections as shown below.</para>

        <programlisting>db2inst1~]$ db2stop
04/12/2015 10:12:27     0   0   SQL1064N  DB2STOP processing was successful.
SQL1064N  DB2STOP processing was successful.
[db2inst1~]$ db2start
12/04/2015 10:12:31     0   0   SQL1063N  DB2START processing was successful.
SQL1063N  DB2START processing was successful.
[db2inst1~]$ db2
(c) Copyright IBM Corporation 1993,2007
Command Line Processor for DB2 Client 10.5.5

You can issue database manager commands and SQL statements from the command 
prompt. For example:
    db2 =&gt; connect to sample
    db2 =&gt; bind sample.bnd

For general help, type: ?.
For command help, type: ? command, where command can be
the first few keywords of a database manager command. For example:
 ? CATALOG DATABASE for help on the CATALOG DATABASE command
 ? CATALOG          for help on all of the CATALOG commands.

To exit db2 interactive mode, type QUIT at the command prompt. Outside 
interactive mode, all commands must be prefixed with 'db2'.
To list the current command option settings, type LIST COMMAND OPTIONS.

For more detailed help, refer to the Online Reference Manual.

db2 =&gt;
</programlisting>

        <para>With Db2 installed and running manually create and configure a
        Db2 Database according to your requirements. Pay particular attention
        to setting a LOGFILSIZ appropriate to your environment, otherwise you
        are likely to receive a transaction log full error message during the
        schema build. Additionally HammerDB is bufferpool and tablespace aware
        and therefore you may wish to create additional bufferpools specific
        to the tables that you are going create. The example below shows a
        configuration where a separate bufferpool has been created for each
        table solely to illustrate the usage of HammerDB parameters. You
        should also use the db2set command to set parameters appropriate to
        your system, for example setting DB2_LARGE_PAGE_MEM=DB for a large
        page configuration. Note that the commands below are examples only and
        should not (and are not) recommendations for optimal
        performance.</para>

        <programlisting>[db2inst1@ ~]$ db2 create database tpcc pagesize 8 k
DB20000I  The CREATE DATABASE command completed successfully.
[db2inst1@ ~]$ db2 update db cfg for tpcc using PCKCACHESZ 1631072
DB20000I  The UPDATE DATABASE CONFIGURATION command completed successfully.
[db2inst1@ ~]$ db2 update db cfg for tpcc using LOGFILSIZ 1048572
DB20000I  The UPDATE DATABASE CONFIGURATION command completed successfully.
[db2inst1@ ~]$ db2 update db cfg for tpcc using LOGPRIMARY 25 
DB20000I  The UPDATE DATABASE CONFIGURATION command completed successfully.
[db2inst1@ ~]$ db2 update db cfg for tpcc using LOGSECOND 5
DB20000I  The UPDATE DATABASE CONFIGURATION command completed successfully.
[db2inst1@ ~]$ db2 update db cfg for tpcc using LOGBUFSZ 17264
DB20000I  The UPDATE DATABASE CONFIGURATION command completed successfully.
[db2inst1@ ~]$ db2 update db cfg for tpcc using MINCOMMIT 1
DB20000I  The UPDATE DATABASE CONFIGURATION command completed successfully.
[db2inst1@ ~]$ db2 update db cfg for tpcc using NUM_IOSERVERS AUTOMATIC
DB20000I  The UPDATE DATABASE CONFIGURATION command completed successfully.
[db2inst1@ ~]$ db2 update db cfg for tpcc using DFT_PREFETCH_SZ AUTOMATIC
DB20000I  The UPDATE DATABASE CONFIGURATION command completed successfully.
[db2inst1@ ~]$ db2 update db cfg for tpcc using LOCKTIMEOUT 15
DB20000I  The UPDATE DATABASE CONFIGURATION command completed successfully.
[db2inst1@ ~]$ db2 update db cfg for tpcc using SOFTMAX 2500
DB20000I  The UPDATE DATABASE CONFIGURATION command completed successfully.
[db2inst1@ ~]$ 
[db2inst1@ ~]$ db2 connect to tpcc

   Database Connection Information

 Database server        = DB2/LINUXX8664 10.5.5
 SQL authorization ID   = DB2INST1
 Local database alias   = TPCC

[db2inst1@ ~]$ db2 create bufferpool C_BP immediate size 2500000 pagesize 8k
DB20000I  The SQL command completed successfully.
[db2inst1@ ~]$ db2 create large tablespace C_TS pagesize 8k managed by automatic storage bufferpool C_BP
DB20000I  The SQL command completed successfully.
[db2inst1@ ~]$ db2 create bufferpool D_BP immediate size 500000 pagesize 4k
DB20000I  The SQL command completed successfully.
[db2inst1@ ~]$ db2 create large tablespace D_TS pagesize 4k managed by automatic storage bufferpool D_BP
DB20000I  The SQL command completed successfully.
[db2inst1@ ~]$ db2 create bufferpool W_BP immediate size 500000 pagesize 4k
DB20000I  The SQL command completed successfully.
[db2inst1@ ~]$ db2 create large tablespace W_TS pagesize 4k managed by automatic storage bufferpool W_BP
DB20000I  The SQL command completed successfully.
[db2inst1@ ~]$ db2 create bufferpool I_BP immediate size 500000 pagesize 4k
DB20000I  The SQL command completed successfully.
[db2inst1@ ~]$ db2 create large tablespace I_TS pagesize 4k managed by automatic storage bufferpool I_BP
DB20000I  The SQL command completed successfully.
[db2inst1@ ~]$ db2 create bufferpool H_BP immediate size 2000000 pagesize 8k
DB20000I  The SQL command completed successfully.
[db2inst1@ ~]$ db2 create large tablespace H_TS pagesize 8k managed by automatic storage bufferpool H_BP
DB20000I  The SQL command completed successfully.
[db2inst1@ ~]$ db2 create bufferpool S_BP immediate size 2000000 pagesize 8k
DB20000I  The SQL command completed successfully.
[db2inst1@ ~]$ db2 create large tablespace S_TS pagesize 8k managed by automatic storage bufferpool S_BP
DB20000I  The SQL command completed successfully.
[db2inst1@ ~]$ db2 create bufferpool NO_BP immediate size 3000000 pagesize 8k
DB20000I  The SQL command completed successfully.
[db2inst1@ ~]$ db2 create large tablespace NO_TS pagesize 8k managed by automatic storage bufferpool NO_BP
DB20000I  The SQL command completed successfully.
[db2inst1@ ~]$ db2 create bufferpool OR_BP immediate size 3000000 pagesize 8k
DB20000I  The SQL command completed successfully.
[db2inst1@ ~]$ db2 create large tablespace OR_TS pagesize 8k managed by automatic storage bufferpool OR_BP
DB20000I  The SQL command completed successfully.
[db2inst1@ ~]$ db2 create bufferpool OL_BP immediate size 5000000 pagesize 8k
DB20000I  The SQL command completed successfully.
[db2inst1@ ~]$ db2 create large tablespace OL_TS pagesize 8k managed by automatic storage bufferpool OL_BP
DB20000I  The SQL command completed successfully.
</programlisting>
      </section>

      <section>
        <title>MySQL</title>

        <para>You should have the MySQL database software installed and
        running. Make sure you set a password for either the root user or a
        user with the correct privileges to create the TPROC-C database, for
        example the following on MySQL 8.0.</para>

        <programlisting>mysql&gt; alter user 'root'@'localhost' identified by 'mysql';
Query OK, 0 rows affected (0.00 sec)
</programlisting>

        <para>and the following on MySQL 5.6</para>

        <para><programlisting>-bash-4.1$ ./mysqladmin -u root password mysql</programlisting>By
        default a MySQL installation will allow connection to the local server
        only, you must grant permission to connect to the MySQL database from
        your load generation server, the following example grants all
        permissions to the root user on the system called merlin.home.</para>

        <programlisting>mysql&gt; grant all on *.* to root@'hummingbird.home' identified by 'mysql';
Query OK, 0 rows affected (0.00 sec)
mysql&gt; flush privileges;
Query OK, 0 rows affected (0.00 sec)
</programlisting>

        <para>Alternatively after the test database is created you can
        restrict the privileges to that databases only.</para>

        <programlisting>mysql&gt; grant all on tpcc.* to root@'hummingbird.home' identified by 'mysql';</programlisting>

        <para>When choosing a MySQL Server to test note that HammerDB load
        testing can drive your system utilization to maximum levels and
        therefore testing an active production system is not recommended. When
        you have installed the load generation server and SUT database and
        have verified that you can communicate between them by logging in
        remotely you can proceed to building a test schema.</para>

        <programlisting>mysql@hummingbird:~&gt; mysql -u root -p -h merlin.home
Enter password: 
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 19
Server version: 5.6.17 MySQL Community Server (GPL)
Copyright (c) 2000, 2014, Oracle and/or its affiliates. All rights reserved.
Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.
Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
mysql&gt; 
</programlisting>
      </section>

      <section>
        <title>MariaDB</title>

        <para>You should have the MariaDB database software installed and
        running. Make sure you set a password for either the root user or a
        user with the correct privileges to create the TPROC-C database, for
        example.</para>

        <programlisting>Server version: 10.5.10-MariaDB MariaDB Server
Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.
Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
MariaDB [(none)]&gt; SET PASSWORD FOR 'root'@localhost = PASSWORD("maria");
</programlisting>

        <para>When choosing a MariaDB Server to test note that HammerDB load
        testing can drive your system utilization to maximum levels and
        therefore testing an active production system is not recommended. When
        you have installed the load generation server and SUT database and
        have verified that you can communicate between them by logging in
        remotely you can proceed to building a test schema. Note that when
        HammerDB is running on the same system as the MariaDB server you have
        the option of connecting through sockets. The socket name is defined
        by the MariaDB socket parameter with the HammerDB default being
        /tmp/mariadb.sock.</para>

        <programlisting>./bin/mysql -uroot -S/tmp/mariadb.sock -p
Enter password: 
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 6
Server version: 10.5.10-MariaDB MariaDB Server
Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.
Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
MariaDB [(none)]&gt; </programlisting>
      </section>

      <section>
        <title>PostgreSQL</title>

        <para>You should have the PostgreSQL database software installed and a
        test database created and running. It is important to note that
        EnterpriseDB produce an enhanced version of the PostgreSQL database
        termed Postgres Plus Advanced Server. This version of PostgreSQL in
        particular includes compatibility features with the Oracle database
        such as PL/SQL support. Also Citus Data have produced a distributed
        PostgreSQL extension. For this reason the Hammer OLTP workload for
        PostgreSQL can operate in 3 modes. Firstly Oracle compatible mode uses
        PL/SQL and additional Postgres Plus Advanced Server features (such as
        DRITA snapshots) that will only operate against Enterprise DB Postgres
        Plus Advanced Server. Secondly Citus compatible mode supports
        distributed PostgreSQL with the Citus Data extension. Finally by not
        selecting Citus or Oracle compatibility HammerDB can continue to
        operate against a regular PostgreSQL build using native PostgreSQL
        features. You must ensure before proceeding with OLTP that you are
        aware of the version of PostgreSQL you have installed and the features
        available, if you wish to test Oracle compatibility then you must use
        Postgres Plus Advanced Server from EnterpriseDB and install in Oracle
        compatible mode, if you wish to use Citus Data you must be running
        against a Citus enabled installation either installed or in the
        cloud.</para>

        <para>During the installation make a note of your postgres superuser
        password, you will need it for the test schema creation. You must be
        able to connect from your load generation server to your SUT database
        server across the network. Firstly check your postgresql.conf file for
        the listen_addresses parameter. If this is set to localhost then only
        connections from the local server will be permitted. Use
        listen_addresses = ‘*’ to permit connections from all servers.
        Successful network connections will also involve successful
        configuration of your pg_hba.conf on the SUT database server. For
        example the following extract from a pg_hba.conf file from a
        PostgreSQL 9.3 installation shows trusted local connections on the SUT
        permitting connection without a password and remote connections from
        the Load Generation server with IP address 192.168.1.67 if the correct
        password is supplied. Note that the syntax of pg_hba.conf has changed
        for different versions of PostgreSQL and you should therefore consult
        the PostgreSQL documentation and sections further in this document to
        troubleshoot connectivity issues.</para>

        <programlisting># TYPE  DATABASE        USER            ADDRESS                 METHOD
# "local" is for Unix domain socket connections only
local      all       all  trust 
# IPv4 local connections:
host  all  all  127.0.0.1/32 md5
host  all  all  192.168.1.67/32 md5
</programlisting>
      </section>
    </section>

    <section>
      <title>Configuring Schema Build Options</title>

      <para>To create the OLTP test schema based on the TPROC-C specification
      you will need to select which benchmark and database you wish to use by
      choosing select benchmark from under the Options menu or double-clicking
      on the chosen database under the benchmark tree-view. (For the currently
      selected database double left-click shows the benchmark options and
      double right-click expands the tree view). The initial settings are
      determined by the values in your XML configuration files. The following
      example shows the selection of SQL Server however the process is the
      same for all databases.</para>

      <figure>
        <title>Benchmark Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch4-2.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>To create the TPROC-C schema select the TPROC-C schema options
      menu tab from the benchmark tree-view or the options menu. This menu
      will change dynamically according to your chosen database.</para>

      <figure>
        <title>Schema Build Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch4-3.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The “Build Options” section details the general login information
      and where the schema will be built and these are the only options of
      importance at this stage. Note that in any circumstance you do not have
      to rebuild the schema every time you change the “Driver Options”, once
      the schema has been built only the “Driver Options” may need to be
      modified. For the “Build Options” fill in the values according to the
      database where the schema will be built as follows.</para>

      <section>
        <title>Oracle Schema Build Options</title>

        <para><figure>
            <title>Oracle Build Options</title>

            <mediaobject>
              <imageobject>
                <imagedata fileref="docs/images/ch4-4.PNG"/>
              </imageobject>
            </mediaobject>
          </figure></para>

        <table>
          <title>Oracle Options</title>

          <tgroup cols="2">
            <thead>
              <row>
                <entry align="center">Option</entry>

                <entry align="center">Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>Oracle Service Name</entry>

                <entry>The Oracle Service Name is the service name that your
                load generation server will use to connect to the database
                running on the SUT database server.</entry>
              </row>

              <row>
                <entry>System User</entry>

                <entry>The “system” user or a user with system level
                privileges</entry>
              </row>

              <row>
                <entry>System User Password</entry>

                <entry>The system user password is the password for the
                “system” user you entered during database creation. The system
                user already exists in all Oracle databases and has the
                necessary permissions to create the TPROC-C user.</entry>
              </row>

              <row>
                <entry>TPROC-C User</entry>

                <entry>The TPROC-C user is the name of a user to be created
                that will own the TPROC-C schema. This user can have any name
                you choose but must not already exist and adhere to the
                standard rules for naming Oracle users. You may if you wish
                run the schema creation multiple times and have multiple
                TPROC-C schemas created with ownership under a different user
                you create each time.</entry>
              </row>

              <row>
                <entry>TPROC-C User Password</entry>

                <entry>The TPROC-C user password is the password to be used
                for the TPROC-C user you create and must adhere to the
                standard rules for Oracle user password. You will need to
                remember the TPROC-C user name and password for running the
                TPROC-C driver script after the schema is built.</entry>
              </row>

              <row>
                <entry>TPROC-C Default Tablespace</entry>

                <entry>The TPROC-C default tablespace is the tablespace that
                will be the default for the TPROC-C user and therefore the
                tablespace to be used for the schema creation. The tablespace
                must have sufficient free space for the schema to be
                created.</entry>
              </row>

              <row>
                <entry>TPROC-C Order Line Tablespace</entry>

                <entry>If the “Number of Warehouses” as described below is set
                to 200 or more then the “Partition Order Line Table” option
                becomes active. If this is selected then the option to select
                a different tablespace for the Order Line table only becomes
                active. For high performance schemas this gives the option of
                using both a separate tablespace and memory cache for the
                order line table with a different block size. Where a
                different cache and blocksize is used 16k is
                recommended.</entry>
              </row>

              <row>
                <entry>TPROC-C Temporary Tablespace</entry>

                <entry>The TPROC-C temporary tablespace is the temporary
                tablespace that already exists in the database to be used by
                the TPROC-C User.</entry>
              </row>

              <row>
                <entry>TimesTen Database Compatible</entry>

                <entry>When selected this option means that the Oracle Service
                Name should be a TimesTen Data Source Name and will grey out
                non-compatible options.</entry>
              </row>

              <row>
                <entry>Use Hash Clusters</entry>

                <entry>When Partitioning is selected this option enables the
                building of static tables as single table hash clusters and
                also disables table locks. These options can provide
                additional levels of scalability on high performance systems
                where contention is observed however will not provide
                significant performance gains on entry level systems. When
                Hash Clusters are enabled table locks are also disabled with
                the command "ALTER TABLE XXX DISABLE TABLE LOCK" and these
                locks will need to be re-enabled to drop the schema when
                required.</entry>
              </row>

              <row>
                <entry>Partition Tables</entry>

                <entry>When more than 200 warehouses are selected this option
                uses Oracle partitioning to divide the Order Line table into
                partitions of 100 warehouses each. Using partitioning enables
                scalability for high performance schemas and should be
                considered with using a separate tablespace for the Order Line
                table. Selecting this option also partitions the Orders and
                History tables.</entry>
              </row>

              <row>
                <entry>Number of Warehouses</entry>

                <entry>The Number of Warehouses is selected by a listbox. You
                should set this value to number of warehouses you have chosen
                for your test.</entry>
              </row>

              <row>
                <entry>Virtual Users to Build Schema</entry>

                <entry>The Virtual Users to Build Schema is the number of
                Virtual Users to be created on the Load Generation Server that
                will complete your multi-threaded schema build. You should set
                this value to either the number of warehouses you are going to
                create (You cannot set the number of virtual users higher than
                the number of warehouses value) or the number of
                cores/Hyper-Threads on your Load Generation Server. If you
                have a significantly larger core/Hyper-Thread count on your
                Database Server then also installing HammerDB locally on this
                server as well to run the schema build can take advantage of
                the higher core count to run the build more quickly.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </section>

      <section>
        <title>Microsoft SQL Server Schema Build Options</title>

        <para>The In-Memory OLTP implementation of HammerDB is intended to be
        as close as possible to the original on-disk HammerDB SQL Server
        Schema to enable comparisons between the two. The key areas for memory
        optimization are in-Memory optimized tables, the isolation level and
        the implementation of the stored procedures. Familiarity with the
        architecture of In-memory OLTP can benefit the understanding of the
        performance characteristics.</para>

        <section>
          <title>In-Memory Optimized Tables</title>

          <para>The key difference with the In-memory schema from the on-disk
          database is the organization of the tables. In-memory tables are
          implemented with hash indexes with no additional indexes created
          during the schema creation. Although the differences between hash
          and standard indexes are out of scope for this guide it is
          recommended to become familiar with the architecture as a key
          difference is the requirement to create all of the tables memory
          requirements ‘up-front’ with too little or too much memory impacting
          performance and therefore monitoring of the memory configuration
          usage is essential for workloads operating on In-memory databases.
          For a full implementation of in-memory tables a primary key is
          mandatory, however by definition the HISTORY table does not have a
          primary key. Therefore to implement all tables as in-memory an
          identity column has been added to the HISTORY table. It is important
          to note that despite the nomenclature of in-memory and on-disk
          databases in fact most of the workload of the on-disk database
          actually operates in-memory and high performance implementations can
          limit disk activity almost entirely to transaction logging in
          similarity to an in-memory database with persistence. Consequently
          orders of magnitude performance improvements should not be expected
          by moving to in-memory compared to a well optimised on-disk
          database.</para>

          <para>During schema creation HammerDB sets the option
          MEMORY_OPTIMIZED_ELEVATE_TO_SNAPSHOT for the memory optimized
          database. As a result the use of the snapshot isolation mode is
          mandatory and this will be set without intervention of the user. For
          the on-disk schema the default isolation level of READ COMMITTED is
          used with the addition of hints within the stored procedures for
          specific statements.</para>

          <para>In-memory OLTP introduces the concept of Native Compilation
          for stored procedures that access in-memory tables and the tables
          configured for HammerDB have been implemented with this in mind.
          However at current releases supported features of native compilation
          are highly restricted to the extent that it would not be possible to
          implement stored procedures in a native compilation form that would
          then provide a fair comparison with the on-disk schema. For this
          reason the same T-SQL stored procedures have been implemented with
          minor changes in areas such as removed hints locks and transaction
          isolation levels. Native compilation remains a consideration for
          future releases when the necessary features are supported to provide
          a fair comparison.</para>

          <para>An in-memory database must reside in a memory optimized
          filegroup with one or more containers. This database must be
          pre-created before running the HammerDB schema creation. If the
          database does not exist HammerDB will report the following
          error:</para>

          <programlisting>Database imoltp must be pre-created in a MEMORY_OPTIMIZED_DATA filegroup and empty, to specify an In-Memory build</programlisting>

          <para>If the database exists but is not in a
          MEMORY_OPTIMIZED_FILEGROUP HammerDB will report the following
          error.</para>

          <programlisting>Database imoltp exists but is not in a MEMORY_OPTIMIZED_DATA filegroup</programlisting>

          <para>Therefore to create an in-memory database firstly create a
          standard database using SSMS or at the command line as
          follows:</para>

          <programlisting>use imoltp
GO
ALTER DATABASE imoltp ADD FILEGROUP imoltp_mod CONTAINS memory_optimized_data
GO  
ALTER DATABASE imoltp ADD FILE (NAME='imoltp_mod', FILENAME='C:\Program Files\Microsoft SQL Server\MSSQL13.SQLDEVELOP\MSSQL\data\imoltp_mod') TO FILEGROUP imoltp_mod
GO

</programlisting>

          <para>For SQL Server on Linux specify the filesystem as
          follows:</para>

          <programlisting>ALTER DATABASE imoltp ADD FILE (NAME='imoltp_mod', FILENAME='C:\var\opt\mssql\data\imoltp_mod') TO FILEGROUP imoltp_mod
GO
</programlisting>

          <para>Once the above statements have been run successfully the
          database is ready for an in-memory schema creation.</para>
        </section>

        <section>
          <title>Build Options</title>

          <para><figure>
              <title>SQL Server Build Options</title>

              <mediaobject>
                <imageobject>
                  <imagedata fileref="docs/images/ch4-5.PNG"/>
                </imageobject>
              </mediaobject>
            </figure></para>

          <para><table>
              <title>SQL Server Build Options</title>

              <tgroup cols="2">
                <thead>
                  <row>
                    <entry align="center">Option</entry>

                    <entry align="center">Description</entry>
                  </row>
                </thead>

                <tbody>
                  <row>
                    <entry>SQL Server</entry>

                    <entry>The Microsoft SQL Server is the host name or host
                    name and instance that your load generation server will
                    use to connect to the database running on the SUT database
                    server.</entry>
                  </row>

                  <row>
                    <entry>TCP</entry>

                    <entry>Use the TCP Protocol</entry>
                  </row>

                  <row>
                    <entry>SQL Server Port</entry>

                    <entry>When TCP is enabled, the SQL Server Port is the
                    network port that your load generation server will use to
                    connect to the database running on the SUT database
                    server. In most cases this will be the default port of
                    1433 and will not need to be changed.</entry>
                  </row>

                  <row>
                    <entry>Azure</entry>

                    <entry>Include the Database name in the connect string
                    typical of Azure connections. To successfully build the
                    schema this database must be created and empty.</entry>
                  </row>

                  <row>
                    <entry>Encrypt Connection</entry>

                    <entry>Provides the same functionality as the "Encrypt
                    Connection" option in the SQL Server Management Studio
                    connect dialog. This option is the default with ODBC
                    Driver 18.</entry>
                  </row>

                  <row>
                    <entry>Trust Server Certificate</entry>

                    <entry>Provides the same functionality as the "Trust
                    Server Certificate" option in the SQL Server Management
                    Studio connect dialog. This option is the default with
                    ODBC Driver 18.</entry>
                  </row>

                  <row>
                    <entry>SQL Server ODBC Driver</entry>

                    <entry>The Microsoft SQL ODBC Driver is the ODBC driver
                    you will use to connect to the SQL Server database. To
                    view which drivers are available on Windows view the ODBC
                    Data Source Administrator Tool.</entry>
                  </row>

                  <row>
                    <entry>Authentication</entry>

                    <entry>When installing SQL Server on Windows you will have
                    configured SQL Server for Windows or Windows and SQL
                    Server Authentication. On Linux you will be using SQL
                    Server Authentication. If you specify Windows
                    Authentication then SQL Server will use a trusted
                    connection to your SQL Server using your Windows
                    credentials without requiring a username and password. If
                    SQL Server Authentication is specified and SQL
                    Authentication is enabled on your SQL Server then you will
                    be able connect by specifying a username and password that
                    you have already configured on your SQL Server. From v4.10
                    Microsoft Entra Authentication can also be used with a
                    minimum of ODBC Driver 18 and SQL Server 2022. Where Entra
                    is used an MSI Object ID can also be specified. If Entra
                    is chosen and the MSI Object ID is set to null then
                    Interactive Entra Authentication will be used prompting
                    for login credentials.</entry>
                  </row>

                  <row>
                    <entry>SQL Server User ID</entry>

                    <entry>The SQL Server User ID is the User ID of a user
                    that you have already created on your SQL Server.</entry>
                  </row>

                  <row>
                    <entry>SQL Server User Password</entry>

                    <entry>The SQL Server User Password is the Password
                    configured on the SQL Server for the User ID you have
                    specified. Note that when configuring the password on the
                    SQL Server there is a checkbox that when selected enforces
                    more complex rules for passwords or if unchecked enables a
                    simple password such as “admin”.</entry>
                  </row>

                  <row>
                    <entry>TRPOC-C SQL Server Database</entry>

                    <entry>The SQL Server Database is the name of the Database
                    to be created on the SQL Server to contain the schema. If
                    this database does not already exist then HammerDB will
                    create it, if the database does already exist and the
                    database is empty then HammerDB will use this existing
                    database. Therefore if you wish to create a particular
                    layout or schema then pre-creating the database and using
                    this database is an advanced method to use this
                    configuration.</entry>
                  </row>

                  <row>
                    <entry>In-Memory OLTP</entry>

                    <entry>Creates the database as In-Memory OLTP. The
                    database must be pre-created in a MEMORY_OPTIMIZED_DATA
                    filegroup and empty to specify an In-Memory build.</entry>
                  </row>

                  <row>
                    <entry>In-Memory Hash bucket Multiplier</entry>

                    <entry>The size of the In-memory database is specified at
                    creation time, however the OLTP/TPROC-C schema allows for
                    the insertion of additional rows. This value enables the
                    creation of larger tables for orders, new_order and
                    order_line to allow for these inserts. Note: Do not
                    specify too large a value or the table creation will fail
                    or performance will be significantly impacted. Typically
                    the default value of 1 is sufficient and will suffice for
                    manually run tests. For autopilot tests where are large
                    number of tests are to be run a value of 3 or 4 will
                    typically be sufficient, however of course the number of
                    inserts will depend on the performance of the system under
                    test and therefore testing is the best way to determine
                    the correct schema size for a particular
                    environment.</entry>
                  </row>

                  <row>
                    <entry>in-Memory Durability</entry>

                    <entry>Sets the durability option. If SCHEMA_ONLY is
                    chosen when SQL Server is stopped only the tables remain
                    without data loaded.</entry>
                  </row>

                  <row>
                    <entry>Number of Warehouses</entry>

                    <entry>The Number of Warehouses is selected by a listbox.
                    You should set this value to number of warehouses you have
                    chosen for your test.</entry>
                  </row>

                  <row>
                    <entry>Virtual Users to Build Schema</entry>

                    <entry>The Virtual Users to Build Schema is the number of
                    Virtual Users to be created on the Load Generation Server
                    that will complete your multi-threaded schema build. You
                    should set this value to either the number of warehouses
                    you are going to create (You cannot set the number of
                    virtual users higher than the number of warehouses value)
                    or the number of cores/Hyper-Threads on your Load
                    Generation Server. If you have a significantly larger
                    core/Hyper-Thread count on your Database Server then also
                    installing HammerDB locally on this server as well to run
                    the schema build can take advantage of the higher core
                    count to run the build more quickly.</entry>
                  </row>

                  <row>
                    <entry>Use BCP Option</entry>

                    <entry>This option uses the SQL Server BCP utility to bulk
                    load data. Temporary staging files will be created and
                    deleted in the location defined by the TEMP environment
                    variable. When unselected an insert based build is used. A
                    BCP build offers vastly improved performance over an
                    insert based build.</entry>
                  </row>
                </tbody>
              </tgroup>
            </table></para>
        </section>
      </section>

      <section>
        <title>Db2 Schema Build Options</title>

        <para>Note that as previously described the host and port are defined
        externally in the db2dsdriver.cfg file.</para>

        <figure>
          <title>Db2 Build Options</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch4-6.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para><table>
            <title>Db2 Build Options</title>

            <tgroup cols="2">
              <thead>
                <row>
                  <entry align="center">Option</entry>

                  <entry align="center">Description</entry>
                </row>
              </thead>

              <tbody>
                <row>
                  <entry>TPROC-C Db2 User</entry>

                  <entry>The name of the operating system user to connect to
                  the DB2 database for example db2inst1.</entry>
                </row>

                <row>
                  <entry>TPROC-C Db2 Password</entry>

                  <entry>The password for the operating system DB2 user by
                  default “ibmdb2”</entry>
                </row>

                <row>
                  <entry>TPROC-C Db2 Database</entry>

                  <entry>The name of the Db2 database that you have already
                  created, for example “tpcc”</entry>
                </row>

                <row>
                  <entry>TPROC-C Db2 Default Tablespace</entry>

                  <entry>The name of the existing tablespace where tables
                  should be located if a specific tablespace has not been
                  defined for that table in the tablespace list. The default
                  is “USERSPACE1”.</entry>
                </row>

                <row>
                  <entry>TPROC-C Db2 Tablespace List (Space Separated
                  Values)</entry>

                  <entry>When partitioning is selected, a space separated list
                  of Tablespace initials followed by a pre-existing tablespace
                  name in double-quotes into which to install a specific
                  table. If no tablespace is given for a specific table then
                  the default tablespace is used. The values are C: CUSTOMER
                  D: DISTRICT H: HISTORY I: ITEM W: WAREHOUSE S: STOCK NO:
                  NEW_ORDER OR: ORDERS OL: ORDER_LINE. And for example the
                  following list, would create all tables in the default. C ""
                  D "" H "" I "" W "" S "" NO "" OR "" OL "". Whereas the
                  following would create the ITEM table in the ITEM_TS
                  tablespace, the STOCK table in the STOCK_TS tablespace and
                  the other tables in the default. C "" D "" H "" I "ITEM_TS"
                  W "" S "STOCK_TS" NO "" OR "" OL "". You may configure all
                  or no distinct tablespaces according to your
                  requirements.</entry>
                </row>

                <row>
                  <entry>Number of Warehouses</entry>

                  <entry>The Number of Warehouses is selected by a listbox.
                  You should set this value to number of warehouses you have
                  chosen for your test.</entry>
                </row>

                <row>
                  <entry>Virtual Users to Build Schema</entry>

                  <entry>The Virtual Users to Build Schema is the number of
                  Virtual Users to be created on the Load Generation Server
                  that will complete your multi-threaded schema build. You
                  should set this value to either the number of warehouses you
                  are going to create (You cannot set the number of virtual
                  users higher than the number of warehouses value) or the
                  number of cores/Hyper-Threads on your Load Generation
                  Server. If you have a significantly larger core/Hyper-Thread
                  count on your Database Server then also installing HammerDB
                  locally on this server as well to run the schema build can
                  take advantage of the higher core count to run the build
                  more quickly.</entry>
                </row>

                <row>
                  <entry>Partition Tables</entry>

                  <entry>This check option becomes active when more than 10
                  warehouses are configured and transparently divides the
                  schema into 10 separate tables for the larger tables for
                  improved scalability and performance. This option is
                  recommended for larger configurations.</entry>
                </row>
              </tbody>
            </tgroup>
          </table></para>
      </section>

      <section>
        <title>MySQL Schema Build Options</title>

        <figure>
          <title>MySQL Build Options</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch4-7.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para><table>
            <title>MySQL Build Options</title>

            <tgroup cols="2">
              <thead>
                <row>
                  <entry align="center">Option</entry>

                  <entry align="center">Description</entry>
                </row>
              </thead>

              <tbody>
                <row>
                  <entry>MySQL Host</entry>

                  <entry>The MySQL Host Name is the host name that your load
                  generation server will use to connect to the database
                  running on the SUT database server.</entry>
                </row>

                <row>
                  <entry>MySQL Port</entry>

                  <entry>The MySQL Port is the network port that your load
                  generation server will use to connect to the database
                  running on the SUT database server. In most cases this will
                  be the default port of 3306.</entry>
                </row>

                <row>
                  <entry>MySQL Socket</entry>

                  <entry>The MySQL Socket option is enabled on Linux only. If
                  HammerDB is running on the same server and the MySQL Host is
                  127.0.0.1 or localhost then HammerDB will open a connection
                  on the socket given instead of using TCP/IP. To force the
                  use of the port instead of the socket on the localhost set
                  the socket value to "null".</entry>
                </row>

                <row>
                  <entry>Enable SSL</entry>

                  <entry>Enable SSL permits SSL/TLS encryption on the network
                  connection to MySQL with the option of choosing One-Way or
                  Two-Way encryption.</entry>
                </row>

                <row>
                  <entry>SSL CApath</entry>

                  <entry>If the SSL CA, SSL Cert and SSL Key entries are empty
                  HammerDB will pass the value in SSL CApath as the SSL capath
                  option. If the SSL CA, SSL Cert and SSL Key entries contain
                  the names of .pem files then the SSL CApath path option will
                  be appended to these file names and used to locate these
                  files.</entry>
                </row>

                <row>
                  <entry>SSL CA</entry>

                  <entry>The name of the SSL CA .pem file in the SSL CApath
                  directory.</entry>
                </row>

                <row>
                  <entry>SSL Cert</entry>

                  <entry>The name of the SSL Cert .pem file in the SSL CApath
                  directory.</entry>
                </row>

                <row>
                  <entry>SSL Key</entry>

                  <entry>The name of the SSL Key .pem file in the SSL CApath
                  directory.</entry>
                </row>

                <row>
                  <entry>SSL Cipher</entry>

                  <entry>The default of SSL Cipher is "server" meaning that
                  the server-side selected cipher is used. Alternatively this
                  option can be used to specify a different cipher. The Cipher
                  used will be reported by each Virtual User on connection. If
                  the value is shown as {} then SSL is not enabled.</entry>
                </row>

                <row>
                  <entry>MySQL User</entry>

                  <entry>The MySQL User is the user which has permission to
                  create a database and you previously granted access to from
                  the load generation server. The root user already exists in
                  all MySQL databases and has the necessary permissions to
                  create the TPROC-C database.</entry>
                </row>

                <row>
                  <entry>MySQL User Password</entry>

                  <entry>The MySQL user password is the password for the user
                  defined as the MySQL User. You will need to remember the
                  MySQL user name and password for running the TPROC-C driver
                  script after the database is built.</entry>
                </row>

                <row>
                  <entry>TPROC-C MySQL Database</entry>

                  <entry>The MySQL Database is the database that will be
                  created containing the TPROC-C schema creation. There must
                  have sufficient free space for the database to be
                  created.</entry>
                </row>

                <row>
                  <entry>Transactional Storage Engine</entry>

                  <entry>Use the "show engine" command to display available
                  storage engines and select a storage engine that supports
                  transactions. By default set to InnoDB.</entry>
                </row>

                <row>
                  <entry>Number of Warehouses</entry>

                  <entry>The Number of Warehouses is selected by a listbox.
                  You should set this value to number of warehouses you have
                  chosen for your test.</entry>
                </row>

                <row>
                  <entry>Virtual Users to Build Schema</entry>

                  <entry>The Virtual Users to Build Schema is the number of
                  Virtual Users to be created on the Load Generation Server
                  that will complete your multi-threaded schema build. You
                  should set this value to either the number of warehouses you
                  are going to create (You cannot set the number of virtual
                  users higher than the number of warehouses value) or the
                  number of cores/Hyper-Threads on your Load Generation
                  Server. If you have a significantly larger core/Hyper-Thread
                  count on your Database Server then also installing HammerDB
                  locally n this server as well to run the schema build can
                  take advantage of the higher core count to run the build
                  more quickly.</entry>
                </row>

                <row>
                  <entry>Partition Order Line Table</entry>

                  <entry>Partition Order Line Table for improved
                  scalability.</entry>
                </row>

                <row>
                  <entry>History Table Primary Key</entry>

                  <entry>In the TPC-C specification that the HammerDB TPROC-C
                  workload is derived from the history table does not have a
                  primary key. However for MySQL replication all tables must
                  have a primary key. This option creates the history table
                  with an invisible primary key at supported MySQL
                  versions.</entry>
                </row>
              </tbody>
            </tgroup>
          </table></para>
      </section>

      <section>
        <title>MariaDB Schema Build Options</title>

        <figure>
          <title>MariaDB Build Options</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch4-7a.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para><table>
            <title>MariaDB Build Options</title>

            <tgroup cols="2">
              <thead>
                <row>
                  <entry align="center">Option</entry>

                  <entry align="center">Description</entry>
                </row>
              </thead>

              <tbody>
                <row>
                  <entry>MariaDB Host</entry>

                  <entry>The MariaDB Host Name is the host name that your load
                  generation server will use to connect to the database
                  running on the SUT database server.</entry>
                </row>

                <row>
                  <entry>MariaDB Port</entry>

                  <entry>The MariaDB Port is the network port that your load
                  generation server will use to connect to the database
                  running on the SUT database server. In most cases this will
                  be the default port of 3306.</entry>
                </row>

                <row>
                  <entry>MariaDB Socket</entry>

                  <entry>The MariaDB Socket option is enabled on Linux only.
                  If HammerDB is running on the same server and the MariaDB
                  Host is 127.0.0.1 or localhost then HammerDB will open a
                  connection on the socket given instead of using TCP/IP. To
                  force the use of the port instead of the socket on the
                  localhost set the socket value to "null".</entry>
                </row>

                <row>
                  <entry>Enable SSL</entry>

                  <entry>Enable SSL permits SSL/TLS encryption on the network
                  connection to MariaDB with the option of choosing One-Way or
                  Two-Way encryption.</entry>
                </row>

                <row>
                  <entry>SSL CApath</entry>

                  <entry>If the SSL CA, SSL Cert and SSL Key entries are empty
                  HammerDB will pass the value in SSL CApath as the SSL capath
                  option. If the SSL CA, SSL Cert and SSL Key entries contain
                  the names of .pem files then the SSL CApath path option will
                  be appended to these file names and used to locate these
                  files. If the SSL CApath is an empty string then the SSL
                  Cert and SSL Key entries will be used only.</entry>
                </row>

                <row>
                  <entry>SSL CA</entry>

                  <entry>The name of the SSL CA .pem file in the SSL CApath
                  directory.</entry>
                </row>

                <row>
                  <entry>SSL Cert</entry>

                  <entry>The name of the SSL Cert .pem file in the SSL CApath
                  directory.</entry>
                </row>

                <row>
                  <entry>SSL Key</entry>

                  <entry>The name of the SSL Key .pem file in the SSL CApath
                  directory.</entry>
                </row>

                <row>
                  <entry>SSL Cipher</entry>

                  <entry>The default of SSL Cipher is "server" meaning that
                  the server-side selected cipher is used. Alternatively this
                  option can be used to specify a different cipher. The Cipher
                  used will be reported by each Virtual User on connection. If
                  the value is shown as {} then SSL is not enabled.</entry>
                </row>

                <row>
                  <entry>MariaDB User</entry>

                  <entry>The MariaDB User is the user which has permission to
                  create a database and you previously granted access to from
                  the load generation server. The root user already exists in
                  all MariaDB databases and has the necessary permissions to
                  create the TPROC-C database.</entry>
                </row>

                <row>
                  <entry>MariaDB User Password</entry>

                  <entry>The MariaDB user password is the password for the
                  user defined as the MariaDB User. You will need to remember
                  the MariaDB user name and password for running the TPROC-C
                  driver script after the database is built. If SSL
                  authentication is being used it is also possible to enter
                  "null" for the password to skip passing a password and use
                  SSL authentication only.</entry>
                </row>

                <row>
                  <entry>TRPOC-C MariaDB Database</entry>

                  <entry>The MariaDB Database is the database that will be
                  created containing the TPROC-C schema creation. There must
                  have sufficient free space for the database to be
                  created.</entry>
                </row>

                <row>
                  <entry>Transactional Storage Engine</entry>

                  <entry>Use the "show engine" command to display available
                  storage engines and select a storage engine that supports
                  transactions. By default set to InnoDB.</entry>
                </row>

                <row>
                  <entry>Number of Warehouses</entry>

                  <entry>The Number of Warehouses is selected by a listbox.
                  You should set this value to number of warehouses you have
                  chosen for your test.</entry>
                </row>

                <row>
                  <entry>Virtual Users to Build Schema</entry>

                  <entry>The Virtual Users to Build Schema is the number of
                  Virtual Users to be created on the Load Generation Server
                  that will complete your multi-threaded schema build. You
                  should set this value to either the number of warehouses you
                  are going to create (You cannot set the number of virtual
                  users higher than the number of warehouses value) or the
                  number of cores/Hyper-Threads on your Load Generation
                  Server. If you have a significantly larger core/Hyper-Thread
                  count on your Database Server then also installing HammerDB
                  locally n this server as well to run the schema build can
                  take advantage of the higher core count to run the build
                  more quickly.</entry>
                </row>

                <row>
                  <entry>Partition Order Line Table</entry>

                  <entry>Partition Order Line Table for improved
                  scalability.</entry>
                </row>

                <row>
                  <entry>History Table Primary Key</entry>

                  <entry>In the TPC-C specification that the HammerDB TPROC-C
                  workload is derived from the history table does not have a
                  primary key. However for MySQL replication all tables must
                  have a primary key. This option creates the history table
                  with an invisible primary key at supported MySQL
                  versions.</entry>
                </row>
              </tbody>
            </tgroup>
          </table></para>
      </section>

      <section>
        <title>PostgreSQL Schema Build Options</title>

        <figure>
          <title>PostgreSQL Build Options</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch4-8.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para/>

        <para><table>
            <title>PostgreSQL Build Options</title>

            <tgroup cols="2">
              <thead>
                <row>
                  <entry align="center">Option</entry>

                  <entry align="center">Description</entry>
                </row>
              </thead>

              <tbody>
                <row>
                  <entry>PostgreSQL Host</entry>

                  <entry>The host name of the SUT running PostgreSQL which the
                  load generation server running HammerDB will connect
                  to.</entry>
                </row>

                <row>
                  <entry>PostgreSQL Port</entry>

                  <entry>The port of the PostgreSQL service. By default this
                  will be 5432 for a standard PostgreSQL installation or 5444
                  for EnterpriseDB.</entry>
                </row>

                <row>
                  <entry>PostgreSQL Superuser</entry>

                  <entry>The PostgreSQL Superuser is a user with sufficient
                  privileges to create both new users (roles) and databases to
                  enable the creation of the test schema.</entry>
                </row>

                <row>
                  <entry>PostgreSQL Superuser Password</entry>

                  <entry>The PostgreSQL Superuser Password is the password for
                  the PostgreSQL superuser which will have been defined during
                  installation. If you have forgotten the password it can be
                  reset from a psql prompt that has logged in from a trusted
                  connection therefore requiring no password using postgres=#
                  alter role postgres password ‘postgres’;</entry>
                </row>

                <row>
                  <entry>PostgreSQL Default Database</entry>

                  <entry>The PostgreSQL default databases is the database to
                  specify for the superuser connection. Typically this will be
                  postgres for a standard PostgreSQL installation or edb for
                  EnterpriseDB.</entry>
                </row>

                <row>
                  <entry>TPROC-C PostgreSQL User</entry>

                  <entry>The PostgreSQL User is the user (role) that will be
                  created that owns the database containing the TPROC-C
                  schema.</entry>
                </row>

                <row>
                  <entry>TPROC-C PostgreSQL User Password</entry>

                  <entry>The PostgreSQL User Password is the password that
                  will be specified for the PostgreSQL user when it is
                  created.</entry>
                </row>

                <row>
                  <entry>TPROC-C PostgreSQL Database</entry>

                  <entry>The PostgreSQL Database is the database that if it
                  does not already exist will be created and owned by the
                  PostgreSQL User that contains the TPROC-C schema. If the
                  named database has already been created and is empty then
                  that database will be used to create the schema.</entry>
                </row>

                <row>
                  <entry>TPROC-C PostgreSQL Tablespace</entry>

                  <entry>The PostgreSQL Tablespace in which to create the
                  schema. By default the tablespace is pg_default.</entry>
                </row>

                <row>
                  <entry>EnterpriseDB Oracle Compatible</entry>

                  <entry>Choosing EnterpriseDB Oracle compatible creates a
                  schema using the Oracle compatible features of EnterpriseDB
                  in an installation of Postgres Plus Advanced Server. This
                  build uses Oracle PL/SQL for the creation of the stored
                  procedures.</entry>
                </row>

                <row>
                  <entry>PostgreSQL Stored Procedures</entry>

                  <entry>When running on PostgreSQL v11 or upwards use
                  PostgreSQL stored procedures instead of functions.</entry>
                </row>

                <row>
                  <entry>Citus Compatible</entry>

                  <entry>Installs a schema into a PostgreSQL database with the
                  Citus Data Distributed PostgreSQL extension.</entry>
                </row>

                <row>
                  <entry>Prefer PostgreSQL SSL Mode</entry>

                  <entry>If both the PostgreSQL client and server have been
                  compiled to support SSL this option when selected enables
                  "prefer" SSL, when unselected it sets this option to
                  "disable" for connections. Other valid options such as
                  "allow" or "require" can be set directly in the build and
                  driver scripts if required.</entry>
                </row>

                <row>
                  <entry>Number of Warehouses</entry>

                  <entry>The Number of Warehouses is selected by a listbox.
                  You should set this value to number of warehouses you have
                  chosen for your test.</entry>
                </row>

                <row>
                  <entry>Virtual Users to Build Schema</entry>

                  <entry>The Virtual Users to Build Schema is the number of
                  Virtual Users to be created on the Load Generation Server
                  that will complete your multi-threaded schema build. You
                  should set this value to either the number of warehouses you
                  are going to create (You cannot set the number of virtual
                  users higher than the number of warehouses value) or the
                  number of cores/Hyper-Threads on your Load Generation
                  Server. If you have a significantly larger core/Hyper-Thread
                  count on your Database Server then also installing HammerDB
                  locally on this server as well to run the schema build can
                  take advantage of the higher core count to run the build
                  more quickly.</entry>
                </row>
              </tbody>
            </tgroup>
          </table></para>
      </section>
    </section>

    <section>
      <title>Creating the Schema</title>

      <para>When you have completed your Build Options click OK to store the
      values you have entered. To begin the schema creation select Build from
      the tree-view.</para>

      <para><figure>
          <title>Build</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch4-10.PNG"/>
            </imageobject>
          </mediaobject>
        </figure></para>

      <para>On clicking this button a dialog box is shown</para>

      <para><figure>
          <title>Create Schema</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch4-11.PNG"/>
            </imageobject>
          </mediaobject>
        </figure></para>

      <para>When you click Yes HammerDB will login to your chosen database a
      monitor virtual user and depending on the database create the user with
      the password you have chosen. It will then log out and log in again as
      your chosen user, create the tables and then load the item table data
      before waiting and monitoring the other virtual users. The worker
      virtual users will wait for the monitor virtual user to complete its
      initial work. Subsequently the worker virtual users will create and
      insert the data for their assigned warehouses. There are no intermediate
      data files or manual builds required, HammerDB will both create and load
      your requested data dynamically. Data is inserted in a batch format for
      optimal network performance.</para>

      <figure>
        <title>Schema Build Start</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch4-12.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>When the worker virtual users are complete the monitor virtual
      users will depending on the database create the indexes, stored
      procedures and gather the statistics. When the schema build is complete
      Virtual User 1 will display the message SCHEMA COMPLETE and all virtual
      users will show an end timestamp and that they completed their action
      successfully. If this is not the case then then build did not complete
      successfully, the schema is not valid for testing and should therefore
      be deleted and reinstalled.</para>

      <figure>
        <title>Schema complete</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch4-13.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>

    <section>
      <title>Checking the Schema</title>

      <para>From v4.10 a new schema check option has been added. This schema
      check can be run after the build process to verify that the build
      completed successfully and also after running the workload to ensure
      that the data in your database remained consistent. This can also be
      useful for example when testing clustered, distributed or replicated
      database environments. </para>

      <para>To check the schema choose the Check option from the treeview or
      the top level menu. </para>

      <figure>
        <title>Check TPROC-C Schema</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="../DocBook/docs/images/ch4-13a.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>If the check completes without error it will report a message that
      the check has been successful meaning that the schema is valid and the
      data is consistent. </para>

      <figure>
        <title>TPROC-C Schema Check Complete</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="../DocBook/docs/images/ch4-13aa.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The following checks will be run against the schema and
      data:</para>

      <orderedlist>
        <listitem>
          <para>Database Exists.</para>
        </listitem>

        <listitem>
          <para>Tables Exist.</para>
        </listitem>

        <listitem>
          <para>Warehouse count in schema is the same as the HammerDB
          configuration.</para>
        </listitem>

        <listitem>
          <para>Tables are indexed.</para>
        </listitem>

        <listitem>
          <para>Tables are populated.</para>
        </listitem>

        <listitem>
          <para>Stored Procedures Exist.</para>
        </listitem>

        <listitem>
          <para> Consistency Checks.</para>

          <orderedlist>
            <listitem>
              <para>The sum of balances (d_ytd) for all Districts within a
              specific Warehouse is equal to the balance (w_ytd) of that
              Warehouse.</para>
            </listitem>

            <listitem>
              <para>For each District within a Warehouse, the next available
              Order ID (d_next_o_id) minus one is equal to the most recent
              Order ID [max(o_id)] for the ORDER table associated with the
              preceding District and Warehouse. Additionally, that same
              relationship exists for the most recent Order ID [max(o_id)] for
              the NEW-ORDER table associated with the same District and
              Warehouse. Those relationships can be illustrated as:
              d_next_o_id – 1 = max(o_id) = max(no_o_id) where (d_w_id =
              o_w_id = no_w_id) and (d_id = o_d_id = no_d_id)</para>
            </listitem>

            <listitem>
              <para>For each District within a Warehouse, the value of the
              most recent Order ID [max(no_o_id)] minus the first Order ID
              [min(no_o_id)] plus one, for the NEW-ORDER table associated with
              the District and Warehouse, equals the number of rows in that
              NEW-ORDER table. That relationship can be illustrated as:
              max(no_o_id) – min(no_o_id) + 1 = rows in NEW-ORDER where
              (o_w_id = no_w_id) and (o_d_id = no_d_id)</para>
            </listitem>

            <listitem>
              <para>For each District within a Warehouse, the sum of
              Order-Line counts [sum(o_ol_cnt)] for the Orders associated with
              the District equals the number of rows in the ORDER-LINE table
              associated with the same District. That relationship can be
              illustrated as: sum(o_ol_cnt) = rows in the ORDER-LINE table for
              the Warehouse and District</para>
            </listitem>
          </orderedlist>
        </listitem>
      </orderedlist>
    </section>

    <section>
      <title>Deleting the Schema</title>

      <para>To delete the schema for your chosen database, select the Delete
      schema option from the tree-view or menu.</para>

      <figure>
        <title>Delete Schema Option</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="../DocBook/docs/images/ch2-16a.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Select Yes at the Delete Schema dialog.</para>

      <figure>
        <title>Delete Schema Dialog</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="../DocBook/docs/images/ch2-16b.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>and confirm that the schema has been deleted.</para>

      <figure>
        <title>Schema Deleted</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="../DocBook/docs/images/ch2-16c.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Note that you may also delete the schema manually if preferred as
      detailed in the following section.</para>

      <section>
        <title>Deleting or Verifying the Oracle Schema</title>

        <para>If you have made a mistake simply close the application and run
        the following SQL to undo the user you have created.</para>

        <programlisting>SQL&gt;drop user tpcc cascade;</programlisting>

        <para>Note that if Hash Clusters were used it will first be necessary
        to re-enable the table locks as follows before deleting the
        schema.</para>

        <programlisting>ALTER TABLE WAREHOUSE DISABLE TABLE LOCK;
ALTER TABLE DISTRICT DISABLE TABLE LOCK;
ALTER TABLE CUSTOMER DISABLE TABLE LOCK;
ALTER TABLE ITEM DISABLE TABLE LOCK;
ALTER TABLE STOCK DISABLE TABLE LOCK;
ALTER TABLE ORDERS DISABLE TABLE LOCK;
ALTER TABLE NEW_ORDER DISABLE TABLE LOCK;
ALTER TABLE ORDER_LINE DISABLE TABLE LOCK;
ALTER TABLE HISTORY DISABLE TABLE LOCK;</programlisting>

        <para>When you have created your schema you can verify the contents
        with SQL*PLUS or your favourite admin tool as the newly created
        user.</para>

        <para><programlisting>SQL&gt; select tname, tabtype from tab;

TNAME                          TABTYPE
------------------------------ -------
HISTORY                          TABLE
CUSTOMER                         TABLE
DISTRICT                         TABLE
ITEM                             TABLE
WAREHOUSE                        TABLE
STOCK                            TABLE
NEW_ORDER                        TABLE
ORDERS                           TABLE
ORDER_LINE                       TABLE

9 rows selected.

SQL&gt; select * from warehouse;

      W_ID      W_YTD      W_TAX W_NAME     W_STREET_1
---------- ---------- ---------- ---------- --------------------
W_STREET_2           W_CITY               W_ W_ZIP
-------------------- -------------------- -- ---------
         1  773095764        .11 4R0mUe     rM8f7zFYdx
JyiNY5zg1gQNBDO      v2973cRoiFSJ0z       OF 374311111


SQL&gt; select index_name, index_type from ind;

INDEX_NAME                     INDEX_TYPE
------------------------------ ---------------------------
IORDL                          IOT - TOP
ORDERS_I1                      NORMAL
ORDERS_I2                      NORMAL
INORD                          IOT - TOP
STOCK_I1                       NORMAL
WAREHOUSE_I1                   NORMAL
ITEM_I1                        NORMAL
DISTRICT_I1                    NORMAL
CUSTOMER_I1                    NORMAL
CUSTOMER_I2                    NORMAL

10 rows selected.

SQL&gt;

SQL&gt; select object_name from user_procedures;

OBJECT_NAME
------------------------------
NEWORD
DELIVERY
PAYMENT
OSTAT
SLEV

SQL&gt; select sum(bytes)/1024/1024 as MB from user_segments;
        MB
----------
   838.125
</programlisting></para>
      </section>

      <section>
        <title>Deleting or Verifying the SQL Server Schema and In-memory
        Schema</title>

        <para>If you have made a mistake simply close the application and in
        SQL Server Management Studio right-click the database and choose
        Delete. Select the Close existing connections checkbox and click OK.
        When you have created your schema you can verify the contents with the
        SQL Server Management Studio or SQL Connection, for example:</para>

        <para><programlisting>C:\Users&gt;sqlcmd -S (local)\SQLDEVELOP -E -Q "use tpcc; select name from sys.tables"
Changed database context to 'tpcc'.
name
---------------------------------------------------------------------------
CUSTOMER
DISTRICT
HISTORY
ITEM
NEW_ORDER
ORDERS
ORDER_LINE
STOCK
WAREHOUSE
(9 rows affected)

C:\Users&gt;sqlcmd -S (local)\SQLDEVELOP -E -Q "use tpcc; select * from wareh
ouse where w_id = 1"
Changed database context to 'tpcc'.
w_id        w_ytd                 w_tax        w_name     w_street_1           w
_street_2           w_city               w_state w_zip     padding
          1          3000000.0000        .1000 s21C90Ft   pd1mYv9GlqyIww      u
6sOhAB9HF7iOZpM     llz9x35NhpVcrJc47Wy  VL      182111111 xxxxxxxxxxxxxxxxxxxxx(....)
(1 rows affected)
</programlisting>When an In-memory schema has been created under SSMS right
        click the created database and select reports followed memory usage by
        memory optimized objects, this produces a report such as follows for a
        10 warehouse configuration. As with an on-disk schema a rough estimate
        of 100MB per warehouse can be used for the space required. In
        particular note that SQL Server Express has a particularly small
        memory allocation of 252MB and can be used for tests on 1 warehouse
        only for a short period of time before this limit will be reached. The
        error reported in the log will be as follows:</para>

        <programlisting>Could not perform the operation because the database has reached its quota for in-memory tables.

</programlisting>

        <figure>
          <title>SQL Server in-Memory</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch4-22.png"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>Additionally after schema creation and during testing monitor
        bucket usage as follows:</para>

        <programlisting>use imoltp 
SELECT  
    QUOTENAME(SCHEMA_NAME(t.schema_id)) + N'.' + QUOTENAME(OBJECT_NAME(h.object_id)) as [table],   
    i.name                   as [index],   
    h.total_bucket_count,  
    h.empty_bucket_count,  
      
    FLOOR((  
      CAST(h.empty_bucket_count as float) /  
        h.total_bucket_count) * 100)  
                             as [empty_bucket_percent],  
    h.avg_chain_length,   
    h.max_chain_length  
  FROM  
         sys.dm_db_xtp_hash_index_stats  as h   
    JOIN sys.indexes                     as i  
            ON h.object_id = i.object_id  
           AND h.index_id  = i.index_id  
    JOIN sys.memory_optimized_tables_internal_attributes ia ON h.xtp_object_id=ia.xtp_object_id
    JOIN sys.tables t on h.object_id=t.object_id
  WHERE ia.type=1
  ORDER BY [table], [index];

</programlisting>

        <para>This script produces a report as follows where the
        empty_bucket_percent should indicate a good level of free space and
        the max_chain_length is not too long.</para>

        <figure>
          <title>In-memory report</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch4-23.png"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>

      <section>
        <title>Deleting or Verifying the Db2 Schema</title>

        <para>If you have made a mistake simply close the application and run
        the following SQL to undo the user you have created.</para>

        <programlisting>db2inst1 ~]$ db2 drop database tpcc
DB20000I  The DROP DATABASE command completed successfully.
</programlisting>

        <para>To browse the Db2 schema do the following.</para>

        <programlisting>[db2inst1 ~]$ db2 connect to tpcc

   Database Connection Information

 Database server        = DB2/LINUXX8664 10.5.5
 SQL authorization ID   = DB2INST1
 Local database alias   = TPCC

[db2inst1 ~]$ db2
(c) Copyright IBM Corporation 1993,2007
Command Line Processor for DB2 Client 10.5.5

db2 =&gt;  select * from warehouse fetch first 10 rows only

W_NAME     W_STREET_1           W_STREET_2           W_CITY               W_STATE W_ZIP     W_TAX                    W_YTD          W_ID       
---------- -------------------- -------------------- -------------------- ------- --------- ------------------------ -------------- -----------
5SuPObQR4  FCPEw6PzfOCdp5DHDq7e d9lOkysRKPyPtqB      G0Nt9PuUyR8qZxCOXms0 9Y      546011111            +1.70000E-001     3000000.00           1
QP75kKTagb sOaOeFYpGjc5lvA8BW   f6HbFCH2S6mh         cCPt1emu6hFjobgOqeP  TT      533211111            +1.50000E-001     3000000.00           2
Hu3QQhR    KwwcMmuWbpoiQRM      9MaTxygtYX4Dz        NFSkHHdHyEChXclP4iqA cE      919511111            +1.60000E-001     3000000.00           3
aqN3Df     PAJg6lOtk7r          XxWjB1HMQhOlJ        jknxafMFlirG8pUpntm  mG      217211111            +1.80000E-001     3000000.00           4
zZBreP     gCMDTWuJUHh          AG0vp9mbvGh          t7dDHFKFhd72WKP      xa      342611111            +1.30000E-001     3000000.00           5
bleOmY     pzPzlBidlwneHdMkq    dmZvxDxmrL4WdQNg     jC2DTpxGc1g1LQlk5P8n bt      980911111            +1.50000E-001     3000000.00           6
BFmMdkLUUK joucFFovxwZWcdsBPZ   IBjiEBzqn7dtuU       8FNwUX40bJ56Iwh      gC      751911111            +1.00000E-001     3000000.00           7
xWY9EugeeD t5dK0z1bQWwEuMGMnb59 sYEzAdgb9FeuX        K7PkSQHSno0NSHEet4xr 1Q      270611111            +1.70000E-001     3000000.00           8
5XtsHe1kw  uNJGs1Y1lQnYLAX      qvOfjMIqml5kHzm      C3iX14JTbnCyoRVR     ai      203011111            +2.00000E-001     3000000.00           9
t89Pm591   CKjgdxmZ5AgvZ        LqyRXzAoFUO          2O0j38eGPNMXFb       XU      372011111            +1.40000E-001     3000000.00          10

  10 record(s) selected.

db2 =&gt; 
</programlisting>
      </section>

      <section>
        <title>Deleting or Verifying the MySQL Schema</title>

        <para>If you have made a mistake simply close the application and run
        the following SQL to undo the database you have created.</para>

        <programlisting>mysql&gt;drop database tpcc;</programlisting>

        <para>you can verify the contents with SQL or your favourite admin
        tool as the newly created user.</para>

        <programlisting>mysql&gt; use tpcc;
Database changed
mysql&gt; show tables;
+----------------+
| Tables_in_tpcc |
+----------------+
| customer       |
| district       |
| history        |
| item           |
| new_order      |
| order_line     |
| orders         |
| stock          |
| warehouse      |
+----------------+
9 rows in set (0.00 sec)

mysql&gt; select * from warehouse limit 1 \G
*************************** 1. row ***************************
      w_id: 1
     w_ytd: 3000000.00
     w_tax: 0.1300
    w_name: mBr6dkgK
w_street_1: FH0SO5CUEREo
w_street_2: cBcStSxKcIIs4IAUUsJy
    w_city: FKaak9ZBgtJr3Tr6gESW
   w_state: Tt
     w_zip: 432611111
1 row in set (0.00 sec)

mysql&gt; show indexes from warehouse \G
*************************** 1. row ***************************
        Table: warehouse
   Non_unique: 0
     Key_name: PRIMARY
 Seq_in_index: 1
  Column_name: w_id
    Collation: A
  Cardinality: 10
     Sub_part: NULL
       Packed: NULL
         Null:
   Index_type: BTREE
      Comment:
Index_comment:
1 row in set (0.00 sec)

mysql&gt; select routine_name from information_schema.routines where routine_schema
 = 'TPCC';
+--------------+
| routine_name |
+--------------+
| DELIVERY     |
| NEWORD       |
| OSTAT        |
| PAYMENT      |
| SLEV         |
+--------------+
5 rows in set (0.03 sec)
</programlisting>
      </section>

      <section>
        <title>Deleting or Verifying the MariaDB Schema</title>

        <para>If you have made a mistake simply close the application and run
        the following SQL to undo the database you have created.</para>

        <programlisting>Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.
Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
MariaDB [(none)]&gt; drop database tpcc;
Query OK, 9 rows affected (0.137 sec)
MariaDB [(none)]&gt; </programlisting>

        <para>you can verify the contents with SQL or your favourite admin
        tool as the newly created user.</para>

        <programlisting>MariaDB [(none)]&gt; use tpcc;
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Database changed
MariaDB [tpcc]&gt; show tables;
+----------------+
| Tables_in_tpcc |
+----------------+
| customer       |
| district       |
| history        |
| item           |
| new_order      |
| order_line     |
| orders         |
| stock          |
| warehouse      |
+----------------+
9 rows in set (0.001 sec)

MariaDB [tpcc]&gt; select * from warehouse limit 1 \G
*************************** 1. row ***************************
      w_id: 1
     w_ytd: 4976593.00
     w_tax: 0.1300
    w_name: vBxysQ
w_street_1: bWwcsLWCJ9Ftiw
w_street_2: loLreYeoH3icmKt
    w_city: dUB0TsCnUUHD
   w_state: sQ
     w_zip: 632911111
1 row in set (0.002 sec)

MariaDB [tpcc]&gt; show indexes from warehouse \G
*************************** 1. row ***************************
        Table: warehouse
   Non_unique: 0
     Key_name: PRIMARY
 Seq_in_index: 1
  Column_name: w_id
    Collation: A
  Cardinality: 20
     Sub_part: NULL
       Packed: NULL
         Null: 
   Index_type: BTREE
      Comment: 
Index_comment: 
1 row in set (0.001 sec)

MariaDB [tpcc]&gt; select routine_name from information_schema.routines where routine_schema = 'TPCC';
+--------------+
| routine_name |
+--------------+
| DELIVERY     |
| NEWORD       |
| OSTAT        |
| PAYMENT      |
| SLEV         |
+--------------+
5 rows in set (0.008 sec)
</programlisting>
      </section>

      <section>
        <title>Deleting or Verifying the PostgreSQL Schema</title>

        <para>If you have made a mistake simply close the application and run
        the following SQL to undo the user you have created.</para>

        <programlisting>postgres=# drop database tpcc;
postgres=# drop role tpcc;
</programlisting>

        <para>You can browse the created schema, for example:</para>

        <para><programlisting>-bash-4.1$ ./bin/psql -d tpcc
Password: 
psql.bin (9.3.4.10)
Type "help" for help.

tpcc=# select relname, n_tup_ins - n_tup_del as rowcount from pg_stat_user_tables;
  relname   | rowcount 
------------+----------
 orders     |   300000
 district   |      100
 stock      |  1000000
 warehouse  |       10
 history    |   300000
 new_order  |    90000
 item       |   100000
 order_line |  3001170
 customer   |   300000
(9 rows)</programlisting></para>
      </section>
    </section>

    <section>
      <title>Configuring Driver Script options</title>

      <para>To configure the Driver Script select Options under the Driver
      Script tree-view.</para>

      <figure>
        <title>Driver Script Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch4-14.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>This displays the Driver Script Options dialog. The connection
      options are common to the Schema Build Dialog in addition to new Driver
      Options. For advanced options more details are provided in the
      subsequent section.</para>

      <figure>
        <title>Driver Script Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch4-15.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <table>
        <title>Driver Script Options</title>

        <tgroup cols="2">
          <thead>
            <row>
              <entry align="center">Option</entry>

              <entry align="center">Description</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>TPROC-C Driver Script</entry>

              <entry>For all databases you have the option of selecting a Test
              Driver Script or a Timed Driver Script. The This choice will
              dynamically change the Driver Script that is loaded when the
              TPROC-C Driver Script menu option is chosen. The Test Driver
              Script is intended for verifying and testing a configuration
              only by displaying virtual user output for a small number of
              virtual users. In particular both Windows and Linux graphical
              displays are single-threaded permitting only one Virtual User to
              write to the display at any one time. Therefore the performance
              of writing to the display will limit throughput. Consequently
              once a schema is verified to conduct measured tests you should
              select the Timed Driver Script Option.</entry>
            </row>

            <row>
              <entry>Total Transactions per User</entry>

              <entry>Total transactions per user is reported as
              total_iterations within the EDITABLE OPTIONS section of the
              driver script. This value will set the number of transactions
              each virtual user will process before logging off. You can use
              this value to determine how long the virtual user will remain
              active for. The length of time for activity will depend upon the
              performance of the Database Server under test. A higher
              performing server will process the defined number of
              transactions more quickly than a lower performing one. It is
              important to draw the distinction between the total_iterations
              value and the Iterations value set in the Virtual User Options
              window. The Iterations value in the Virtual User Options window
              determines the number of times that a script will be run in its
              entirety. The total_iterations value is internal to the TPROC-C
              driver script and determines the number of times the internal
              loop is iterated ie for {set it 0} {$it &lt; $total_iterations}
              {incr it} { ... } In other words if total_iterations is set to
              1000 then the executing user will log on once execute 1000
              transactions and then log off. If on the other hand Iterations
              in the Virtual User Options window is set to 1000 and
              total_iterations in the script set to 1 then the executing user
              will log on execute one transaction and then log off 1000
              times.You should ensure that the number of transactions is set
              to a suitably high vale to ensure that the virtual users do not
              complete their tests before the timed test is complete, doing so
              will mean the you will be timing idle virtual users and the
              results will be invalid.</entry>
            </row>

            <row>
              <entry>Exit on Error</entry>

              <entry>Exit on Error is shown as the parameter RAISEERROR in the
              Driver Script. RAISEERROR impacts the behaviour of an individual
              virtual user on detecting a database error. If set to TRUE on
              detecting an error the user will report the error into the
              HammerDB console and then terminate execution. If set to FALSE
              the virtual user will ignore the error and proceed with
              executing the next transaction. It is therefore important to be
              aware that if set to FALSE firstly if there has been a
              configuration error resulting in repeated errors then the
              workload might not be reported accurately and secondly you may
              not be aware of any occasional errors being reported as they are
              silently ignored.</entry>
            </row>

            <row>
              <entry>Keying and Thinking Time</entry>

              <entry>Keying and Thinking Time is shown as KEYANDTHINK in the
              Driver Script. This parameter will have the biggest impact on
              the type of workload that your test will take. Keying and
              thinking time is an integral part of a TPROC-C test in order to
              simulate the effect of the workload being run by a real user who
              takes time to key in an actual order and think about the output.
              If KEYANDTHINK is set to TRUE each user will simulate this real
              user type workload testing a workload scenario that will be
              closer to a real production environment. Whereas with
              KEYANDTHINK set to TRUE each user will execute maybe 2 or 3
              transactions a minute setting KEYANDTHINK to FALSE each user
              will now execute tens of thousands of transactions a minute. If
              this parameter is set to TRUE you will need at least hundreds or
              thousands of virtual users and warehouses, if FALSE then you
              will need to begin testing with 1 or 2 Virtual Users, building
              from here up to a maximum workload with the number of warehouses
              set to a level where the users are not contending for the same
              data. The default mode is to run with KEYANDTHINK set to FALSE
              and this is the method that will drive the highest transaction
              rates. To run with KEYANDTHINK set to TRUE the event driven
              scaling feature has been introduced to scale up the number of
              sessions connecting to the system. This feature is activated by
              selecting the Asynchronous Scaling option (which will also
              enable Keying and Thinking time). When enabled you are able to
              configure multiple sessions per Virtual User. Each Virtual User
              will then manage multiple clients processing the Keying and
              Thinking time asynchronously. With this feature you are able to
              configure significantly more sessions than with a single Virtual
              User configuration.</entry>
            </row>

            <row>
              <entry>No Stored Procedures</entry>

              <entry>For MariaDB and MySQL there is a no stored procedures
              option that uses client side SQL instead of stored procedures to
              drive the workload. Performance is expected to be higher when
              using stored procedures.</entry>
            </row>

            <row>
              <entry>Checkpoint/Vacuum/Purge when complete</entry>

              <entry>Where available the database will trigger a checkpoint,
              vacuum or purge after the workload is complete to write out the
              modified data from the in-memory cache to the disk. If the
              database is correctly configured this will prevent this activity
              being conducted during a test to result in higher
              performance.</entry>
            </row>

            <row>
              <entry>Minutes of Rampup Time</entry>

              <entry>The Minutes of Ramup Time is shown as rampup in the
              Driver Script. The rampup time defines the time in minutes for
              the monitoring virtual user to wait for the virtual users
              running the workload to connect to the database and build up the
              transaction rate by caching data in the database buffer cache
              before taking the first timed value and timing the test. The
              rampup time should be sufficiently long enough for a workload to
              reach a steady transaction rate before the first timed value is
              taken.</entry>
            </row>

            <row>
              <entry>Minutes for Test Duration</entry>

              <entry>The Minutes for Test Duration is shown as duration in the
              Driver Script. The test duration defines the time of the test
              measured as the time the monitor thread waits after the first
              timed value before taking the second one to signal the test is
              complete and the active virtual users to complete their
              workload.</entry>
            </row>

            <row>
              <entry>Use All Warehouses</entry>

              <entry>By default each Virtual User selects a home warehouse at
              random from at the start of a test and remains with that home
              warehouse. Therefore for example if there are 100 warehouses
              created and 10 virtual users selected to run the Driver Script
              then most of the activity will take place on 10 warehouse only.
              This option means that the Virtual Users select a new warehouse
              for each transaction from an available list divided between all
              Virtual Users at the start of the test therefore ensuring
              greater I/O activity.</entry>
            </row>

            <row>
              <entry>Time Profile</entry>

              <entry>When this option is selected client side time profiling
              will be conducted. There are two time profilers available, the
              xtprof profiler and the etprof profiler. The profiler chosen can
              be configured in the generic.xml file with xtprof profiling all
              virtual users and printing the timing output at the end of a run
              and the etprof profiler timing only the first virtual user but
              printing output at 10 second intervals.</entry>
            </row>

            <row>
              <entry>Asynchronous Scaling</entry>

              <entry>Enable the event driven scaling feature to configure
              multiple client sessions per Virtual User. When selected this
              will also enable the Keying and Thinking Time option. As the
              keying and thinking time is managed asynchronously this option
              is not valid to be run without keying and thinking time.
              Asynchronous Scaling is also a feature that is appropriate to
              test connection pooling by scaling up the number of client
              sessions that connect to the database.</entry>
            </row>

            <row>
              <entry>Asynch Client per Virtual User</entry>

              <entry>Configures the number of sessions that each Virtual User
              will connect to the database and manage. For example if there
              are 5 Virtual Users and 10 Asynchronous Clients there will be 50
              active connections to the database.</entry>
            </row>

            <row>
              <entry>Asynch Client Login Delay</entry>

              <entry>The delay that each Virtual User will allow before
              logging on each asynchronous client.</entry>
            </row>

            <row>
              <entry>Asynchronous Verbose</entry>

              <entry>Report asynchronous operations such as the time taken for
              keying and thinking time.</entry>
            </row>

            <row>
              <entry>XML Connect Pool</entry>

              <entry>XML Connect Pool is intended for simultaneously testing
              multiple instances of related clustered databases and when
              selected the virtual user database connections will open a pool
              of connections defined in the database specific XML file for
              example mssqlscpool.xml for SQL Server located in the directory
              connectpool in the config directory. Note that each virtual user
              (or asynchronous client) will open and hold all of the defined
              connections. The monitor virtual user and each virtual user will
              also continue to open the main standalone database connection.
              The monitor virtual user will continue to report NOPM and TPM
              and the virtual users to extract the warehouse count from this
              standalone connection and therefore the reliance is on the
              database to accurately report cluster wide transactions and for
              the instances to have the same warehouse count. For verification
              of the results from the master connection when using connect
              pooling HammerDB will also report client side transactions. To
              use the XML Connect Pool the XML configuration file should be
              modified according to the cluster database names with each
              connection defined by the tag c1, c2 c3 etc respectively. Under
              the sprocs section in the XML file is defined which stored
              procedures will use which connections and what policy is to be
              used. The policy can be first_named, last_named, random or
              round_robin. For example with connections c1, c2 and c3 for
              neworder and a policy of round_robin the first neworder
              transaction would execute against connection c1, the second c2,
              the third c3 and the fourth c1. first_named uses the first given
              connection, last_named the last and random chooses a connection
              at random. stocklevel and orderstatus are read only stored
              procedures that may be run against read only cluster nodes.
              There is no restriction on the number of connections that may be
              opened per virtual user. For further information on the
              connections opened there is a commented information line in the
              driver script such as #puts "sproc_cur:$st connections:[ set
              $cslist ] cursors:[set $cursor_list] number of cursors:[set
              $len] execs:[set $cnt]" prior to the opening of the standalone
              connection that may be uncommented for more detail when the
              script is run.</entry>
            </row>

            <row>
              <entry>Mode</entry>

              <entry>The mode value is taken from the operational mode setting
              set under the Mode Options menu tab under the Mode menu. If set
              to Local or Primary then the monitor thread takes snapshots, if
              set to Replica no snapshots are taken. This is useful if
              multiple instances of HammerDB are running in Primary and
              Replica mode against a clustered database configuration to
              ensure that only one instance takes the snapshots.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>

    <section>
      <title>Advanced Driver Script Options</title>

      <para>This section includes advanced driver script options intended for
      expert usage. These options can be used independently or simultaneously
      for advanced testing scenarios.</para>

      <section>
        <title>Use All Warehouses for increased I/O</title>

        <para>By default each Virtual User will select one home warehouse at
        random and keep that home warehouse for the duration of a run meaning
        the majority of its workload will take place on a single warehouse.
        This means that when running for example 10 Virtual Users most of the
        workload will take place on 10 warehouses regardless of whether 100,
        1000 or 10,000 are configured in the schema. Use All Warehouses is an
        option that enables increased I/O to the database data area by
        assigning all of the warehouses in the schema to the Virtual Users in
        turn. The Virtual Users will then select a new warehouse for each
        transaction. Consequently this means that the schema size impacts on
        the overall level of performance placing a great emphasis on I/O. To
        select this option check the Use All Warehouses check-box.</para>

        <figure>
          <title>Use All Warehouses Option</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch4-15l.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>On running the workload it can now be seen that the Virtual
        Users will evenly assign the available warehouses between them.</para>

        <figure>
          <title>Use All Warehouses</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch4-15k.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>The listing shows an example of a schema with 30 warehouses and
        3 Virtual Users. This approach is particularly applicable when testing
        I/O capabilities for database.</para>

        <programlisting>Vuser 1:Beginning rampup time of 2 minutes
Vuser 2:VU 2 : Assigning WID=1 based on VU count 3, Warehouses = 30 (1 out of 10)
Vuser 2:VU 2 : Assigning WID=4 based on VU count 3, Warehouses = 30 (2 out of 10)
Vuser 2:VU 2 : Assigning WID=7 based on VU count 3, Warehouses = 30 (3 out of 10)
Vuser 2:VU 2 : Assigning WID=10 based on VU count 3, Warehouses = 30 (4 out of 10)
Vuser 2:VU 2 : Assigning WID=13 based on VU count 3, Warehouses = 30 (5 out of 10)
Vuser 2:VU 2 : Assigning WID=16 based on VU count 3, Warehouses = 30 (6 out of 10)
Vuser 2:VU 2 : Assigning WID=19 based on VU count 3, Warehouses = 30 (7 out of 10)
Vuser 2:VU 2 : Assigning WID=22 based on VU count 3, Warehouses = 30 (8 out of 10)
Vuser 2:VU 2 : Assigning WID=25 based on VU count 3, Warehouses = 30 (9 out of 10)
Vuser 2:VU 2 : Assigning WID=28 based on VU count 3, Warehouses = 30 (10 out of 10)
Vuser 2:Processing 1000000 transactions with output suppressed...
Vuser 3:VU 3 : Assigning WID=2 based on VU count 3, Warehouses = 30 (1 out of 10)
Vuser 3:VU 3 : Assigning WID=5 based on VU count 3, Warehouses = 30 (2 out of 10)
Vuser 3:VU 3 : Assigning WID=8 based on VU count 3, Warehouses = 30 (3 out of 10)
Vuser 3:VU 3 : Assigning WID=11 based on VU count 3, Warehouses = 30 (4 out of 10)
Vuser 3:VU 3 : Assigning WID=14 based on VU count 3, Warehouses = 30 (5 out of 10)
Vuser 3:VU 3 : Assigning WID=17 based on VU count 3, Warehouses = 30 (6 out of 10)
Vuser 3:VU 3 : Assigning WID=20 based on VU count 3, Warehouses = 30 (7 out of 10)
Vuser 3:VU 3 : Assigning WID=23 based on VU count 3, Warehouses = 30 (8 out of 10)
Vuser 3:VU 3 : Assigning WID=26 based on VU count 3, Warehouses = 30 (9 out of 10)
Vuser 3:VU 3 : Assigning WID=29 based on VU count 3, Warehouses = 30 (10 out of 10)
Vuser 3:Processing 1000000 transactions with output suppressed...
Vuser 4:VU 4 : Assigning WID=3 based on VU count 3, Warehouses = 30 (1 out of 10)
Vuser 4:VU 4 : Assigning WID=6 based on VU count 3, Warehouses = 30 (2 out of 10)
Vuser 4:VU 4 : Assigning WID=9 based on VU count 3, Warehouses = 30 (3 out of 10)
Vuser 4:VU 4 : Assigning WID=12 based on VU count 3, Warehouses = 30 (4 out of 10)
Vuser 4:VU 4 : Assigning WID=15 based on VU count 3, Warehouses = 30 (5 out of 10)
Vuser 4:VU 4 : Assigning WID=18 based on VU count 3, Warehouses = 30 (6 out of 10)
Vuser 4:VU 4 : Assigning WID=21 based on VU count 3, Warehouses = 30 (7 out of 10)
Vuser 4:VU 4 : Assigning WID=24 based on VU count 3, Warehouses = 30 (8 out of 10)
Vuser 4:VU 4 : Assigning WID=27 based on VU count 3, Warehouses = 30 (9 out of 10)
Vuser 4:VU 4 : Assigning WID=30 based on VU count 3, Warehouses = 30 (10 out of 10)</programlisting>
      </section>

      <section>
        <title>Time Profile for measuring Response Times</title>

        <para>In addition to performance profiles based on throughput you
        should also take note of transaction response times. Whereas
        performance profiles show the cumulative performance of all of the
        virtual users running on the system, response times show performance
        based on the experience of the individual user. When comparing systems
        both throughput and response time are important comparative
        measurements.</para>

        <para>Time Profile functionality is enabled by selecting the Time
        Profile checkbox in the driver options for the GUI or setting the
        database prefix specific timeprofile option to true in the CLI.</para>

        <para/>

        <figure>
          <title>Time Profile</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch4-15o.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>There are different time profiling packages named xtprof and
        etprof with xtprof introduced at version 4.1 being the default and
        recommended time profiling package to use. The time profiling package
        is selected by modifying the timeprofile section in the generic.xml
        file with the options being xtprof and etprof respectively. The
        additional option xt_unique_log_name when set to 1 enables a unique
        log name to be used for the output of the time profiling data.</para>

        <programlisting>&lt;timeprofile&gt;
           &lt;profiler&gt;xtprof&lt;/profiler&gt;
           &lt;xt_unique_log_name&gt;0&lt;/xt_unique_log_name&gt;
           &lt;xt_gather_timeout&gt;60&lt;/xt_gather_timeout&gt;
&lt;/timeprofile&gt;</programlisting>

        <para>When xtprof is selected HammerDB will collect time profiling
        data for all active virtual users. At the end of the run after the
        performance has been reported all of the data will be gathered,
        calculated and appended to a logfile called hdbxtprofile.log or if a
        unique log name is used for example a log name such as
        hdbxtprofile_607407EE586C03E293732393.log. Measured overhead for
        profiling all of the virtual users is expected to be up to 4% of the
        throughput. Calculations take place after the test has run and will
        therefore not impact the performance measurements.</para>

        <para>The xt_gather_timeout option sets a user configurable value in
        minutes for the time for HammerDB to wait for Virtual Users to report
        timings when event driven scaling is used with many 1000's of
        sessions. In this scenario xt_gather_timeout defines how long to wait
        for the Virtual User timing report until proceeding to the
        calculation. If no Virtual Users have reported an error is printed, if
        a subset of the Virtual Users have reported a timing report is printed
        for those Virtual Users that did report.</para>

        <programlisting>Vuser 1:Test complete, Taking end Transaction Count.
Vuser 1:5 VU * 10 AC = 50 Active Sessions configured
Vuser 1:TEST RESULT : System achieved 53 NOPM from 399 PostgreSQL TPM
Vuser 1:Gathering timing data from Active Virtual Users...
Vuser 4:All asynchronous clients complete
Vuser 5:All asynchronous clients complete
Vuser 3:All asynchronous clients complete
Vuser 2:All asynchronous clients complete
Vuser 1:WARNING:Timing Data Incomplete
Vuser 1:4 out of 5 Virtual Users reported
Vuser 1:Writing timing data to /tmp/hdbxtprofile.log</programlisting>

        <para>Within the logfile HammerDB will report for all Active Virtual
        Users meaning the first Virtual User will be 2 as Virtual User 1 is
        the monitor. The report will also provide a summary of all Active
        Virtual Users. Note that for the summary the total time will be for
        all virtual users and therefore will be equivalent to the running time
        multiplied by the virtual user count.</para>

        <table>
          <title>Xt Time Profile</title>

          <tgroup cols="2">
            <thead>
              <row>
                <entry align="center">Value</entry>

                <entry align="center">Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>CALLS</entry>

                <entry>Number of times that the stored procedure was
                called.</entry>
              </row>

              <row>
                <entry>MIN</entry>

                <entry>Minimum response time in milliseconds.</entry>
              </row>

              <row>
                <entry>AVG</entry>

                <entry>Average response time in milliseconds.</entry>
              </row>

              <row>
                <entry>MAX</entry>

                <entry>Maximum response time in milliseconds.</entry>
              </row>

              <row>
                <entry>TOTAL</entry>

                <entry>Total time spent in that stored procedure during the
                measuring interval. The total time will include both rampup
                and timed test times.</entry>
              </row>

              <row>
                <entry>P99</entry>

                <entry>99th percentile in milliseconds.</entry>
              </row>

              <row>
                <entry>P95</entry>

                <entry>95th percentile in milliseconds.</entry>
              </row>

              <row>
                <entry>P50</entry>

                <entry>50th percentile in milliseconds.</entry>
              </row>

              <row>
                <entry>SD</entry>

                <entry>Standard Deviation showing variance in captured
                values.</entry>
              </row>

              <row>
                <entry>RATIO</entry>

                <entry>Ratio showing percentage time taken for that stored
                procedure. The total of these values may be less than 100% as
                only timings for the stored procedures are shown.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>

        <para>an example xt time profile report is shown.</para>

        <programlisting>MSSQLServer Hammerdb Time Profile Report @ Fri Apr 02 10:23:56 BST 2021
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
&gt;&gt;&gt;&gt;&gt; VIRTUAL USER 2 : ELAPSED TIME : 421062ms
&gt;&gt;&gt;&gt;&gt; PROC: NEWORD
CALLS: 169244 MIN: 0.430ms AVG: 0.994ms MAX: 1042.998ms TOTAL: 168189.754ms
P99: 2.465ms P95: 1.383ms P50: 0.857ms SD: 41300.051 RATIO: 39.944%
&gt;&gt;&gt;&gt;&gt; PROC: PAYMENT
CALLS: 169863 MIN: 0.342ms AVG: 0.854ms MAX: 1726.371ms TOTAL: 145028.480ms
P99: 2.353ms P95: 1.390ms P50: 0.685ms SD: 64474.647 RATIO: 34.443%
&gt;&gt;&gt;&gt;&gt; PROC: DELIVERY
CALLS: 17027 MIN: 1.298ms AVG: 2.709ms MAX: 163.655ms TOTAL: 46129.940ms
P99: 5.360ms P95: 3.647ms P50: 2.541ms SD: 25810.720 RATIO: 10.956%
&gt;&gt;&gt;&gt;&gt; PROC: SLEV
CALLS: 16822 MIN: 0.617ms AVG: 1.903ms MAX: 764.873ms TOTAL: 32006.646ms
P99: 2.308ms P95: 1.821ms P50: 1.343ms SD: 156601.733 RATIO: 7.601%
&gt;&gt;&gt;&gt;&gt; PROC: OSTAT
CALLS: 17188 MIN: 0.213ms AVG: 1.035ms MAX: 468.477ms TOTAL: 17790.442ms
P99: 2.047ms P95: 1.483ms P50: 0.774ms SD: 77601.204 RATIO: 4.225%
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
&gt;&gt;&gt;&gt;&gt; VIRTUAL USER 3 : ELAPSED TIME : 420313ms
&gt;&gt;&gt;&gt;&gt; PROC: NEWORD
CALLS: 159980 MIN: 0.444ms AVG: 1.022ms MAX: 1643.138ms TOTAL: 163441.841ms
P99: 2.567ms P95: 1.492ms P50: 0.862ms SD: 58276.974 RATIO: 38.886%
&gt;&gt;&gt;&gt;&gt; PROC: PAYMENT
CALLS: 160285 MIN: 0.346ms AVG: 0.881ms MAX: 2023.197ms TOTAL: 141213.427ms
P99: 2.503ms P95: 1.473ms P50: 0.705ms SD: 65938.596 RATIO: 33.597%
&gt;&gt;&gt;&gt;&gt; PROC: DELIVERY
CALLS: 16069 MIN: 1.272ms AVG: 3.417ms MAX: 206.193ms TOTAL: 54904.266ms
P99: 7.418ms P95: 5.324ms P50: 3.192ms SD: 32127.714 RATIO: 13.063%
&gt;&gt;&gt;&gt;&gt; PROC: SLEV
CALLS: 16050 MIN: 0.678ms AVG: 2.006ms MAX: 626.750ms TOTAL: 32200.179ms
P99: 2.643ms P95: 1.910ms P50: 1.343ms SD: 168030.717 RATIO: 7.661%
&gt;&gt;&gt;&gt;&gt; PROC: OSTAT
CALLS: 16000 MIN: 0.222ms AVG: 1.072ms MAX: 578.273ms TOTAL: 17157.497ms
P99: 2.063ms P95: 1.489ms P50: 0.782ms SD: 90527.786 RATIO: 4.082%
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
&gt;&gt;&gt;&gt;&gt; VIRTUAL USER 4 : ELAPSED TIME : 419788ms
&gt;&gt;&gt;&gt;&gt; PROC: NEWORD
CALLS: 171907 MIN: 0.439ms AVG: 0.970ms MAX: 1013.525ms TOTAL: 166696.636ms
P99: 2.488ms P95: 1.342ms P50: 0.833ms SD: 40049.741 RATIO: 39.710%
&gt;&gt;&gt;&gt;&gt; PROC: PAYMENT
CALLS: 171395 MIN: 0.348ms AVG: 0.825ms MAX: 2027.276ms TOTAL: 141448.137ms
P99: 2.309ms P95: 1.353ms P50: 0.669ms SD: 59576.289 RATIO: 33.695%
&gt;&gt;&gt;&gt;&gt; PROC: DELIVERY
CALLS: 17242 MIN: 1.292ms AVG: 2.783ms MAX: 1662.594ms TOTAL: 47976.469ms
P99: 5.314ms P95: 3.614ms P50: 2.487ms SD: 145408.189 RATIO: 11.429%
&gt;&gt;&gt;&gt;&gt; PROC: SLEV
CALLS: 17149 MIN: 0.637ms AVG: 1.911ms MAX: 1053.436ms TOTAL: 32775.092ms
P99: 2.231ms P95: 1.805ms P50: 1.319ms SD: 169897.509 RATIO: 7.808%
&gt;&gt;&gt;&gt;&gt; PROC: OSTAT
CALLS: 17379 MIN: 0.221ms AVG: 1.076ms MAX: 385.704ms TOTAL: 18699.482ms
P99: 2.059ms P95: 1.463ms P50: 0.754ms SD: 82806.139 RATIO: 4.455%
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
&gt;&gt;&gt;&gt;&gt; VIRTUAL USER 5 : ELAPSED TIME : 419663ms
&gt;&gt;&gt;&gt;&gt; PROC: NEWORD
CALLS: 171092 MIN: 0.424ms AVG: 0.967ms MAX: 1229.934ms TOTAL: 165440.937ms
P99: 2.408ms P95: 1.356ms P50: 0.839ms SD: 43716.072 RATIO: 39.422%
&gt;&gt;&gt;&gt;&gt; PROC: PAYMENT
CALLS: 171379 MIN: 0.331ms AVG: 0.829ms MAX: 1731.137ms TOTAL: 142134.192ms
P99: 2.240ms P95: 1.372ms P50: 0.676ms SD: 66909.786 RATIO: 33.869%
&gt;&gt;&gt;&gt;&gt; PROC: DELIVERY
CALLS: 17376 MIN: 1.333ms AVG: 2.711ms MAX: 522.322ms TOTAL: 47110.423ms
P99: 5.066ms P95: 3.665ms P50: 2.502ms SD: 55115.193 RATIO: 11.226%
&gt;&gt;&gt;&gt;&gt; PROC: SLEV
CALLS: 17000 MIN: 0.677ms AVG: 2.060ms MAX: 638.143ms TOTAL: 35019.365ms
P99: 2.227ms P95: 1.820ms P50: 1.324ms SD: 181337.117 RATIO: 8.345%
&gt;&gt;&gt;&gt;&gt; PROC: OSTAT
CALLS: 17393 MIN: 0.230ms AVG: 1.028ms MAX: 416.741ms TOTAL: 17880.809ms
P99: 2.069ms P95: 1.485ms P50: 0.765ms SD: 78776.046 RATIO: 4.261%
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
&gt;&gt;&gt;&gt;&gt; VIRTUAL USER 6 : ELAPSED TIME : 418748ms
&gt;&gt;&gt;&gt;&gt; PROC: NEWORD
CALLS: 156981 MIN: 0.440ms AVG: 1.046ms MAX: 1612.589ms TOTAL: 164162.970ms
P99: 2.758ms P95: 1.525ms P50: 0.882ms SD: 63714.779 RATIO: 39.203%
&gt;&gt;&gt;&gt;&gt; PROC: PAYMENT
CALLS: 157020 MIN: 0.351ms AVG: 0.895ms MAX: 2026.866ms TOTAL: 140488.701ms
P99: 2.654ms  P95: 1.525ms P50: 0.719ms SD: 61228.592 RATIO: 33.550%
&gt;&gt;&gt;&gt;&gt; PROC: DELIVERY
CALLS: 15770 MIN: 1.383ms AVG: 3.498ms MAX: 788.532ms TOTAL: 55159.762ms
P99: 7.326ms P95: 5.431ms P50: 3.250ms SD: 66690.551 RATIO: 13.173%
&gt;&gt;&gt;&gt;&gt; PROC: SLEV
CALLS: 15797 MIN: 0.670ms AVG: 1.947ms MAX: 583.215ms TOTAL: 30754.041ms
P99: 2.723ms P95: 1.898ms P50: 1.380ms SD: 152847.699 RATIO: 7.344%
&gt;&gt;&gt;&gt;&gt; PROC: OSTAT
CALLS: 15976 MIN: 0.238ms AVG: 1.024ms MAX: 343.665ms TOTAL: 16364.749ms
P99: 2.089ms P95: 1.527ms P50: 0.790ms SD: 63035.213 RATIO: 3.908%
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
&gt;&gt;&gt;&gt;&gt; SUMMARY OF 5 ACTIVE VIRTUAL USERS : MEDIAN ELAPSED TIME : 419788ms
&gt;&gt;&gt;&gt;&gt; PROC: NEWORD
CALLS: 829204 MIN: 0.424ms AVG: 0.998ms MAX: 1643.139ms TOTAL: 827932.303ms
P99: 2.542ms P95: 1.419ms P50: 0.854ms SD: 49988.460 RATIO: 39.445%
&gt;&gt;&gt;&gt;&gt; PROC: PAYMENT
CALLS: 829942 MIN: 0.331ms AVG: 0.856ms MAX: 2027.278ms TOTAL: 710313.076ms
P99: 2.412ms P95: 1.424ms P50: 0.690ms SD: 63696.833 RATIO: 33.841%
&gt;&gt;&gt;&gt;&gt; PROC: DELIVERY
CALLS: 83484 MIN: 1.272ms AVG: 3.010ms MAX: 1662.596ms TOTAL: 251280.909ms
P99: 6.723ms P95: 4.607ms P50: 2.694ms SD: 78650.494 RATIO: 11.972%
&gt;&gt;&gt;&gt;&gt; PROC: SLEV
CALLS: 82818 MIN: 0.617ms AVG: 1.965ms MAX: 1053.437ms TOTAL: 162755.356ms
P99: 2.419ms P95: 1.851ms P50: 1.341ms SD: 166238.802 RATIO: 7.754%
&gt;&gt;&gt;&gt;&gt; PROC: OSTAT
CALLS: 83936 MIN: 0.213ms AVG: 1.047ms MAX: 578.273ms TOTAL: 87892.997ms
P99: 2.067ms P95: 1.488ms P50: 0.772ms SD: 79101.309 RATIO: 4.187%
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

</programlisting>

        <para>When Jobs are enabled by default the time profile data will be
        stored alongside the Jobid and can be viewed with the web service.
        Charts are automatically generated to show the response times.</para>

        <figure>
          <title>graphical response times</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="../DocBook/docs/images/ch10ws-1i.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>An alternative time profiling package time called etprof can be
        selected by modifying the timeprofile section in generic.xml to change
        xtprof to etprof. A key difference between etprof and xtprof is that
        whereas xtprof enables you to capture time profiling for all virtual
        users etprof only enables you to select the first Active Virtual User
        and measure the response times. When enabled the time profile will
        show response time percentile values at 10 second intervals, reporting
        the minimum, 50th percentile, 95th percentile, 99th percentile and
        maximum for each of the procedures during that 10 second interval as
        well as cumulative values for all of the test at the end of the test
        run. For etprof the time profile values are recorded in microseconds.
        etprof should be viewed as way to drill down into an individual
        virtual user after analyzing xtprof data and is not recorded in the
        jobs related data for viewing with the web service.</para>

        <programlisting>Hammerdb Log @ Fri Jul 05 09:55:26 BST 2019
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-
Vuser 1:Beginning rampup time of 1 minutes
Vuser 2:Processing 1000000 transactions with output suppressed...
Vuser 3:Processing 1000000 transactions with output suppressed...
Vuser 4:Processing 1000000 transactions with output suppressed...
Vuser 5:Processing 1000000 transactions with output suppressed...
Vuser 2:|PERCENTILES 2019-07-05 09:55:46 to 2019-07-05 09:55:56
Vuser 2:|neword|MIN-391|P50%-685|P95%-1286|P99%-3298|MAX-246555|SAMPLES-3603
Vuser 2:|payment|MIN-314|P50%-574|P95%-1211|P99%-2253|MAX-89367|SAMPLES-3564
Vuser 2:|delivery|MIN-1128|P50%-1784|P95%-2784|P99%-6960|MAX-267012|SAMPLES-356
Vuser 2:|slev|MIN-723|P50%-884|P95%-1363|P99%-3766|MAX-120687|SAMPLES-343
Vuser 2:|ostat|MIN-233|P50%-568|P95%-1325|P99%-2387|MAX-82538|SAMPLES-365
Vuser 2:|gettimestamp|MIN-2|P50%-4|P95%-7|P99%-14|MAX-39|SAMPLES-7521
Vuser 2:|prep_statement|MIN-188|P50%-209|P95%-1067|P99%-1067|MAX-1067|SAMPLES-6
Vuser 2:+-----------------+--------------+------+--------+--------------+--------------+
...
Vuser 2:+-----------------+--------------+------+--------+--------------+--------------+
Vuser 2:|PERCENTILES 2019-07-05 09:59:26 to 2019-07-05 09:59:36
Vuser 2:|neword|MIN-410|P50%-678|P95%-1314|P99%-4370|MAX-32030|SAMPLES-4084
Vuser 2:|payment|MIN-331|P50%-583|P95%-1271|P99%-3152|MAX-43996|SAMPLES-4142
Vuser 2:|delivery|MIN-1177|P50%-2132|P95%-3346|P99%-4040|MAX-8492|SAMPLES-416
Vuser 2:|slev|MIN-684|P50%-880|P95%-1375|P99%-1950|MAX-230733|SAMPLES-364
Vuser 2:|ostat|MIN-266|P50%-688.5|P95%-1292|P99%-1827|MAX-9790|SAMPLES-427
Vuser 2:|gettimestamp|MIN-3|P50%-4|P95%-7|P99%-14|MAX-22|SAMPLES-8639
Vuser 2:+-----------------+--------------+------+--------+--------------+--------------+
Vuser 2:|PERCENTILES 2019-07-05 09:59:36 to 2019-07-05 09:59:46
Vuser 2:|neword|MIN-404|P50%-702|P95%-1296|P99%-4318|MAX-71663|SAMPLES-3804
Vuser 2:|payment|MIN-331|P50%-597|P95%-1250|P99%-4190|MAX-47539|SAMPLES-3879
Vuser 2:|delivery|MIN-1306|P50%-2131|P95%-4013|P99%-8742|MAX-25095|SAMPLES-398
Vuser 2:|slev|MIN-713|P50%-913|P95%-1438|P99%-2043|MAX-7434|SAMPLES-386
Vuser 2:|ostat|MIN-268|P50%-703|P95%-1414|P99%-3381|MAX-249963|SAMPLES-416
Vuser 2:|gettimestamp|MIN-3|P50%-4|P95%-8|P99%-16|MAX-27|SAMPLES-8079
Vuser 2:+-----------------+--------------+------+--------+--------------+--------------+
Vuser 1:3 ...,
Vuser 1:Test complete, Taking end Transaction Count.
Vuser 1:4 Active Virtual Users configured
Vuser 1:TEST RESULT : System achieved 468610 SQL Server TPM at 101789 NOPM
Vuser 2:+-----------------+--------------+------+--------+--------------+--------------+
Vuser 2:|PROCNAME | EXCLUSIVETOT| %| CALLNUM| AVGPERCALL| CUMULTOT|
Vuser 2:+-----------------+--------------+------+--------+--------------+--------------+
Vuser 2:|neword | 82051665|39.96%| 93933| 873| 88760245|
Vuser 2:|payment | 73823956|35.95%| 93922| 786| 80531339|
Vuser 2:|delivery | 22725292|11.07%| 9577| 2372| 23418195|
Vuser 2:|slev | 14396765| 7.01%| 9340| 1541| 14402033|
Vuser 2:|ostat | 10202116| 4.97%| 9412| 1083| 10207260|
Vuser 2:|gettimestamp | 2149552| 1.05%| 197432| 10| 13436919|
Vuser 2:|TOPLEVEL | 2431| 0.00%| 1| 2431| NOT AVAILABLE|
Vuser 2:|prep_statement | 1935| 0.00%| 5| 387| 1936|
Vuser 2:+-----------------+--------------+------+--------+--------------+--------------+</programlisting>

        <para>After capturing the response time the script below can be run at
        the command line and provided with a logfile with the data for one run
        only. Note that it is important that you only provide a logfile for
        one run of a HammerDB benchmark to convert, otherwise all of the data
        will be combined from multiple runs. When run on a logfile with data
        such as shown above this will output the data in tab delimited format
        that can be interpreted by a spreadsheet.</para>

        <programlisting>!/bin/tclsh
 set filename [lindex $argv 0]
 set fp [open "$filename" r]
 set file_data [ read $fp ]
 set data [split $file_data "\n"]
 foreach line $data {
 if {[ string match *PERCENTILES* $line ]} {
 set timeval "[ lindex [ split $line ] 3 ]"
 append xaxis "$timeval\t"
         }
     }
 puts "TIME INTERVALS"
 puts "\t$xaxis"
 foreach storedproc {neword payment delivery slev ostat} {
 puts [ string toupper $storedproc ]
 foreach line $data {
 if {[ string match *PROCNAME* $line ]} { break }
 if {[ string match *$storedproc* $line ]} {
 regexp {MIN-[0-9.]+} $line min
 regsub {MIN-} $min "" min
 append minlist "$min\t"
 regexp {P50%-[0-9.]+} $line p50
 regsub {P50%-} $p50 "" p50
 append p50list "$p50\t"
 regexp {P95%-[0-9.]+} $line p95
 regsub {P95%-} $p95 "" p95
 append p95list "$p95\t"
 regexp {P99%-[0-9.]+} $line p99
 regsub {P99%-} $p99 "" p99
 append p99list "$p99\t"
 regexp {MAX-[0-9.]+} $line max
 regsub {MAX-} $max "" max
 append maxlist "$max\t"
     }
       }
 puts -nonewline "MIN\t"
 puts $minlist
 unset -nocomplain minlist
 puts -nonewline "P50\t"
 puts $p50list 
 unset -nocomplain p50list
 puts -nonewline "P95\t"
 puts $p95list 
 unset -nocomplain p95list
 puts -nonewline "P99\t"
 puts $p99list
 unset -nocomplain p99list
 puts -nonewline "MAX\t"
 puts $maxlist
 unset -nocomplain maxlist
     }
 close $fp</programlisting>

        <para>Pass the name of the logfile for the run where response times
        were captured and output them to a file with a spreadsheet extension
        name. Note that it is important to output the data to a file and not
        to a terminal with that data then cut and paste into a spreadsheet. If
        output to a terminal it may format the output by removing the tab
        characters which are essential to the formatting.</para>

        <programlisting>$ ./extracttp.tcl pgtp.log &gt; pgtp.txt</programlisting>

        <para>With Excel 2013 and above you can give this file a .xls
        extension and open it. If you do it will give the following warning,
        however if you click OK it will open with the correctly formatted
        data.</para>

        <figure>
          <title>Excel Warning</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch4-15a.png"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>Alternatively if we open the file with the .txt extension it
        will show 3 steps for the Text Import Wizard. Click through the Wizard
        until Finish, After clicking Finish the data has been imported into
        the spreadsheet without warnings. Highlight the rows you want to graph
        by clicking on the row numbers.</para>

        <para><figure>
            <title>Highlighted Rows</title>

            <mediaobject>
              <imageobject>
                <imagedata fileref="docs/images/ch4-15b.png"/>
              </imageobject>
            </mediaobject>
          </figure></para>

        <para>Click on Insert and Recommended Charts, the default graph
        produced by Excel is shown below with the addition of a vertical axis
        title and a chart header. When saving the spreadsheet it is saved in
        Excel format rather than the imported Tab (Text Delimited).</para>

        <figure>
          <title>Response Time Graph</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch4-15c.png"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>

      <section>
        <title>Event Driven Scaling for Keying and Thinking Times</title>

        <para>Event driven scaling enables the scaling of virtual users to
        thousands of sessions running with keying and thinking time enabled.
        This feature adds additional benefit to your testing scenarios with
        the ability to handle large numbers of connections or testing with
        connection pooling. When running transactional workloads with HammerDB
        the default mode is CPU intensive meaning that one virtual user will
        run as many transactions as possible without keying and thinking time
        enabled. When keying and thinking time is enabled there is a large
        time delay both before and after running a transaction meaning that
        each Virtual User will spend most of its time idle. However creating a
        very large number of Virtual Users requires a significant use of load
        test generation server resources. Consequently event driven scaling is
        a feature that enables each Virtual User to create multiple database
        sessions and manage the keying and thinking time for each
        asynchronously in an event-driven loop enabling HammerDB to create a
        much larger session count within an existing Virtual User footprint.
        It should be clear that this feature is only designed to work with
        keying and thinking time enabled as it is only the keying and thinking
        time that is managed asynchronously.</para>

        <para>To configure this feature select Asynchronous Scaling noting
        that Keying and Thinking Time is automatically selected. Select a
        number of Asynch Clients per Virtual User and set the Asynch Login
        Delay in milliseconds. This Login Delay means that each client will
        wait for this time after the previous client has logged in before then
        logging in itself. For detailed output select Asynchronous Verbose.
        Note that with this feature it is important to allow the clients
        enough time to both login fully before measuring performance and also
        at the end it will take additional time for the clients to all
        complete their current keying and thinking time and to exit before the
        Virtual User reports all clients as complete.</para>

        <figure>
          <title>Asynchronous Options</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch4-15e.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>When all Virtual Users have logged in (example from SQL Server)
        the session count will show as the number of Virtual Users multiplied
        by the Asynchronous Clients.</para>

        <programlisting>SELECT DB_NAME(dbid) as DBName, COUNT(dbid) as NumberOfConnections FROM sys.sysprocesses WHERE dbid &gt; 0 GROUP BY dbid;</programlisting>

        <figure>
          <title>Session Count</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch4-15g.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>As each Asynchronous Client logs in it will be reported in the
        Virtual User output.</para>

        <figure>
          <title>Logging In Asynchronous Clients</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch4-15f.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>When the workload is running with Asynchronous Verbose enabled
        HammerDB will report the events as they happen.</para>

        <figure>
          <title>Asynchronous Workload Running</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch4-15h.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>With logging enabled and Asynchronous Verbose HammerDB will
        report the events as they happen for each Virtual User such as when
        they enter keying or thinking time and when they process a
        transaction.</para>

        <programlisting>Vuser 6:keytime:payment:vuser6:ac9:3 secs
Vuser 7:keytime:payment:vuser7:ac92:3 secs
Vuser 7:thinktime:delivery:vuser7:ac77:3 secs
Vuser 3:keytime:payment:vuser3:ac30:3 secs
Vuser 9:keytime:delivery:vuser9:ac49:2 secs
Vuser 7:vuser7:ac77:w_id:21:payment
Vuser 9:keytime:neword:vuser9:ac64:18 secs
Vuser 3:thinktime:neword:vuser3:ac72:15 secs
Vuser 3:vuser3:ac72:w_id:4:payment
Vuser 3:keytime:neword:vuser3:ac52:18 secs
Vuser 7:thinktime:neword:vuser7:ac43:8 secs
Vuser 7:vuser7:ac43:w_id:6:payment
Vuser 7:keytime:ostat:vuser7:ac9:2 secs
Vuser 3:keytime:payment:vuser3:ac9:3 secs
Vuser 3:thinktime:payment:vuser3:ac97:7 secs
Vuser 11:keytime:payment:vuser11:ac42:3 secs
Vuser 5:keytime:neword:vuser5:ac42:18 secs
Vuser 9:thinktime:ostat:vuser9:ac71:3 secs
Vuser 3:vuser3:ac97:w_id:24:payment
Vuser 9:vuser9:ac71:w_id:9:delivery
Vuser 9:keytime:delivery:vuser9:ac69:2 secs
Vuser 5:keytime:delivery:vuser5:ac19:2 secs
Vuser 11:thinktime:neword:vuser11:ac53:13 secs
Vuser 11:vuser11:ac53:w_id:8:neword
Vuser 9:keytime:delivery:vuser9:ac2:2 secs
Vuser 7:thinktime:neword:vuser7:ac81:12 secs
Vuser 3:keytime:neword:vuser3:ac47:18 secs
Vuser 7:vuser7:ac81:w_id:5:payment
Vuser 3:keytime:payment:vuser3:ac81:3 secs
Vuser 7:keytime:slev:vuser7:ac46:2 secs
Vuser 11:thinktime:payment:vuser11:ac65:2 secs
Vuser 11:vuser11:ac65:w_id:21:slev
Vuser 9:keytime:neword:vuser9:ac86:18 secs
Vuser 11:thinktime:payment:vuser11:ac20:1 secs
Vuser 7:thinktime:neword:vuser7:ac76:9 secs
Vuser 11:vuser11:ac20:w_id:6:payment
Vuser 7:vuser7:ac76:w_id:1:payment
Vuser 11:keytime:delivery:vuser11:ac79:2 secs
Vuser 9:thinktime:neword:vuser9:ac57:15 secs
Vuser 11:thinktime:payment:vuser11:ac30:14 secs
Vuser 9:vuser9:ac57:w_id:3:ostat
Vuser 11:vuser11:ac30:w_id:5:neword
Vuser 9:keytime:payment:vuser9:ac3:3 secs
Vuser 11:keytime:payment:vuser11:ac62:3 secs
Vuser 3:keytime:payment:vuser3:ac35:3 secs
Vuser 7:keytime:neword:vuser7:ac88:18 secs
Vuser 11:keytime:payment:vuser11:ac96:3 secs
Vuser 11:thinktime:payment:vuser11:ac47:8 secs
Vuser 11:vuser11:ac47:w_id:4:neword
Vuser 3:thinktime:payment:vuser3:ac24:21 secs
Vuser 5:keytime:neword:vuser5:ac37:18 secs
Vuser 7:keytime:payment:vuser7:ac16:3 secs
Vuser 11:keytime:payment:vuser11:ac88:3 secs
Vuser 3:vuser3:ac24:w_id:16:neword
Vuser 11:thinktime:slev:vuser11:ac25:6 secs
Vuser 11:vuser11:ac25:w_id:3:payment
Vuser 5:thinktime:payment:vuser5:ac40:2 secs
Vuser 5:vuser5:ac40:w_id:26:neword
Vuser 5:thinktime:neword:vuser5:ac63:7 secs
Vuser 5:vuser5:ac63:w_id:10:payment</programlisting>

        <para>One particular advantage of this type of workload is to be able
        to run a fixed throughput test defined by the number of Virtual
        Users.</para>

        <figure>
          <title>Steady State</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch4-15i.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>On completion of the workloads the Monitor Virtual User will
        report the number of Active Sessions and the performance achieved. The
        active Virtual Users will report when all of the asynchronous clients
        have completed their workloads and logged off.</para>

        <figure>
          <title>Asynchronous Workload Complete</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch4-15j.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>The event driven scaling feature is not intended to replace the
        default CPU intensive mode of testing and it is expected that this
        will continue to be the most popular methodology. Instead being able
        to increase up client sessions with keying and thinking time adds
        additional test scenarios for highly scalable systems and in
        particular is an effective test methodology for testing middle tier or
        proxy systems.</para>
      </section>

      <section>
        <title>XML Connect Pool for Cluster Testing</title>

        <para>The XML Connect Pool is intended for simultaneously testing
        related multiple instances of a clustered database. It enables each
        Virtual User to open a pool of connections (Note that each virtual
        user (or asynchronous client) will open and hold all of the defined
        connections) and direct the individual transactions to run on a
        specific instance according to a pre-defined policy. With this
        approach it is possible for example to direct the read-write
        transactions to a primary instance on a cluster whilst directing the
        read-only transactions to the secondary.</para>

        <para><figure>
            <title>Connect Pooling</title>

            <mediaobject>
              <imageobject>
                <imagedata fileref="docs/images/ch4-15d.png"/>
              </imageobject>
            </mediaobject>
          </figure>Note that for testing or evaluation of this feature it is
        also possible to direct one HammerDB client to test multiple separate
        instances at the same time provided that the instances have exactly
        the same warehouse count as shown in the example below. However for a
        valid and comparable test consistency should be ensured between the
        database instances. Therefore for example directing transactions
        against any instance in an Oracle RAC configuration would be valid as
        would running the read only transactions against a secondary read only
        instance in a cluster. However running against separate unrelated
        instances is possible for testing but not comparable for performance
        results. The monitor virtual user will continue to connect to the
        instance defined in the driver options and report NOPM and TPM from
        this standalone connection only and therefore the reliance is on the
        database to accurately report a cluster wide transactions and for the
        instances to have the same warehouse count. Nevertheless when using
        the XML connect pooling a client side transaction count will also be
        reported to provide detailed transaction data from all Virtual
        Users.</para>

        <para>The configuration is defined in the database specific XML file
        in the config/connpool directory. It is recommended to make a backup
        of the file before it is modified. The XML configuration file is in 2
        sections, connections and sprocs. For connections the XML
        configuration file should be modified according to the cluster
        database names with each connection defined by the tags c1, c2, c3
        respectively. There is no restriction to the number of connections
        that you define. Under the sprocs section in the XML configuration
        file is defined which stored procedures will use which connections and
        what policy is to be used. The policy can be first_named, last_named,
        random or round_robin. For example with connections c1, c2 and c3 for
        neworder and a policy of round_robin the first neworder transaction
        would execute against connection c1, the second c2, the third c3, the
        fourth c1 and so on. For all databases and all stored procedures
        prepared statements are used meaning that a statement is prepared for
        each connection for each virtual user and a reference kept for that
        prepared statement for execution.</para>

        <para>For further information on the connections opened there is a
        commented information line in the driver script such as #puts
        "sproc_cur:$st connections:[ set $cslist ] cursors:[set $cursor_list]
        number of cursors:[set $len] execs:[set $cnt]" prior to the opening of
        the standalone connection that may be uncommented for more detail when
        the script is run.</para>

        <programlisting>&lt;connpool&gt;
&lt;connections&gt;
    &lt;c1&gt;
        &lt;mssqls_server&gt;(local)\SQLDEVELOP&lt;/mssqls_server&gt;
        &lt;mssqls_linux_server&gt;host1&lt;/mssqls_linux_server&gt;
        &lt;mssqls_tcp&gt;false&lt;/mssqls_tcp&gt;
        &lt;mssqls_port&gt;1433&lt;/mssqls_port&gt;
        &lt;mssqls_azure&gt;false&lt;/mssqls_azure&gt;
        &lt;mssqls_authentication&gt;windows&lt;/mssqls_authentication&gt;
        &lt;mssqls_linux_authent&gt;sql&lt;/mssqls_linux_authent&gt;
&lt;mssqls_odbc_driver&gt;ODBC Driver 17 for SQL Server&lt;/mssqls_odbc_driver&gt;
&lt;mssqls_linux_odbc&gt;ODBC Driver 17 for SQL Server&lt;/mssqls_linux_odbc&gt;
        &lt;mssqls_uid&gt;sa&lt;/mssqls_uid&gt;
        &lt;mssqls_pass&gt;admin&lt;/mssqls_pass&gt;
&lt;mssqls_dbase&gt;tpcc1&lt;/mssqls_dbase&gt;
    &lt;/c1&gt;
    &lt;c2&gt;
        &lt;mssqls_server&gt;(local)\SQLDEVELOP&lt;/mssqls_server&gt;
        &lt;mssqls_linux_server&gt;host2&lt;/mssqls_linux_server&gt;
        &lt;mssqls_tcp&gt;false&lt;/mssqls_tcp&gt;
        &lt;mssqls_port&gt;1433&lt;/mssqls_port&gt;
        &lt;mssqls_azure&gt;false&lt;/mssqls_azure&gt;
        &lt;mssqls_authentication&gt;windows&lt;/mssqls_authentication&gt;
        &lt;mssqls_linux_authent&gt;sql&lt;/mssqls_linux_authent&gt;
&lt;mssqls_odbc_driver&gt;ODBC Driver 17 for SQL Server&lt;/mssqls_odbc_driver&gt;
&lt;mssqls_linux_odbc&gt;ODBC Driver 17 for SQL Server&lt;/mssqls_linux_odbc&gt;
        &lt;mssqls_uid&gt;sa&lt;/mssqls_uid&gt;
        &lt;mssqls_pass&gt;admin&lt;/mssqls_pass&gt;
&lt;mssqls_dbase&gt;tpcc2&lt;/mssqls_dbase&gt;
    &lt;/c2&gt;
    &lt;c3&gt;
        &lt;mssqls_server&gt;(local)\SQLDEVELOP&lt;/mssqls_server&gt;
        &lt;mssqls_linux_server&gt;host3&lt;/mssqls_linux_server&gt;
        &lt;mssqls_tcp&gt;false&lt;/mssqls_tcp&gt;
        &lt;mssqls_port&gt;1433&lt;/mssqls_port&gt;
        &lt;mssqls_azure&gt;false&lt;/mssqls_azure&gt;
        &lt;mssqls_authentication&gt;windows&lt;/mssqls_authentication&gt;
        &lt;mssqls_linux_authent&gt;sql&lt;/mssqls_linux_authent&gt;
&lt;mssqls_odbc_driver&gt;ODBC Driver 17 for SQL Server&lt;/mssqls_odbc_driver&gt;
&lt;mssqls_linux_odbc&gt;ODBC Driver 17 for SQL Server&lt;/mssqls_linux_odbc&gt;
        &lt;mssqls_uid&gt;sa&lt;/mssqls_uid&gt;
        &lt;mssqls_pass&gt;admin&lt;/mssqls_pass&gt;
&lt;mssqls_dbase&gt;tpcc3&lt;/mssqls_dbase&gt;
    &lt;/c3&gt;
&lt;/connections&gt;
&lt;sprocs&gt;
  &lt;neworder&gt;
&lt;connections&gt;c1 c2 c3&lt;/connections&gt;
    &lt;policy&gt;round_robin&lt;/policy&gt;
&lt;/neworder&gt;
    &lt;payment&gt;
&lt;connections&gt;c1 c2&lt;/connections&gt;
    &lt;policy&gt;first_named&lt;/policy&gt;
&lt;/payment&gt;
    &lt;delivery&gt;
&lt;connections&gt;c2 c3&lt;/connections&gt;
    &lt;policy&gt;last_named&lt;/policy&gt;
&lt;/delivery&gt;
    &lt;stocklevel&gt;
&lt;connections&gt;c1 c2 c3&lt;/connections&gt;
    &lt;policy&gt;random&lt;/policy&gt;
&lt;/stocklevel&gt;
    &lt;orderstatus&gt;
&lt;connections&gt;c2 c3&lt;/connections&gt;
    &lt;policy&gt;round_robin&lt;/policy&gt;
&lt;/orderstatus&gt;
&lt;/sprocs&gt;
&lt;/connpool&gt;
</programlisting>

        <para>After modifying the XML configuration file select XML Connect
        Pool in the Driver Options to activate this feature.</para>

        <figure>
          <title>XML Connect Pool</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch4-15m.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>For this example the additional information for the comments is
        also added to illustrate the connections made.</para>

        <figure>
          <title>Connections Comment</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch4-15n.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>When the Virtual Users are run the logfile shows that
        connections are made for the active Virtual Users according to the
        connections and policies defined in the XML configuration file. Also
        prepared statements are created and held in a pool for execution
        against the defined policy. Also note that the standalone connection
        "tpcc1" is also made to monitor the transaction rates and define the
        warehouse count for the run.</para>

        <programlisting>Vuser 2:sproc_cur:neword_st connections:{odbcc1 odbcc2 odbcc3} cursors:::oo::Obj23::Stmt::3 ::oo::Obj28::Stmt::3 ::oo::Obj33::Stmt::3 number of cursors:3 execs:0
Vuser 2:sproc_cur:payment_st connections:{odbcc1 odbcc2} cursors:::oo::Obj23::Stmt::4 ::oo::Obj28::Stmt::4 number of cursors:2 execs:0
Vuser 2:sproc_cur:ostat_st connections:{odbcc2 odbcc3} cursors:::oo::Obj28::Stmt::5 ::oo::Obj33::Stmt::4 number of cursors:2 execs:0
Vuser 2:sproc_cur:delivery_st connections:{odbcc1 odbcc2 odbcc3} cursors:::oo::Obj23::Stmt::5 ::oo::Obj28::Stmt::6 ::oo::Obj33::Stmt::5 number of cursors:3 execs:0
Vuser 2:sproc_cur:slev_st connections:{odbcc2 odbcc3} cursors:::oo::Obj28::Stmt::7 ::oo::Obj33::Stmt::6 number of cursors:2 execs:0
Vuser 3:sproc_cur:neword_st connections:{odbcc1 odbcc2 odbcc3} cursors:::oo::Obj23::Stmt::3 ::oo::Obj28::Stmt::3 ::oo::Obj33::Stmt::3 number of cursors:3 execs:0
Vuser 3:sproc_cur:payment_st connections:{odbcc1 odbcc2} cursors:::oo::Obj23::Stmt::4 ::oo::Obj28::Stmt::4 number of cursors:2 execs:0
Vuser 3:sproc_cur:ostat_st connections:{odbcc2 odbcc3} cursors:::oo::Obj28::Stmt::5 ::oo::Obj33::Stmt::4 number of cursors:2 execs:0
Vuser 3:sproc_cur:delivery_st connections:{odbcc1 odbcc2 odbcc3} cursors:::oo::Obj23::Stmt::5 ::oo::Obj28::Stmt::6 ::oo::Obj33::Stmt::5 number of cursors:3 execs:0
Vuser 3:sproc_cur:slev_st connections:{odbcc2 odbcc3} cursors:::oo::Obj28::Stmt::7 ::oo::Obj33::Stmt::6 number of cursors:2 execs:0</programlisting>

        <para>On completion of the run the NOPM and TPM is recorded. This is
        the area where it is of particular importance to be aware of the
        database and cluster configuration for the results to be consistent.
        It is therefore valid to reiterate that if the cluster and standalone
        connection does not record all of the transactions in the cluster then
        the NOPM results will only be returned for the standalone connection.
        By way of example in the test configuration shown there are 3 separate
        databases and the standalone connection is made to tpcc1. Therefore
        the test results shows the NOPM value at approximately 1/3rd of the
        ratio expected against the TPM value that records all of the
        transactions against the SQL Server. For this reason the CLIENT SIDE
        TPM is also shown. In this example the neworder value per minute is
        78319 a close equivalent to 3 x 26207 and therefore gives an
        indication of the NOPM value for multiple instances in a non-cluster
        configuration. In this case 3 connections were made to tpcc1, tpcc2
        and tpcc3 and the connections chosen to round robin between them,
        therefore the actual number of NOPM is 3X that recorded from just the
        standalone connection. In a correctly configured cluster environment
        it would be the same and the results wouyld be both consistent and
        valid. Be aware that these client side values are recorded during both
        rampup and timed test periods and therefore may not accurately reflect
        the results from a valid timed test.</para>

        <programlisting>Vuser 1:2 Active Virtual Users configured
Vuser 1:TEST RESULT : System achieved 26207 NOPM from 180515 SQL Server TPM
Vuser 1:CLIENT SIDE TPM : neworder 78319 payment 78185 delivery 7855 stocklevel 7826 orderstatus 7809
</programlisting>

        <para>In addition to the CLIENT SIDE TPM each Virtual User will also
        report the total number of transactions that it processed from the
        time that it started running to the end of the test.</para>

        <programlisting>Vuser 2:VU2 processed neworder 275335 payment 273822 delivery 27495 stocklevel 27588 orderstatus 27568 transactions
Vuser 3:VU3 processed neworder 272901 payment 273475 delivery 27493 stocklevel 27194 orderstatus 27097 transactions
</programlisting>

        <para>The XML Connect Pool feature provides advanced features for the
        expert user to test clusters and multiple instances simultaneously, it
        also gives the user a high degree of control on how this is used,
        therefore it is at the users discretion to use these settings
        appropriately to ensure consistent results.</para>
      </section>

      <section>
        <title>Step Testing with Variable Load</title>

        <para>Step testing is methodology of varying the load on the database
        over a period time. In HammerDB this variable load us achieved by
        automating the Remote Primary and Replica modes functionality. As the
        primary will automatically create and connect a number of replicas
        this functionality is available only when using the CLI and is started
        using the steprun command. Designing the steps is best approached by
        considering each horizontal block a primary or replica level with the
        primary at the base and the replicas at subsequent steps. Note that
        with a variable load the number of NOPM or TPM recorded over time is
        less relevant than for a non-variable timed test as the number of
        transactions will vary throughout the test. Instead this approach is
        best for capturing changes in response times and transaction rates
        throughout the test for further analysis such as rapidly finding peak
        performance in one test by increasing the load and observing the
        transaction rates.</para>

        <figure>
          <title>Step Test</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch4-16a.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>The base of the steps is the primary configuration and this
        continues to be configured as normal with time defined by rampup and
        duration and the number of virtual users with the vuset vu command.
        The replicas are configured in the steps.xml file in the config
        directory. For each replica (there is no limit on the number of
        replicas defined) the configuration takes the time after the previous
        replica started, the duration to run (with no rampup as the primary
        has already done the rampup) and the number of virtual users, all
        other configuration is inherited from the primary.</para>

        <programlisting>&lt;steps&gt;
&lt;replica1&gt;
  &lt;start_after_prev&gt;1&lt;/start_after_prev&gt;
  &lt;duration&gt;4&lt;/duration&gt;
  &lt;virtual_users&gt;2&lt;/virtual_users&gt;
 &lt;/replica1&gt;
 &lt;replica2&gt;
  &lt;start_after_prev&gt;1&lt;/start_after_prev&gt;
  &lt;duration&gt;3&lt;/duration&gt;
  &lt;virtual_users&gt;2&lt;/virtual_users&gt;
 &lt;/replica2&gt;
 &lt;replica3&gt;
  &lt;start_after_prev&gt;1&lt;/start_after_prev&gt;
  &lt;duration&gt;2&lt;/duration&gt;
  &lt;virtual_users&gt;2&lt;/virtual_users&gt;
 &lt;/replica3&gt;
 &lt;replica4&gt;
  &lt;start_after_prev&gt;1&lt;/start_after_prev&gt;
  &lt;duration&gt;1&lt;/duration&gt;
  &lt;virtual_users&gt;2&lt;/virtual_users&gt;
 &lt;/replica4&gt;
&lt;/steps&gt;</programlisting>

        <para>When the workload is started it will provide a summary of the
        configuration.</para>

        <programlisting>primary starts immediately, runs rampup for 2 minutes then runs test for 5 minutes with 2 Active VU
replica1 starts 1 minutes after rampup completes and runs test for 4 minutes with 2 Active VU
replica2 starts 1 minutes after previous replica starts and runs test for 3 minutes with 2 Active VU
replica3 starts 1 minutes after previous replica starts and runs test for 2 minutes with 2 Active VU
replica4 starts 1 minutes after previous replica starts and runs test for 1 minutes with 2 Active VU</programlisting>

        <para>The configuration must resemble a pyramid configuration with the
        primary providing the base and running for the longest period of time.
        If the replicas overrun this base then the step workload will error
        and exit without running. The default configuration is an example
        where there is a 2 minute rampup, 5 minute test on the primary, with 3
        replicas starting 1 minute after the rampup or previous replicas and
        then running for 4,3,2 and 1 minute respectively meaning primary and
        replicas complete at the same time. To start a step workload configure
        a driver script as normal and instead of vurun call the steprun
        command.</para>

        <programlisting>dbset db mssqls
dbset bm TPC-C
diset connection mssqls_server {(local)\SQLDEVELOP}
diset tpcc mssqls_dbase tpcc
diset tpcc mssqls_total_iterations 10000000
diset tpcc mssqls_driver timed
diset tpcc mssqls_rampup 2
diset tpcc mssqls_duration 5
#diset tpcc mssqls_timeprofile true
vuset logtotemp 1
vuset vu 2
steprun</programlisting>

        <para>No further intervention is needed and the primary will
        automatically create, configure and run replica instances of HammerDB
        at the specified time intervals, with an example from the default
        shown.</para>

        <programlisting>HammerDB-4.7&gt;hammerdbcli auto steprun.tcl
HammerDB CLI v4.7
Copyright (C) 2003-2023 Steve Shaw
Type "help" for a list of commands
Initialized SQLite on-disk database C:/Users/Steve/AppData/Local/Temp/hammer.DB using existing tables (36,864 KB)
Database set to MSSQLServer
Benchmark set to TPC-C for MSSQLServer
Value (local)\SQLDEVELOP for connection:mssqls_server is the same as existing value (local)\SQLDEVELOP, no change made
Value tpcc for tpcc:mssqls_dbase is the same as existing value tpcc, no change made
Value 10000000 for tpcc:mssqls_total_iterations is the same as existing value 10000000, no change made
Value timed for tpcc:mssqls_driver is the same as existing value timed, no change made
Value 2 for tpcc:mssqls_rampup is the same as existing value 2, no change made
Value 5 for tpcc:mssqls_duration is the same as existing value 5, no change made
primary starts immediately, runs rampup for 2 minutes then runs test for 5 minutes with 2 Active VU
replica1 starts 1 minutes after rampup completes and runs test for 4 minutes with 2 Active VU
replica2 starts 1 minutes after previous replica starts and runs test for 3 minutes with 2 Active VU
replica3 starts 1 minutes after previous replica starts and runs test for 2 minutes with 2 Active VU
replica4 starts 1 minutes after previous replica starts and runs test for 1 minutes with 2 Active VU
Switch from Local
to Primary mode?
Enter yes or no: replied yes
Setting Primary Mode at id : 23320, hostname : osprey
Primary Mode active at id : 23320, hostname : osprey
Starting 1 replica HammerDB instance
Starting 2 replica HammerDB instance
HammerDB CLI v4.7
Copyright (C) 2003-2023 Steve Shaw
Type "help" for a list of commands
Starting 3 replica HammerDB instance
HammerDB CLI v4.7
Copyright (C) 2003-2023 Steve Shaw
Type "help" for a list of commands
Starting 4 replica HammerDB instance
HammerDB CLI v4.7
Copyright (C) 2003-2023 Steve Shaw
Type "help" for a list of commands
Doing wait to connect ....
Primary waiting for all replicas to connect .... 0 out of 4 are connected
HammerDB CLI v4.7
Copyright (C) 2003-2023 Steve Shaw
Type "help" for a list of commands
Initialized SQLite on-disk database C:/Users/Steve/AppData/Local/Temp/hammer.DB using existing tables (36,864 KB)
Switch from Local
to Replica mode?
Enter yes or no: replied yes
Initialized SQLite on-disk database C:/Users/Steve/AppData/Local/Temp/hammer.DB using existing tables (36,864 KB)
Setting Replica Mode at id : 15676, hostname : osprey
Initialized SQLite on-disk database C:/Users/Steve/AppData/Local/Temp/hammer.DB using existing tables (36,864 KB)
Replica connecting to localhost 23320 : Connection succeeded
Switch from Local
Received a new replica connection from host ::1

Enter yes or no: replied yes
New replica joined : {15676 osprey}
Switch from Local
to Replica mode?
Initialized SQLite on-disk database C:/Users/Steve/AppData/Local/Temp/hammer.DB using existing tables (36,864 KB)
Enter yes or no: replied yes
Setting Replica Mode at id : 16552, hostname : osprey
Switch from Local
to Replica mode?
Primary call back successful
Switched to Replica mode via callback
Received a new replica connection from host ::1
Replica connecting to localhost 23320 : Connection succeeded
Enter yes or no: replied yes
New replica joined : {15676 osprey} {16552 osprey}
Setting Replica Mode at id : 16072, hostname : osprey
Replica connecting to localhost 23320 : Connection succeeded
Received a new replica connection from host ::1
Setting Replica Mode at id : 18116, hostname : osprey
Primary call back successful
Switched to Replica mode via callback
Replica connecting to localhost 23320 : Connection succeeded
Received a new replica connection from host ::1
New replica joined : {15676 osprey} {16552 osprey} {16072 osprey}
New replica joined : {15676 osprey} {16552 osprey} {16072 osprey} {18116 osprey}
Primary call back successful
Switched to Replica mode via callback
Primary call back successful
Switched to Replica mode via callback
Primary waiting for all replicas to connect .... {15676 osprey} {16552 osprey} {16072 osprey} {18116 osprey} out of 4 are connected
Primary Received all replica connections {15676 osprey} {16552 osprey} {16072 osprey} {18116 osprey}
Database set to MSSQLServer
Database set to MSSQLServer
Database set to MSSQLServer
Database set to MSSQLServer
Database set to MSSQLServer
Setting primary to run 2 virtual users for 5 duration
Value 5 for tpcc:mssqls_duration is the same as existing value 5, no change made
Sending dbset all to 15676 osprey
Setting replica1 to start after 1 duration 4 VU count 2, Replica instance is 15676 osprey
Sending "diset tpcc mssqls_timeprofile false" to 15676 osprey
Value false for tpcc:mssqls_timeprofile is the same as existing value false, no change made
Sending "diset tpcc mssqls_rampup 0" to 15676 osprey
Changed tpcc:mssqls_rampup from 2 to 0 for MSSQLServer
Sending "diset tpcc mssqls_duration 4" to 15676 osprey
Changed tpcc:mssqls_duration from 5 to 4 for MSSQLServer
Sending "vuset vu 2" to 15676 osprey
Sending dbset all to 16552 osprey
Setting replica2 to start after 1 duration 3 VU count 2, Replica instance is 16552 osprey
Sending "diset tpcc mssqls_timeprofile false" to 16552 osprey
Value false for tpcc:mssqls_timeprofile is the same as existing value false, no change made
Sending "diset tpcc mssqls_rampup 0" to 16552 osprey
Changed tpcc:mssqls_rampup from 2 to 0 for MSSQLServer
Sending "diset tpcc mssqls_duration 3" to 16552 osprey
Changed tpcc:mssqls_duration from 5 to 3 for MSSQLServer
Sending "vuset vu 2" to 16552 osprey
Sending dbset all to 16072 osprey
Setting replica3 to start after 1 duration 2 VU count 2, Replica instance is 16072 osprey
Sending "diset tpcc mssqls_timeprofile false" to 16072 osprey
Value false for tpcc:mssqls_timeprofile is the same as existing value false, no change made
Sending "diset tpcc mssqls_rampup 0" to 16072 osprey
Changed tpcc:mssqls_rampup from 2 to 0 for MSSQLServer
Sending "diset tpcc mssqls_duration 2" to 16072 osprey
Changed tpcc:mssqls_duration from 5 to 2 for MSSQLServer
Sending "vuset vu 2" to 16072 osprey
Sending dbset all to 18116 osprey
Setting replica4 to start after 1 duration 1 VU count 2, Replica instance is 18116 osprey
Sending "diset tpcc mssqls_timeprofile false" to 18116 osprey
Value false for tpcc:mssqls_timeprofile is the same as existing value false, no change made
Sending "diset tpcc mssqls_rampup 0" to 18116 osprey
Changed tpcc:mssqls_rampup from 2 to 0 for MSSQLServer
Sending "diset tpcc mssqls_duration 1" to 18116 osprey
Changed tpcc:mssqls_duration from 5 to 1 for MSSQLServer
Sending "vuset vu 2" to 18116 osprey
Script loaded, Type "print script" to view
Script loaded, Type "print script" to view
Script loaded, Type "print script" to view
Script loaded, Type "print script" to view
Script loaded, Type "print script" to view
Vuser 1 created MONITOR - WAIT IDLE
Vuser 2 created - WAIT IDLE
Vuser 1 created MONITOR - WAIT IDLE
Vuser 1 created MONITOR - WAIT IDLE
Vuser 1 created MONITOR - WAIT IDLE
Vuser 1 created MONITOR - WAIT IDLE
Vuser 2 created - WAIT IDLE
Vuser 2 created - WAIT IDLE
Vuser 2 created - WAIT IDLE
Vuser 3 created - WAIT IDLE
Vuser 2 created - WAIT IDLE
Vuser 3 created - WAIT IDLE
Vuser 3 created - WAIT IDLE
Vuser 3 created - WAIT IDLE
Vuser 3 created - WAIT IDLE
Logging activated
to C:/Users/Steve/AppData/Local/Temp/hammerdb.log
3 Virtual Users Created with Monitor VU
3 Virtual Users Created with Monitor VU
3 Virtual Users Created with Monitor VU
3 Virtual Users Created with Monitor VU
3 Virtual Users Created with Monitor VU
Starting Primary VUs
Vuser 1:RUNNING
Vuser 1:Beginning rampup time of 2 minutes
Vuser 2:RUNNING
Vuser 2:Processing 10000000 transactions with output suppressed...
Vuser 3:RUNNING
Vuser 3:Processing 10000000 transactions with output suppressed...
Delaying Start of Replicas to rampup 2 replica1 1 replica2 1 replica3 1 replica4 1
Delaying replica1 for 3 minutes.
Delaying replica2 for 4 minutes.
Delaying replica3 for 5 minutes.
Delaying replica4 for 6 minutes.
Primary entering loop waiting for vucomplete
Vuser 1:Rampup 1 minutes complete ...
Vuser 1:Rampup 2 minutes complete ...
Vuser 1:Rampup complete, Taking start Transaction Count.
Vuser 1:Timing test period of 5 in minutes</programlisting>

        <para>Viewing the GUI or CLI transaction counter the effect can be
        seen that the load is varied as the replicas are automatically
        started.</para>

        <figure>
          <title>Step Load</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch4-16b.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>
    </section>

    <section>
      <title>Additional Driver Script Options for Stored Procedures and Server
      Side Reports: PostgreSQL, MySQL, MariaDB, Oracle, Db2 and EnterpriseDB
      PostgreSQL</title>

      <section>
        <title>PostgreSQL Stored Procedures</title>

        <para>With PostgreSQL by default the 5 TPROC-C transactions are
        implemented using PostgreSQL functions. From PostgreSQL v11.0 there is
        the option to use PostgreSQL stored procedures instead. However
        prepared statements are not supported by PostgreSQL for stored
        procedures only for functions and therefore if using the XML connect
        pool feature only PostgreSQL functions are supported. Conversely
        PgBouncer does not at the time of writing keep track of prepared
        statements and therefore when using event driven scaling, stored
        procedures are recommended.</para>
      </section>

      <section>
        <title>MySQL Prepare Statements</title>

        <para>With MySQL there is the option to use server side prepared
        statements. This option is mandatory if using the XML connect pool
        feature.</para>
      </section>

      <section>
        <title>MariaDB Prepare Statements</title>

        <para>With MariaDB there is the option to use server side prepared
        statements. This option is mandatory if using the XML connect pool
        feature.</para>
      </section>

      <section>
        <title>No Stored Procedures Option for MySQL and MariaDB</title>

        <para>From version 4.9 for MySQL and MariaDB there is an advanced
        option of No Stored Procedures. When selected this option will drive
        the TPROC-C workload entirely from the client, running the same SQL
        statements with the application logic within HammerDB. This option is
        useful when looking to run a workload with an increased network load
        and in tests increases the Bytes received and Bytes sent by 6X and 8X
        respectively. This option should not be expected to deliver NOPM/TPM
        values as high as using stored procedures due to the additional time
        spent at the network.</para>

        <para><figure>
            <title>No Stored Procedures Option</title>

            <mediaobject>
              <imageobject>
                <imagedata fileref="../DocBook/docs/images/ch4-15p.PNG"/>
              </imageobject>
            </mediaobject>
          </figure></para>
      </section>

      <section>
        <title>Oracle AWR Reports</title>

        <para>The Generation of Oracle AWR reports is built-in functionality
        with the Oracle Timed Test. At the end of the test HammerDB will
        report the snapshot numbers between which the report corresponds to
        the test.</para>
      </section>

      <section>
        <title>Db2 MONREPORT</title>

        <para>In the Db2 driver script options the Minutes for Test Duration
        is shown as monreportinterval in the Driver Script. This defines the
        period of time taken from the minutes for test duration that the
        monitoring user runs a monreport capture. The results are output at
        the end of the test and therefore selecting this option should be done
        in conjunction with the logfile enabled. While the MONREPORT is being
        captured the monitoring virtual user cannot bet terminated as control
        is handed over to the DB2 database and therefore shorter periods of
        report are optimal. In all cases in the MONREPORT interval specified
        is longer than the minutes for test duration then no MONREPORT will be
        captured.</para>
      </section>

      <section>
        <title>EnterpriseDB PostgreSQL DRITA</title>

        <para>If you have Enterprise DB installed and DRITA functionality
        enabled, by selecting this option HammerDB will automatically take
        DRITA snapshots for performance analysis of the workload between
        tests. For DRITA functionality to work you need the parameter
        timed_statistics = on set in your postgresql.conf file. With the test
        complete and the values you recorded if you selected the DRITA option
        you should next generate the DRITA report that corresponds to the
        reported SNAPIDs to show the PostgreSQL wait events, in the example
        below snapshots 2 and 3.</para>

        <programlisting>edb=# select * from sys_rpt(2,3,1000);
                                   sys_rpt                                   
-----------------------------------------------------------------------------
 WAIT NAME                                COUNT      WAIT TIME       % WAIT
 ---------------------------------------------------------------------------
 wal insert lock acquire                  1054357    2.300713        88.25
 xid gen lock acquire                     83471      0.195263        7.49
 db file read                             5523       0.067953        2.61
 buffer free list lock acquire            11133      0.029317        1.12
 query plan                               205        0.013703        0.53
 freespace lock acquire                   3          0.000007        0.00
 rel cache init lock acquire              0          0.000000        0.00
(9 rows)

edb=# 
</programlisting>
      </section>
    </section>

    <section>
      <title>Loading the Driver Script</title>

      <para>After selecting the Driver Script Options the Driver Script is
      loaded. The configured options can be seen in the Driver Script window
      and also modified directly there. The Load option can also be used to
      refresh the script to the configured Options.</para>

      <figure>
        <title>Driver Script Loaded</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch4-16.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>

    <section>
      <title>Configure Virtual Users</title>

      <para>Select Virtual User Options from the tree-view.</para>

      <figure>
        <title>Virtual User Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch4-17.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>This displays the Virtual User Options dialog.</para>

      <figure>
        <title>Virtual User</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch4-18.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The values have the following meaning.</para>

      <para><table>
          <title>Virtual User Options</title>

          <tgroup cols="2">
            <thead>
              <row>
                <entry align="center">Option</entry>

                <entry align="center">Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>Virtual Users</entry>

                <entry>The number of Virtual Users to create. Note that when
                running a Timed Workload HammerDB will automatically create an
                additional Virtual User to monitor the workload.</entry>
              </row>

              <row>
                <entry>User Delay(ms)</entry>

                <entry>User Delay(ms) defines the time to wait a Virtual User
                will wait behind the previous Virtual User before starting its
                test, this is to prevent a login storm with all Virtual Users
                attempting to login at the same time.</entry>
              </row>

              <row>
                <entry>Repeat Delay(ms)</entry>

                <entry>Repeat Delay(ms) is the time that each Virtual User
                will wait before running its next Iteration of the Driver
                Script. For the TPROC-C workload this should be considered as
                an 'outer loop' to the 'inner loop' of the Total Transactions
                per User in the TPROC-C Driver Script.</entry>
              </row>

              <row>
                <entry>Iterations</entry>

                <entry>Iterations is the number of times that the Driver
                Script is run in its entirety.</entry>
              </row>

              <row>
                <entry>Show Output</entry>

                <entry>Show Output will report Virtual User Output to the
                Virtual User Output Window, For TPROC-C tests this should be
                enabled.</entry>
              </row>

              <row>
                <entry>Log Output to Temp</entry>

                <entry>When enabled this appends all Virtual User Output to a
                text file in an available temp directory named
                hammerdb.log</entry>
              </row>

              <row>
                <entry>Use Unique Log Name</entry>

                <entry>Use a unique identifier for the Log Name.</entry>
              </row>

              <row>
                <entry>No Log Buffer</entry>

                <entry>By default text log output is buffered in memory before
                being written, this option writes the log output
                immediately.</entry>
              </row>

              <row>
                <entry>Log Timestamps</entry>

                <entry>Add an additional line of output with a timestamp every
                time that the log is written to.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>Select the Virtual User options, Press OK.</para>
    </section>

    <section>
      <title>Create and Run Virtual Users</title>

      <para>Double-click Create in the tree-view. The Virtual Users will be
      created and waiting in an idle status ready to run the Driver Script in
      the Script Editor Window. If you press Run instead it will both Create
      and Run the Virtual Users. If you have selected a Timed Workload the
      additional Virtual User created will be shown as a monitor.</para>

      <para><figure>
          <title>Virtual Users Create</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch4-19.PNG"/>
            </imageobject>
          </mediaobject>
        </figure></para>

      <para>Double-click on Run and the Virtual Users will login to the target
      database and begin running their workload.</para>

      <para><figure>
          <title>Virtual Users Running</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch4-20.PNG"/>
            </imageobject>
          </mediaobject>
        </figure></para>

      <para>When complete the Monitor Virtual User will report the Test
      Result, refer to Chapter for the configuration of how the NOPM and TPM
      values are reported. If logging has been selected these values will also
      be reported in the log. Where supported additional database side server
      report information will also be reported.</para>

      <para><figure>
          <title>Virtual Users Complete</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch4-21.PNG"/>
            </imageobject>
          </mediaobject>
        </figure></para>

      <para>you may choose to run all of your performance tests manually in
      this way, however generating a performance profile is the key to
      successful database performance analysis requiring the running of a
      sequence of tests. Consequently HammerDB enables a way to automate the
      running of this sequence of tests with the Autopilot Feature.</para>
    </section>
  </chapter>

  <chapter>
    <title>Autopilot for Automated Testing</title>

    <para>To automate this process of repeated tests HammerDB provides the
    autopilot feature that enables you to configure a single test to be
    repeated by a different numbers of virtual users a number of times.
    Conceptually autopilot is best understood as having instructed a virtual
    DBA to manually repeat the test you have configured a number of times at a
    pre-determined time interval. That virtual DBA will then run the tests by
    ‘virtually’ pressing exactly the same buttons on the HammerDB interface
    that you would press as if running the test manually yourself. It is
    important to understand this concept as the most frequent user errors in
    using autopilot are as a result of not following this approach. Before
    running autopilot you should ensure that you have run a number of tests
    manually and your system is in an optimal configuration for running tests
    up to your planned maximum Virtual User count. For example you should
    enable enough space to schema growth throughout all of the tests you plan
    to run.</para>

    <section>
      <title>Configure and Run Autopilot</title>

      <para>To begin configuring Autopilot mode follow the steps described in
      the previous Chapter for Running OLTP Timed Tests before creating and
      running the Virtual Users, these will be configured automatically.
      Select Autopilot Options from the tree-view as shown.</para>

      <para><figure>
          <title>Autopilot Options</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch5-1.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>This shows the Autopilot Options Dialog.</para>

      <para><figure>
          <title>Autopilot Options</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch5-2.PNG"/>
            </imageobject>
          </mediaobject>
        </figure></para>

      <para>Configure the Autopilot options in the same manner as you would
      use to instruct your Virtual DBA:</para>

      <table>
        <title>Autopilot Options</title>

        <tgroup cols="2">
          <thead>
            <row>
              <entry align="center">Option</entry>

              <entry align="center">Description</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>Autopilot Disabled/Autopilot Enabled</entry>

              <entry>This Autopilot Disabled/Autopilot Enabled Radio buttons
              give you the option to select whether the Autopilot button is
              enabled on the main window.</entry>
            </row>

            <row>
              <entry>Minutes per Test in Virtual User Sequence</entry>

              <entry>The minutes for test duration defines the time interval
              between which your virtual DBA will create the Virtual Users,
              stop the test and create the next Virtual Users in the sequence.
              You should configure this value in relation to the Minutes for
              Ramup Time and Minutes for Test Duration given in the Timed Test
              options. For example if the values in the test script are 2 and
              5 minutes respectively then 10 minutes for the Autopilot Options
              is a good value to allow the test to complete before the next
              test in the sequence is run. If the test overruns the time
              interval and the Virtual Users are still running the sequence
              will wait for the Virtual Users to complete before proceeding
              however note any pending output will be discarded and therefore
              for example if the TPM and NOPM values have not been reported by
              the time the test is stopped they will not be reported at
              all.</entry>
            </row>

            <row>
              <entry>Virtual User Sequence (Space Separated Values)</entry>

              <entry>The Virtual User Sequence defines the number of Virtual
              Users to be configured in order for a sequence of tests
              separated by the Minutes for Test Duration. Note that for a
              Timed workload the Monitor Virtual User will be added and
              therefore the sequence defines the number of active worker
              Virtual Users. Therefore in this example the actual users
              running the workload will be 1, 2, 4, 8, 12, 16, 20 and 24
              however and additional one will be created.</entry>
            </row>

            <row>
              <entry>Virtual User Options</entry>

              <entry>These values are exactly the same as set when defining
              the Virtual User Options, you should ensure that Output is
              enabled and configure preferred logging options.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <para>Double-click on the Autopilot Icon to begin running the sequence
      of tests</para>

      <figure>
        <title>Run Autopilot</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch5-3.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The Autopilot Window is shown tracking the Monitor Virtual User
      output and the time interval, no further interaction is required.</para>

      <figure>
        <title>Autopilot Running</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch5-4.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>After the first test HammerDB reports the output and then
      configures the Virtual Users and runs the second test
      automatically.</para>

      <figure>
        <title>Autopilot Continuing</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch5-5.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>When the sequence is complete you will see the message Autopilot
      Sequence ended. You can now gather all of your test results.</para>

      <para><figure>
          <title>Autopilot Complete</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch5-6.PNG"/>
            </imageobject>
          </mediaobject>
        </figure></para>
    </section>

    <section>
      <title>Autopilot Troubleshooting</title>

      <para>The most Frequent Autopilot Configuration Error is caused by
      configuring the Autopilot Time Interval to be less than the combined
      rampup and duration time of the test that is running. When viewed from
      the concept of a "Virtual DBA" this User has been instructed to press
      the Stop button before the test has ended, consequently a warning is
      produced and no output results are reported. To resolve this issue
      ensure that the time interval is set long enough to allow the configured
      tests to complete inside this interval.</para>

      <para><figure>
          <title>Autopilot Error</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch5-7.PNG"/>
            </imageobject>
          </mediaobject>
        </figure></para>
    </section>

    <section>
      <title>Extending Autopilot to start automatically</title>

      <para>Autopilot can be started automatically by adding the keyword
      “auto” followed by the name of a script to run, this script must end in
      the extension .tcl.</para>

      <programlisting>./hammerdb auto
Usage: hammerdb [ auto [ script_to_autoload.tcl  ] ]
</programlisting>

      <para>For example</para>

      <programlisting>./hammerdb auto newtpccscript.tcl</programlisting>

      <para>On doing so HammerDB will now load the script newtpccscript.tcl at
      startup and immediately enter the autopilot sequence defined in
      config.xml. Upon completion HammerDB will exit. This functionality
      enables the potential to run scripted workloads with the HammerDB GUI
      such as the following with multiple sequences of autopilot interspersed
      with a database refresh.</para>

      <programlisting>#!/bin/bash

set -e
SEQ1="4 6 8 10"
SEQ2="12 14 16 18"
SEQ3="20 22 24 26"
CONFIGFILE='/usr/local/hammerDB/config.xml'
RUNS=6

for x in $(eval echo "{1..$RUNS}")
do
        # Running a number of passes for this autopilot sequence
        echo "running run $x of $RUNS"

        for s in "$SEQ1" "$SEQ2" "$SEQ3"
        do
                echo "Running tests for series: $s"
                sed -i "s/&lt;autopilot_sequence&gt;.*&lt;\/autopilot_sequence&gt;/&lt;autopilot_sequence&gt;${s}&lt;\/autopilot_sequence&gt;/" $CONFIGFILE

                (cd /usr/local/hammerDB/ &amp;&amp; ./hammerdb auto TPCC.postgres.tcl)

                echo "Reloading data"
                ssh postgres@postgres  '/var/lib/pgsql/reloadData.sh'
        done
done
</programlisting>
    </section>
  </chapter>

  <chapter>
    <title>Transactions</title>

    <para>HammerDB includes a Transaction Counter that logs into the target
    database and samples the transaction rate displaying it in graph format to
    view the TPM of a test in real time. Note that the TPM value is displayed
    as opposed to the NOPM value as TPM is selected from a database in-memory
    table and therefore sampling does not impact the test being measured. NOPM
    on the other hand is sampled from the schema itself and is therefore only
    measured at the start and end of the test to minimize the impact of
    testing upon performance. To configure the Transaction Counter select the
    Transactions tree-view. If Virtual Users are running the Transaction
    Counter Options can be selected from the menu.</para>

    <figure>
      <title>Transaction Counter Options</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="docs/images/ch6-1.PNG"/>
        </imageobject>
      </mediaobject>
    </figure>

    <section>
      <title>Oracle Transaction Counter</title>

      <para>For Oracle the connection parameters are the same as the schema
      options. There is also an option to query a TimesTen database instead of
      an Oracle one and to select transactions from an Oracle RAC cluster. The
      refresh rate determines the sampling interval.</para>

      <figure>
        <title>Oracle TX Counter Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch6-2.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>For single instance Oracle, transactions are sampled with the
      following statement. This displays transactions with the same value as
      used in Oracle Enterprise Manager and in Oracle AWR reports.</para>

      <programlisting>select sum(value) from v$sysstat where name = 'user commits' or name = 'user rollbacks'</programlisting>

      <para>For Oracle RAC gv$sysstat is queried for global
      transactions.</para>

      <programlisting>select sum(value) from gv$sysstat where name = 'user commits' or name = 'user rollbacks'</programlisting>

      <para>for TimesTen the following SQL is used.</para>

      <programlisting>select (xact_commits + xact_rollbacks) from sys.monitor</programlisting>
    </section>

    <section>
      <title>SQL Server Transaction Counter</title>

      <para>For SQL Server the connection parameters are the same as the
      schema options. The refresh rate determines the sampling
      interval.</para>

      <para/>

      <figure>
        <title>SQL Server TX Counter Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch6-3.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The following SQL is used to sample the transaction rate
      displaying the same value as can be seen in the Activity Monitor in
      SSMS.</para>

      <programlisting>select cntr_value from sys.dm_os_performance_counters where counter_name = 'Batch Requests/sec'</programlisting>
    </section>

    <section>
      <title>Db2 Transaction Counter</title>

      <para>For Db2 the connection parameters are the same as the schema
      options. The refresh rate determines the sampling interval.</para>

      <figure>
        <title>Db2 TX Counter Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch6-4.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The following SQL is used to sample the transaction rate.</para>

      <programlisting>select total_app_commits + total_app_rollbacks from sysibmadm.mon_db_summary</programlisting>
    </section>

    <section>
      <title>MySQL Transaction Counter</title>

      <para>For MySQL the connection parameters are the same as the schema
      options. The refresh rate determines the sampling interval.</para>

      <figure>
        <title>MySQL TX Counter Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch6-5.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The following SQL is used to sample the transaction rate.</para>

      <programlisting>show global status where Variable_name = 'Com_commit' or Variable_name =  'Com_rollback'</programlisting>

      <para>Note that Com_commit is used instead of the handler_commit value
      used in previous releases of HammerDB as a result of MySQL Bug #52453
      handler_commit is incremented for InnoDB SELECT queries.</para>
    </section>

    <section>
      <title>MariaDB Transaction Counter</title>

      <para>For MariaDB the connection parameters are the same as the schema
      options. The refresh rate determines the sampling interval.</para>

      <figure>
        <title>MariaDB TX Counter Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch6-5a.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The following SQL is used to sample the transaction rate.</para>

      <programlisting>show global status where Variable_name = 'Com_commit' or Variable_name =  'Com_rollback'</programlisting>
    </section>

    <section>
      <title>PostgreSQL Transaction Counter</title>

      <para>For PostgreSQL the connection parameters are the same as the
      schema options. The refresh rate determines the sampling
      interval.</para>

      <figure>
        <title>PostgreSQL TX Counter Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch6-6.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The following SQL is used to sample the transaction rate.</para>

      <programlisting>select sum(xact_commit + xact_rollback) from pg_stat_database</programlisting>
    </section>

    <section>
      <title>Running the Transaction Counter</title>

      <para>During a test, select the start transaction counter button.</para>

      <figure>
        <title>Start Transaction Counter</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch6-8.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>On starting the transaction counter will begin sampling the
      transaction data.</para>

      <figure>
        <title>Transaction Counter Starting</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch6-9.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The transaction counter will be displayed and continually sample
      and display the transaction rate during the test. It is important to
      note that the transaction rate is sampled with the SQL detailed above
      for the database selected and therefore all transactions on the database
      are sampled whether from HammerDB or another application running at the
      same time. Similarly if 2 or more instances of HammerDB are run against
      the same database at the same time, the cumulative transaction is
      sampled.</para>

      <figure>
        <title>Transaction Counter Running</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch6-10.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>While active the Transaction Counter Window can be dragged out of
      the main HammerDB display to be displayed in an standalone window by
      selecting and dragging the notebook tab. To return to the main display
      close the window and it will be re-embedded in the main
      interface.</para>

      <figure>
        <title>Transaction Counter standalone.</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch6-11.PNG"/>
          </imageobject>
        </mediaobject>

        <para>From v4.7 the Transaction counter will also resize according to
        the HammerDB Window size.</para>
      </figure>

      <figure>
        <title>Transaction Counter Window Resize</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="../DocBook/docs/images/ch6-11a.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>

    <section>
      <title>Logging Transaction Counter Output</title>

      <para>HammerDB also provides the option to log the output of the
      transaction counter to a separate logfile. This enables the recording of
      transactions throughout the test that can be used to verify the
      transaction rate across time. To enable transaction counter logging
      select Log Output to Temp. There are also options to use a unique log
      name and to generate timestamps. With the following options
      selected.</para>

      <figure>
        <title>Log Transaction Counter Output</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch6-12.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The following output in a logfile called hdbtcount.log will be
      produced in the defined temporary directory.</para>

      <programlisting>Hammerdb Transaction Counter Log @ Mon Apr 12 15:06:00 BST 2021
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
0 MSSQLServer tpm @ Mon Apr 12 15:06:03 BST 2021
317814 MSSQLServer tpm @ Mon Apr 12 15:06:13 BST 2021
316860 MSSQLServer tpm @ Mon Apr 12 15:06:23 BST 2021
300186 MSSQLServer tpm @ Mon Apr 12 15:06:33 BST 2021
298410 MSSQLServer tpm @ Mon Apr 12 15:06:44 BST 2021
282222 MSSQLServer tpm @ Mon Apr 12 15:06:54 BST 2021
275118 MSSQLServer tpm @ Mon Apr 12 15:07:04 BST 2021
263574 MSSQLServer tpm @ Mon Apr 12 15:07:14 BST 2021
283950 MSSQLServer tpm @ Mon Apr 12 15:07:24 BST 2021
276912 MSSQLServer tpm @ Mon Apr 12 15:07:34 BST 2021
278562 MSSQLServer tpm @ Mon Apr 12 15:07:44 BST 2021
272130 MSSQLServer tpm @ Mon Apr 12 15:07:54 BST 2021
269208 MSSQLServer tpm @ Mon Apr 12 15:08:05 BST 2021
290556 MSSQLServer tpm @ Mon Apr 12 15:08:15 BST 2021
245502 MSSQLServer tpm @ Mon Apr 12 15:08:25 BST 2021
278718 MSSQLServer tpm @ Mon Apr 12 15:08:35 BST 2021
279438 MSSQLServer tpm @ Mon Apr 12 15:08:45 BST 2021
245094 MSSQLServer tpm @ Mon Apr 12 15:08:55 BST 2021
267156 MSSQLServer tpm @ Mon Apr 12 15:09:05 BST 2021
278580 MSSQLServer tpm @ Mon Apr 12 15:09:15 BST 2021
260592 MSSQLServer tpm @ Mon Apr 12 15:09:25 BST 2021
290712 MSSQLServer tpm @ Mon Apr 12 15:09:36 BST 2021
275400 MSSQLServer tpm @ Mon Apr 12 15:09:46 BST 2021
304038 MSSQLServer tpm @ Mon Apr 12 15:09:56 BST 2021
302268 MSSQLServer tpm @ Mon Apr 12 15:10:06 BST 2021
251604 MSSQLServer tpm @ Mon Apr 12 15:10:16 BST 2021
256410 MSSQLServer tpm @ Mon Apr 12 15:10:26 BST 2021
273306 MSSQLServer tpm @ Mon Apr 12 15:10:36 BST 2021
</programlisting>

      <para>Equivalent functionality exists in the CLI to enable transaction
      counter logging with the tcset command.</para>
    </section>

    <section>
      <title>Ribbon effect</title>

      <para>From HammerDB v4.7 the ribbon effect transaction counter has been
      deprecated. However if preferred it can be manually enabled through the
      setting tc_graph_ribbon in generic.xml as shown. Note that this should
      be at installation as at first startup the setting will be made in the
      SQLite storage and subsequent changes to the xml file will not be
      detected. For later changes to the xml to be detected you can delete the
      generic.db file in the SQLite storage area and then the settings in
      generic.xml will be applied for future use. It is not planned to have a
      setting to enable/disable the ribbon effect during use and may be fully
      deprecated in a future release.</para>

      <programlisting>&lt;settings&gt;
    &lt;tc_refresh_rate&gt;10&lt;/tc_refresh_rate&gt;
    &lt;tc_log_to_temp&gt;0&lt;/tc_log_to_temp&gt;
    &lt;tc_unique_log_name&gt;0&lt;/tc_unique_log_name&gt;
    &lt;tc_log_timestamps&gt;0&lt;/tc_log_timestamps&gt;
    <emphasis role="bold">&lt;tc_graph_ribbon&gt;true&lt;/tc_graph_ribbon&gt;</emphasis>
&lt;/settings&gt;</programlisting>
    </section>
  </chapter>

  <chapter>
    <title>CPU and Database Metrics</title>

    <para>By default HammerDB metrics displays the CPU utilisation per core
    across the target system. HammerDB has also introduced a database metrics
    display initially for the Oracle Database. HammerDB Metrics uses an agent
    and display configuration meaning that the agent must be installed on the
    SUT. This can be accomplished by installing HammerDB on the SUT as well as
    the server. On Linux the sysstat package must be pre-installed where the
    agent is running. An agent on Linux can can communicate with the HammerDB
    display running on Windows and vice versa.</para>

    <programlisting>$ mpstat -V
sysstat version 11.5.7
(C) Sebastien Godard (sysstat &lt;at&gt; orange.fr</programlisting>

    <para>On Windows a version of mpstat is included with HammerDB.</para>

    <section>
      <title>Start the Agent</title>

      <para>To start the agent on Linux run the agent program locally in the
      agent directory.</para>

      <programlisting>$./agent 
Initializing HammerDB Metric Agent 4.1
HammerDB Metric Agent active @ id 20376 hostname CRANE (Ctrl-C to Exit)</programlisting>

      <para>On Windows double-click on agent.bat in the agent
      directory.</para>

      <figure>
        <title>agent.bat</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch7-1.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>On both Windows and Linux your Firewall configuration should
      permit communication between the hosts where the agent and the display
      are running, for example on Windows you may see the following security
      alert as the agent will open a port for communication, access needs to
      be permitted to enable communication.</para>

      <figure>
        <title>Security Alert</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch7-2.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>A window will open indicating the id that the agent is listening
      on. Pressing Ctrl-C or closing the window will close the agent.</para>

      <figure>
        <title>Windows agent</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch7-3.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>

    <section>
      <title>Configure Metrics</title>

      <para>In HammerDB select Metrics Options</para>

      <figure>
        <title>Metrics Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch7-4.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Enter the id and canonical hostname of the agent and press
      OK.</para>

      <figure>
        <title>Agent Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch7-5.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Double-click on Display</para>

      <figure>
        <title>Display</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch7-6.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>the agent will report the connection of the display</para>

      <figure>
        <title>Agent connected</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch7-7.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>and the display will report the connection information of the
      agent</para>

      <para><figure>
          <title>Display connected</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch7-8.PNG"/>
            </imageobject>
          </mediaobject>
        </figure></para>
    </section>

    <section>
      <title>Monitor Metrics</title>

      <para>The display will now report the CPU utilisation per core on the
      SUT during a workload with user CPU utilisation shown in green and
      system utilisation shown in red. This per core about is particularly
      useful for diagnosing database workload issues where the load is not
      evenly distributed across all cores. A typical example is where all of
      the network interrupt handling is done on the first core, this will be
      evident from the HammerDB CPU metrics showing the first core at 100%
      system utilisation.</para>

      <para>The agent to display configuration is compatible to run
      interchangeably between Linux and Windows with both the agent and
      display on either of the operating systems. Additionally the agent may
      be run to display the CPU metrics whilst the load is run from the
      command line or another system.</para>

      <figure>
        <title>Metrics running</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch7-9.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>As with the transaction counter the Metrics display can be dragged
      out of the main window for separate viewing and the scrollbar used for
      reviewing large core counts.</para>

      <figure>
        <title>Large Core count</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch7-10.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>If the agent or display is closed the corresponding connection
      will also close and wait for a new connection. The Metrics Display can
      be closed by pressing the corresponding stop button.</para>
    </section>

    <section>
      <title>Oracle Database Metrics</title>

      <para>When the Oracle Database is selected on both Windows and Linux an
      additional option is available to connect to the Oracle Database and
      display detailed performance metrics.</para>

      <figure>
        <title>Oracle Metrics Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch7-11.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>When the metrics button is pressed HammerDB connects to the
      database and displays graphical information from the Active Session
      History detailing wait events. By default in embedded mode the Oracle
      Database Metrics will display the Active Session History Graph. For
      detailed Oracle Database Metrics the Notebook tab should be dragged out
      and expanded to display in a separate window.</para>

      <figure>
        <title>Oracle Metrics Display Linux</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch7-12.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>When display in a separate window, it is possible to make a
      selection from the window and display the wait events related to that
      period of time. When the SQL_ID is selected the buttons then enable the
      detailed viewing of SQL text, the explain plan, IO statistics and SQL
      statistics related to that SQL.</para>

      <para><figure>
          <title>Oracle Metrics Display Windows</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch7-13.png"/>
            </imageobject>
          </mediaobject>
        </figure></para>

      <para>When an event is selected the analysis shows details related to
      that particular event.</para>

      <figure>
        <title>Oracle Metrics Event</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch7-14.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The CPU Metrics button displays the current standard HammerDB CPU
      Metrics display in an embedded Window and requires the agent running on
      the database server. The CPU metrics are not recorded as historical data
      relating to the Active Session History.</para>

      <figure>
        <title>Oracle Database CPU Metrics</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch7-15.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>

    <section>
      <title>PostgreSQL Database Metrics</title>

      <para>When the PostgreSQL Database is selected on both Windows and Linux
      an additional option is available to connect to the PostgreSQL Database
      and display detailed performance metrics. PostgreSQL metrics use the
      Active Session History feature and therefore the pg_stat_statements and
      pg_sentinel extensions must be installed and operational with parameters
      set in postgresql.conf such as follows.</para>

      <programlisting>shared_preload_libraries = 'pg_stat_statements,pgsentinel'
track_activity_query_size=2048
pg_stat_statements.save=on
pg_stat_statements.track=all
pgsentinel_pgssh.enable = true
pgsentinel_ash.pull_frequency = 1
pgsentinel_ash.max_entries = 1000000</programlisting>

      <para>The PostgreSQL Metrics Options screen requires details of
      superuser access to connect to the database.</para>

      <figure>
        <title>PostgreSQL Metrics Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch7-16.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>When the metrics button is pressed HammerDB connects to the
      database and verifies if the pg_active_session_history table is
      installed and populated with data, if this is verified it displays
      graphical information from the PostgreSQL Active Session History
      detailing wait events. By default in embedded mode the PostgreSQL
      Metrics will display the Active Session History Graph. For detailed
      PostgreSQL Database Metrics the Notebook tab should be dragged out and
      expanded to display in a separate window.</para>

      <figure>
        <title>PostgreSQL Metrics Display Linux</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch7-17.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>When display in a separate window, it is possible to make a
      selection from the window and display the wait events related to that
      period of time when the test was running. When the SQL_ID is selected
      the buttons then enable the detailed viewing of SQL text, IO statistics
      and SQL statistics related to that SQL.</para>

      <para><figure>
          <title>PostgreSQL Metrics Display Expanded</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch7-18.png"/>
            </imageobject>
          </mediaobject>
        </figure></para>

      <para>When an event is selected the analysis shows details related to
      that particular event for the time period selected. Selecting the
      username shows the statistics related to that particular session.</para>

      <figure>
        <title>PostgreSQL Metrics Event</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch7-19.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>As described in the Oracle metrics section the CPU Metrics button
      displays the current standard HammerDB CPU Metrics display in an
      embedded Window. To enable this functionality follow the guide for CPU
      metrics earlier in this section.</para>
    </section>
  </chapter>

  <chapter>
    <title>Remote Primary and Replica Modes</title>

    <para>HammerDB allows for multiple instances of the HammerDB program to
    run in Primary and Replica modes. Running with multiple modes enables the
    additional instances to be controlled by a single master instance either
    on the same load testing server or across the network. This functionality
    can be particularly applicable when testing Virtualized environments and
    the desire is to test multiple databases running in virtualized guests at
    the same time. Similarly this functionality is useful for clustered
    databases with multiple instances such as Oracle Real Application Clusters
    and wishing to partition a load precisely across servers. HammerDB Remote
    Modes are entirely operating system independent and therefore an instance
    of HammerDB running on Windows can be Primary to one or more instances
    running on Linux and vice versa. Additionally there is no requirement for
    the workload to be the same and therefore it would be possible to connect
    multiple instances of HammerDB running on Windows and Linux simultaneously
    testing SQL Server, Oracle, MySQL and PostrgreSQL workloads in a
    virtualized environment. In the bottom right hand corner of the interface
    the status bar shows the mode that HammerDB is running in. By default this
    will be Local Mode.</para>

    <para>Note that communication between Primary and Replica is enabled by
    the comm module which opens a default channel called ::comm::comm. If an
    exterior application attempts to connect then an warning such as follows
    will be shown.</para>

    <programlisting>hammerdb&gt;Warning: Connection from 127.0.0.1/49436 received on ::comm::comm with unknown protocol "ddffdfdfd"</programlisting>

    <figure>
      <title>Mode</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="docs/images/ch11-1.PNG"/>
        </imageobject>
      </mediaobject>
    </figure>

    <sect1>
      <title>Primary Mode</title>

      <para>From the tree-view select Mode Options.</para>

      <figure>
        <title>Mode Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch11-2.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>This displays the Mode Options as shown in Figure 3 confirming
      that the current mode is Local.</para>

      <figure>
        <title>Mode Options Select</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch11-3.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Select Primary Mode and click OK.</para>

      <figure>
        <title>Primary Mode Select</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch11-6.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Confirm the selection.</para>

      <figure>
        <title>Mode Confirmation</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch11-4.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>This will show that Master Mode is now active and the ID and
      hostname it is running on.</para>

      <para><figure>
          <title>Mode Active</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch11-5.PNG"/>
            </imageobject>
          </mediaobject>
        </figure> Note that this will also be recorded in the console display
      and the current Mode displayed in the status bar at the bottom right of
      the Window.</para>

      <programlisting>Setting Primary Mode at id : 18808, hostname : osprey</programlisting>
    </sect1>

    <sect1>
      <title>Replica Mode</title>

      <para>On another instance of HammerDB select Replica Mode, enter the id
      and hostname of the Primary and select OK.</para>

      <para><figure>
          <title>Replica Mode</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch11-7.PNG"/>
            </imageobject>
          </mediaobject>
        </figure></para>

      <para>Confirm the change and observe the Mode connection on both the
      Replica</para>

      <programlisting>Setting Replica Mode at id : 18424, hostname : osprey
Replica connecting to osprey 18808 : Connection succeeded
Primary call back successful</programlisting>

      <para>and the Primary. There is no restriction on the number of Replicas
      that can be connected to one Primary.</para>

      <programlisting>Received a new replica connection from host fe80::9042:505b:49de:beb4%26
New replica joined : {18424 osprey}</programlisting>
    </sect1>

    <sect1>
      <title>Primary Distribution</title>

      <para>The Primary Distribution button in the edit menu now becomes
      active to distribute scripts across instances.</para>

      <figure>
        <title>Primary Distribution</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch11-8.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Pressing this button enables the distribution of the contents of
      the Script Editor to all connected instances.</para>

      <programlisting>Distributing to 18424 osprey ...Primary Distribution Succeeded</programlisting>

      <para>The TPROC-C timed driver scripts reference the operating Mode and
      when loaded will set the parameter "mode" to the operating Mode running
      on that system.</para>

      <para>If loaded locally a script will show the Mode that the instance of
      HammerDB is running in which by default will be "Local".</para>

      <programlisting>set mode "Local" ;# HammerDB operational mode</programlisting>

      <para>Once the Mode is set to "Primary" when the script is loaded on the
      Primary it will show the correct mode.</para>

      <programlisting>set mode "Primary" ;# HammerDB operational mode</programlisting>

      <para>When distributed from the Primary to the Replica the Replica will
      change the mode to the correct setting.</para>

      <programlisting>set mode "Replica" ;# HammerDB operational mode</programlisting>

      <para>Once a Replica is connected to a Primary all actions that are
      taken on the Primary will be replicated on the Replica. All of your
      workload choices of creating and running and closing down virtual users
      will be replicated automatically on the connected Replicas enabling
      control and simultaneous timing from a central point. This enables
      workloads to be directed to different database instances simultaneously.
      When operating in Replica Mode the Monitor Virtual User on that instance
      of HammerDB will not capture any performance data and report that "No
      snapshots are taken", the Replica will only run the active Virtual
      Users. Note that running a schema creation with multiple connected
      instances is not supported.</para>

      <figure>
        <title>Operating in Replica Mode</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch11-9.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>When the workload is complete the Primary will terminate the
      Virtual Users on the Replicas meaning that running in Remote Mode
      configurations is compatible with Autopilot.</para>

      <figure>
        <title>Replica Mode terminated</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch11-10.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>If it is wished to capture the performance metrics on the Replica
      as well as the Primary the operational mode can be manually changed to
      "Local". In this case the Replicas will capture performance data from
      the databases instances that they are connected to.</para>

      <programlisting>set mode "Local" ;# HammerDB operational mode</programlisting>

      <para>To disable Remote Modes select Local Mode on the Primary on
      confirmation all connected instances will return to Local Mode.</para>

      <para>Remote modes functionality in the CLI can be accessed using the
      switchmode command with the GUI and CLI being interchangeable and
      therefore a number of CLI Replicas can be connected to a GUI Primary if
      desired.</para>
    </sect1>
  </chapter>

  <chapter>
    <title>Command Line Interface (CLI)</title>

    <para>HammerDB can be run from the command line without a graphical
    interface. It is recommend that new users become familiar with using the
    graphical interface before using the command line as the command line
    offers the same workflow and therefore once the graphical interface is
    understood learning the command line will be more straightforward. The CLI
    implements equivalent readline functionality for navigation. The CLI can
    be used in conjunction with scripting to build a powerful automated
    environment. From v4.6 the CLI can run in both interactive and scripting
    mode using the default Tcl mode or Python accepting the syntax and
    commands of either language depending on the mode chosen. Both the CLI and
    the GUI run exactly the same commands underneath the interactive layers,
    for example when operational the Virtual Users run identical workloads and
    therefore performance measurements between the CLI and GUI are
    interchangeable.</para>

    <section>
      <title>Start the CLI</title>

      <para>To start the command line in interactive mode on Linux run:</para>

      <programlisting>steve@CRANE:~/HammerDB-4.10$ ./hammerdbcli 
HammerDB CLI v4.10
Copyright (C) 2003-2024 Steve Shaw
Type "help" for a list of commands
The xml is well-formed, applying configuration
hammerdb&gt;</programlisting>

      <para>On Windows double-click hammerdbcli.bat</para>

      <figure>
        <title>hammerdbcli.bat</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch8-1.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>This will display a console command Window. On Windows this
      console command window has been designed to run with white text on a
      black background and sets the colour scheme accordingly.</para>

      <figure>
        <title>CLI Windows</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch8-2.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>

    <section>
      <title>Start the CLI in Python</title>

      <para>To use Python as the interactive or scripting language HammerDB
      will use the system installed Python environment and will be dependent
      on a particular version as follows:</para>

      <table>
        <title>Python version dependency</title>

        <tgroup cols="2">
          <tbody>
            <row>
              <entry>Linux</entry>

              <entry>3.8</entry>
            </row>

            <row>
              <entry>Red Hat Enterprise Linux</entry>

              <entry>3.6</entry>
            </row>

            <row>
              <entry>Windows</entry>

              <entry>3.10</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <para>Note that if an alternative version is required then HammerDB can
      be build from source to change the Python version.</para>

      <para>To start the CLI in Python mode run the hammerdbcli command from
      the command line in Linux specifying the "py" or "python" options.
      HammerDB will then accept Python syntax commands enabling it to be more
      closely integrated into existing Python environments.</para>

      <figure>
        <title>CLI Python</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="../DocBook/docs/images/ch8-2aa.png"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>

    <section>
      <title>CLI Commands</title>

      <para>To learn CLI commands type "help".</para>

      <programlisting>HammerDB v4.10 CLI Help Index

Type "help command" for more details on specific commands below

        buildschema
        checkschema
        deleteschema
        clearscript
        savescript
        customscript
        custommonitor
        datagenrun
        dbset
        dgset
        diset
        distributescript
        giset
        jobs
        librarycheck
        loadscript
        print
        quit
        steprun
        switchmode
        tcset
        tcstart
        tcstatus
        tcstop
        vucomplete
        vucreate
        vudestroy
        vurun
        vuset
        vustatus
        wsport
        wsstart
        wsstatus
        wsstop

hammerdb&gt;
</programlisting>

      <para>The commands have the following functionality.</para>

      <para><table>
          <title>CLI commands</title>

          <tgroup cols="3">
            <colspec colwidth="297*"/>

            <colspec colwidth="300*"/>

            <colspec colwidth="403*"/>

            <thead>
              <row>
                <entry align="center">Command</entry>

                <entry align="center">Usage</entry>

                <entry align="center">Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>buildschema</entry>

                <entry>Usage: buildschema</entry>

                <entry>Runs the schema build for the database and benchmark
                selected with dbset and variables selected with diset.
                Equivalent to the Build command in the graphical interface.
                Note that the buildschema command will assume the "yes" reply
                to the Yes or no prompt to proceed with the schema build. This
                is to prevent the stalling of CLI scripts during
                builds.</entry>
              </row>

              <row>
                <entry>checkschema</entry>

                <entry>Usage: checkschema</entry>

                <entry>Runs the schema consistency check for the database and
                benchmark selected with dbset and variables selected with
                diset. Equivalent to the Check command in the graphical
                interface.</entry>
              </row>

              <row>
                <entry>deleteschema</entry>

                <entry>Usage: deleteschema</entry>

                <entry>Runs the schema delete for the database and benchmark
                selected with dbset and variables selected with diset.
                Equivalent to the Delete command in the graphical
                interface.</entry>
              </row>

              <row>
                <entry>clearscript</entry>

                <entry>Usage: clearscript</entry>

                <entry>Clears the script. Equivalent to the "Clear the Screen"
                button in the graphical interface.</entry>
              </row>

              <row>
                <entry>savescript</entry>

                <entry>Usage: savescript</entry>

                <entry>Save the script to a file. Equivalent to the "Save"
                button in the graphical interface.</entry>
              </row>

              <row>
                <entry>customscript</entry>

                <entry>Usage: customscript scriptname.tcl</entry>

                <entry>Load an external script. Equivalent to the "Open
                Existing File" button in the graphical interface.</entry>
              </row>

              <row>
                <entry>custommonitor</entry>

                <entry>Usage: custommonitor test|timed</entry>

                <entry>Causes an additional Virtual User to be created when
                running vucreate. Used when loading a custom script.</entry>
              </row>

              <row>
                <entry>datagenrun</entry>

                <entry>Usage: datagenrun</entry>

                <entry>Run Data Generation. Equivalent to the Generate option
                in the graphical interface.</entry>
              </row>

              <row>
                <entry>dbset</entry>

                <entry>Usage: dbset [db|bm] value</entry>

                <entry>Sets the database (db) or benchmark (bm). Equivalent to
                the Benchmark Menu in the graphical interface. Database value
                is set by the database prefix in the XML
                configuration.</entry>
              </row>

              <row>
                <entry>dgset</entry>

                <entry>Usage: dgset [vu|ware|directory]</entry>

                <entry>Set the Datagen options. Equivalent to the Datagen
                Options dialog in the graphical interface.</entry>
              </row>

              <row>
                <entry>diset</entry>

                <entry>Usage: diset dict key value</entry>

                <entry>Set the dictionary variables for the current database.
                Equivalent to the Schema Build and Driver Options windows in
                the graphical interface. Use "print dict" to see what these
                variables area and diset to change: Example: hammerdb&gt;diset
                tpcc count_ware 10 Changed tpcc:count_ware from 1 to 10 for
                Oracle.</entry>
              </row>

              <row>
                <entry>distributescript</entry>

                <entry>Usage: distributescript</entry>

                <entry>In Primary mode distributes the script loaded by
                Primary to the connected Replicas.</entry>
              </row>

              <row>
                <entry>giset</entry>

                <entry>Usage: giset dict key value</entry>

                <entry>Set the dictionary variables for the generic settings.
                Use "print generic" to see what these variables are and giset
                to change Example: hammerdb&gt;giset commandline
                keepalive_margin 60 Changed commandline:keepalive_margin from
                10 to 60 for generic</entry>
              </row>

              <row>
                <entry>jobs (alias job)</entry>

                <entry>Usage: jobs : jobs [jobid|joblist|result|timestamp] :
                jobs format [ text | JSON ] : jobs disable [ 0 | 1 ] : jobs
                jobid
                [bm|db|delete|dict|result|status|tcount|timestamp|timing|vuid]
                : jobs jobid timing vuid</entry>

                <entry>The jobs command can be used to enable or disable jobs
                functionality. If enabled it records the output,
                configuration, result, timing and transactions of a workload
                for retrieval at a later point identified by a jobid.</entry>
              </row>

              <row>
                <entry>librarycheck</entry>

                <entry>Usage: librarycheck</entry>

                <entry>Attempts to load the vendor provided 3rd party library
                for all databases and reports whether the attempt was
                successful.</entry>
              </row>

              <row>
                <entry>loadscript</entry>

                <entry>Usage: loadscript</entry>

                <entry>Load the script for the database and benchmark set with
                dbset and the dictionary variables set with diset. Use "print
                script" to see the script that is loaded. Equivalent to
                loading a Driver Script in the Script Editor window in the
                graphical interface.</entry>
              </row>

              <row>
                <entry>print</entry>

                <entry>Usage: print [db|bm|dict|generic|script|vuconf
                |vucreated|vustatus|datagen]</entry>

                <entry>prints the current configuration: db: database bm:
                benchmark dict: the dictionary for the current database ie all
                active variables generic: the dictionary for generic settings
                script: the loaded script vuconf: the virtual user
                configuration vucreated: the number of virtual users created
                vustatus: the status of the virtual users datagen : the
                configuration to build when datagen is run</entry>
              </row>

              <row>
                <entry>quit</entry>

                <entry>Usage: quit Shuts down the HammerDB CLI.</entry>

                <entry>Calls the exit command and terminates the CLI
                interface</entry>
              </row>

              <row>
                <entry>steprun</entry>

                <entry>Usage: steprun</entry>

                <entry>Automatically switches into Primary mode, creates and
                connects the multiple Replicas defined in config/steps.xml and
                starts the Primary and Replica Virtual Users at the defined
                intervals creating a step workload. Both Primary and Replicas
                will exit on completion.</entry>
              </row>

              <row>
                <entry>switchmode</entry>

                <entry>Usage: switchmode [mode] ?PrimaryID?
                ?PrimaryHostname?</entry>

                <entry>Changes the remote mode to Primary, Replica or Local.
                When Master it will report an id and a hostname. Equivalent to
                the Mode option in the graphical interface. Mode to switch to
                must be one of Local, Primary or Replica. If Mode is Replica
                then the ID and Hostname of the Primary to connect to must be
                given.</entry>
              </row>

              <row>
                <entry>tcset</entry>

                <entry>Usage: tcset
                [refreshrate|logtotemp|unique|timestamps]</entry>

                <entry>Configure the transaction counter options. Equivalent
                to the Transaction Counter Options window in the graphical
                interface.</entry>
              </row>

              <row>
                <entry>tcstart</entry>

                <entry>Usage: tcstart</entry>

                <entry>Starts the Transaction Counter.</entry>
              </row>

              <row>
                <entry>tcstatus</entry>

                <entry>Usage: tcstatus</entry>

                <entry>Checks the status of the Transaction Counter.</entry>
              </row>

              <row>
                <entry>tcstop</entry>

                <entry>Usage: tcstop</entry>

                <entry>Stops the Transaction Counter.</entry>
              </row>

              <row>
                <entry>vucomplete</entry>

                <entry>Usage: vucomplete</entry>

                <entry>Returns "true" or "false" depending on whether all
                virtual users that started a workload have completed
                regardless of whether the status was "FINISH SUCCESS" or
                "FINISH FAILED".</entry>
              </row>

              <row>
                <entry>vucreate</entry>

                <entry>Usage: vucreate</entry>

                <entry>Create the virtual users. Equivalent to the Virtual
                User Create option in the graphical interface. Use "print
                vucreated" to see the number created, vustatus to see the
                status and vucomplete to see whether all active virtual users
                have finished the workload. A script must be loaded before
                virtual users can be created.</entry>
              </row>

              <row>
                <entry>vudestroy</entry>

                <entry>Usage: vudestroy</entry>

                <entry>Destroy the virtual users. Equivalent to the Destroy
                Virtual Users button in the graphical interface that replaces
                the Create Virtual Users button after virtual user
                creation.</entry>
              </row>

              <row>
                <entry>vurun</entry>

                <entry>Usage: vurun</entry>

                <entry>Send the loaded script to the created virtual users for
                execution. Equivalent to the Run command in the graphical
                interface. In the CLI returns a jobid to access the workloads
                output and configuration.</entry>
              </row>

              <row>
                <entry>vuset</entry>

                <entry>Usage: vuset [vu|delay|repeat|iterations|showoutput
                |logtotemp|unique|nobuff|timestamps</entry>

                <entry>Configure the virtual user options. Equivalent to the
                Virtual User Options window in the graphical interface. vuset
                expects an integer or the string "vcpu". Where "vcpu" is
                specified the vu value will be set to the number of logical
                CPUs detected on the system where HammerDB is running.</entry>
              </row>

              <row>
                <entry>vustatus</entry>

                <entry>Usage: vustatus</entry>

                <entry>Show the status of virtual users. Status will be "WAIT
                IDLE" for virtual users that are created but not running a
                workload,"RUNNING" for virtual users that are running a
                workload, "FINISH SUCCESS" for virtual users that completed
                successfully or "FINISH FAILED" for virtual users that
                encountered an error.</entry>
              </row>

              <row>
                <entry>wsport</entry>

                <entry>Usage: wsport [ port number ]</entry>

                <entry>Set or report the Web Service Port.</entry>
              </row>

              <row>
                <entry>wsstart</entry>

                <entry>Usage: wsstart</entry>

                <entry>Start the Web Service.</entry>
              </row>

              <row>
                <entry>wsstop</entry>

                <entry>Usage: wsstop</entry>

                <entry>Stops the Web Service.</entry>
              </row>

              <row>
                <entry>wsstatus</entry>

                <entry>Usage: wsstatus</entry>

                <entry>Checks the status of the Web Service.</entry>
              </row>
            </tbody>
          </tgroup>
        </table></para>

      <para>Note that the runtimer and waittocomplete parameters have been
      deprecated from v4.6. For this reason an additional configuration
      parameter of keepalive_margin with a default value of 10 seconds
      increasing to 60 seconds from v4.10 has been added to generic.xml in the
      commandline section to modify the additional time that HammerDB will
      wait after completion before terminating the workload. This can be
      useful if for example gathering timing data for event driven scaling
      workloads with a large number of asynchronous clients.</para>

      <section>
        <title>CLI commands in Python</title>

        <para>The HammerDB commands in Python are identical to the ones in the
        default Tcl mode except accessed as Python functions with either
        string or integer arguments.</para>

        <programlisting>C:\Program Files\HammerDB-4.6&gt;hammerdbcli py
HammerDB CLI v4.6
Copyright (C) 2003-2022 Steve Shaw
Type "help()" for a list of commands
hammerdb&gt;&gt;&gt;help()
HammerDB v4.6 CLI Help Index

Type "help command" for more details on specific commands below

        buildschema
        deleteschema
        clearscript
        customscript
        datagenrun
        dbset
        dgset
        diset
        distributescript
        jobs
        librarycheck
        loadscript
        print
        quit
        steprun
        switchmode
        tcset
        tcstart
        tcstatus
        tcstop
        vucomplete
        vucreate
        vudestroy
        vurun
        vuset
        vustatus

hammerdb&gt;&gt;&gt;vuset('vu',10)
hammerdb&gt;&gt;&gt;

hammerdb&gt;&gt;&gt;print('vuconf')
Virtual Users = 10
User Delay(ms) = 500
Repeat Delay(ms) = 500
Iterations = 1
Show Output = 1
Log Output = 0
Unique Log Name = 0
No Log Buffer = 0
Log Timestamps = 0

hammerdb&gt;&gt;&gt;</programlisting>
      </section>
    </section>

    <section>
      <title>Configure Schema Build</title>

      <para>Use the dbset command to choose a database and benchmark. For the
      database the database prefix shown in the XML configuration is used. IIf
      an incorrect database is selected the available values are
      prompted.</para>

      <programlisting>hammerdb&gt;dbset db orac
Unknown prefix orac, choose one from ora mssqls db2 mysql pg</programlisting>

      <para>When a valid option is chosen the database is set.</para>

      <programlisting>hammerdb&gt;dbset db mssqls
Database set to MSSQLServer</programlisting>

      <para>The print command can be used to confirm the chosen database and
      available options.</para>

      <programlisting>hammerdb&gt;print db
Database MSSQLServer set.
To change do: dbset db prefix, one of:
Oracle = ora MSSQLServer = mssqls Db2 = db2 MySQL = mysql PostgreSQL = pg </programlisting>

      <para>Similarly the workload is also selected from the available
      configuration also prompting if an incorrect value is chosen. When a
      correct value is chosen the selection is confirmed. For backward
      compatibility with existing scripts TPROC-C and TPC-C and TPROC-H and
      TPC-H are interchangeable.</para>

      <programlisting>hammerdb&gt;dbset bm TPROC-H
Benchmark set to TPROC-H for MSSQLServer

hammerdb&gt;dbset bm TPC-C
Benchmark set to TPC-C for MSSQLServer

hammerdb&gt;print bm
Benchmark set to TPC-C</programlisting>

      <para>After the database and workload is selected the print dict command
      lists all of the available configuration variables for that
      database.</para>

      <para><programlisting>hammerdb&gt;print dict
Dictionary Settings for MSSQLServer
connection {
 mssqls_server         = (local)
 mssqls_linux_server   = localhost
 mssqls_tcp            = false
 mssqls_port           = 1433
 mssqls_azure          = false
 mssqls_authentication = windows
 mssqls_linux_authent  = sql
 mssqls_odbc_driver    = ODBC Driver 17 for SQL Server
 mssqls_linux_odbc     = ODBC Driver 17 for SQL Server
 mssqls_uid            = sa
 mssqls_pass           = admin
}
tpcc       {
 mssqls_count_ware       = 1
 mssqls_num_vu           = 1
 mssqls_dbase            = tpcc
 mssqls_imdb             = false
 mssqls_bucket           = 1
 mssqls_durability       = SCHEMA_AND_DATA
 mssqls_total_iterations = 1000000
 mssqls_raiseerror       = false
 mssqls_keyandthink      = false
 mssqls_checkpoint       = false
 mssqls_driver           = test
 mssqls_rampup           = 2
 mssqls_duration         = 5
 mssqls_allwarehouse     = false
 mssqls_timeprofile      = false
 mssqls_async_scale      = false
 mssqls_async_client     = 10
 mssqls_async_verbose    = false
 mssqls_async_delay      = 1000
 mssqls_connect_pool     = false
}
</programlisting>Use the diset command to change these values for example for
      the number of warehouses to build.</para>

      <programlisting>hammerdb&gt;diset tpcc mssqls_count_ware 10
Changed tpcc:mssqls_count_ware from 1 to 10 for MSSQLServer</programlisting>

      <para>and the number of virtual users to build them.</para>

      <programlisting>hammerdb&gt;diset tpcc mssqls_num_vu 4
Changed tpcc:mssqls_num_vu from 1 to 4 for MSSQLServer</programlisting>

      <para>If the dict value to be set has a special character using curly
      brackets around the value will prevent the interpretation of the special
      character.</para>

      <programlisting>hammerdb&gt;diset connection mssqls_server {(local)\SQLDEVELOP}
Changed connection:mssqls_server from (local) to (local)\SQLDEVELOP for MSSQLServer</programlisting>

      <para>print dict will show the changed values.</para>

      <programlisting>hammerdb&gt;print dict
Dictionary Settings for MSSQLServer
connection {
 mssqls_server         = (local)\SQLDEVELOP
 mssqls_linux_server   = localhost
 mssqls_tcp            = false
 mssqls_port           = 1433
 mssqls_azure          = false
 mssqls_authentication = windows
 mssqls_linux_authent  = sql
 mssqls_odbc_driver    = ODBC Driver 17 for SQL Server
 mssqls_linux_odbc     = ODBC Driver 17 for SQL Server
 mssqls_uid            = sa
 mssqls_pass           = admin
}
tpcc       {
 mssqls_count_ware       = 10
 mssqls_num_vu           = 4
 mssqls_dbase            = tpcc
 mssqls_imdb             = false
 mssqls_bucket           = 1
 mssqls_durability       = SCHEMA_AND_DATA
 mssqls_total_iterations = 1000000
 mssqls_raiseerror       = false
 mssqls_keyandthink      = false
 mssqls_checkpoint       = false
 mssqls_driver           = test
 mssqls_rampup           = 2
 mssqls_duration         = 5
 mssqls_allwarehouse     = false
 mssqls_timeprofile      = false
 mssqls_async_scale      = false
 mssqls_async_client     = 10
 mssqls_async_verbose    = false
 mssqls_async_delay      = 1000
 mssqls_connect_pool     = false
}</programlisting>
    </section>

    <section>
      <title>Building the Schema</title>

      <para>Run the buildschema command and the build will commence without
      prompting using your configuration and if successful report the status
      at the end of the build. Note that exactly as the GUI the build is
      multithreaded with Virtual Users running simultaneously.</para>

      <programlisting>hhammerdb&gt;buildschema
Script cleared
Building 10 Warehouses with 5 Virtual Users, 4 active + 1 Monitor VU(dict value mssqls_num_vu is set to 4)
Ready to create a 10 Warehouse MS SQL Server TPROC-C schema
in host (LOCAL)\SQLDEVELOP in database TPCC?
Enter yes or no: replied yes
Vuser 1 created - WAIT IDLE
Vuser 2 created - WAIT IDLE
Vuser 3 created - WAIT IDLE
Vuser 4 created - WAIT IDLE
Vuser 5 created - WAIT IDLE
Vuser 1:RUNNING
Vuser 1:Monitor Thread
Vuser 1:CREATING TPCC SCHEMA
Vuser 1:CHECKING IF DATABASE tpcc EXISTS
Vuser 1:CREATING DATABASE tpcc
Vuser 1:CREATING TPCC TABLES
Vuser 1:Loading Item
Vuser 2:RUNNING
Vuser 2:Worker Thread
Vuser 2:Waiting for Monitor Thread...
Vuser 2:Loading 2 Warehouses start:1 end:2
Vuser 2:Start:Thu Oct 22 17:56:27 BST 2020
Vuser 2:Loading Warehouse
Vuser 2:Loading Stock Wid=1
Vuser 3:RUNNING
Vuser 3:Worker Thread
Vuser 3:Waiting for Monitor Thread...
Vuser 3:Loading 2 Warehouses start:3 end:4
Vuser 3:Start:Thu Oct 22 17:56:27 BST 2020
Vuser 3:Loading Warehouse
Vuser 3:Loading Stock Wid=3
Vuser 4:RUNNING
Vuser 4:Worker Thread
Vuser 4:Waiting for Monitor Thread...
Vuser 4:Loading 2 Warehouses start:5 end:6
Vuser 4:Start:Thu Oct 22 17:56:28 BST 2020
Vuser 4:Loading Warehouse
Vuser 4:Loading Stock Wid=5
Vuser 5:RUNNING
Vuser 5:Worker Thread
Vuser 5:Waiting for Monitor Thread...
Vuser 5:Loading 2 Warehouses start:7 end:10
Vuser 5:Start:Thu Oct 22 17:56:28 BST 2020
Vuser 5:Loading Warehouse
Vuser 5:Loading Stock Wid=7

.....

Vuser 5:Loading Orders for D=10 W=10
Vuser 5:...1000
Vuser 5:...2000
Vuser 5:...3000
Vuser 5:Orders Done
Vuser 5:End:Thu Oct 22 18:02:45 BST 2020
Vuser 5:FINISHED SUCCESS
Vuser 1:Workers: 0 Active 4 Done
Vuser 1:CREATING TPCC INDEXES
Vuser 1:CREATING TPCC STORED PROCEDURES
Vuser 1:UPDATING SCHEMA STATISTICS
Vuser 1:TPCC SCHEMA COMPLETE
Vuser 1:FINISHED SUCCESS
ALL VIRTUAL USERS COMPLETE

hammerdb&gt;
</programlisting>

      <para>The vustatus command can confirm the status of each Virtual
      User.</para>

      <programlisting>hammerdb&gt;vustatus
1 = FINISH SUCCESS
2 = FINISH SUCCESS
3 = FINISH SUCCESS
4 = FINISH SUCCESS
5 = FINISH SUCCESS</programlisting>

      <para>When the build is complete destroy the Virtual Users and confirm
      the status.</para>

      <programlisting>hammerdb&gt;vudestroy
Destroying Virtual Users
Virtual Users Destroyed

hammerdb&gt;vustatus
No Virtual Users found</programlisting>
    </section>

    <section>
      <title>Configure Driver</title>

      <para>Set the type of workload to run. A timed workload with suppressed
      output is strongly recommended as a test workload will print
      considerable output to the command prompt.</para>

      <programlisting>hammerdb&gt;diset tpcc mssqls_driver timed
Clearing Script, reload script to activate new setting
Script cleared
Changed tpcc:mssqls_driver from test to timed for MSSQLServer</programlisting>

      <para>Configure workload settings, in this example the rampup and
      duration times are set.</para>

      <programlisting>hammerdb&gt;diset tpcc mssqls_rampup 1
Changed tpcc:mssqls_rampup from 2 to 1 for MSSQLServer

hammerdb&gt;diset tpcc mssqls_duration 3
Changed tpcc:mssqls_duration from 5 to 3 for MSSQLServer</programlisting>

      <para>Confirm the settings with the print dict command.</para>

      <programlisting>hammerdb&gt;print dict
Dictionary Settings for MSSQLServer
connection {
 mssqls_server         = (local)\SQLDEVELOP
 mssqls_linux_server   = localhost
 mssqls_tcp            = false
 mssqls_port           = 1433
 mssqls_azure          = false
 mssqls_authentication = windows
 mssqls_linux_authent  = sql
 mssqls_odbc_driver    = ODBC Driver 17 for SQL Server
 mssqls_linux_odbc     = ODBC Driver 17 for SQL Server
 mssqls_uid            = sa
 mssqls_pass           = admin
}
tpcc       {
 mssqls_count_ware       = 10
 mssqls_num_vu           = 4
 mssqls_dbase            = tpcc
 mssqls_imdb             = false
 mssqls_bucket           = 1
 mssqls_durability       = SCHEMA_AND_DATA
 mssqls_total_iterations = 1000000
 mssqls_raiseerror       = false
 mssqls_keyandthink      = false
 mssqls_checkpoint       = false
 mssqls_driver           = timed
 mssqls_rampup           = 1
 mssqls_duration         = 3
 mssqls_allwarehouse     = false
 mssqls_timeprofile      = false
 mssqls_async_scale      = false
 mssqls_async_client     = 10
 mssqls_async_verbose    = false
 mssqls_async_delay      = 1000
 mssqls_connect_pool     = false
}</programlisting>

      <para>When all the settings have been chosen load the driver script with
      the loadscript command.</para>

      <programlisting>hammerdb&gt;loadscript
Script loaded, Type "print script" to view</programlisting>

      <para>The loaded script can be viewed with the print script command.
      Note that the driver script is exactly the same as the driver script
      observed in the GUI. There is no difference whatsoever in what is run in
      the CLI compared to the GUI. If there is a wish to change the script a
      modified version can be loaded with the customscript command and it is
      therefore recommended to use the GUI to save a version of the script to
      modify.</para>

      <programlisting>#!/usr/local/bin/tclsh8.6
#EDITABLE OPTIONS##################################################
set library tdbc::odbc ;# SQL Server Library
set version 1.1.1 ;# SQL Server Library Version
set total_iterations 1000000;# Number of transactions before logging off
set RAISEERROR "false" ;# Exit script on SQL Server error (true or false)
set KEYANDTHINK "false" ;# Time for user thinking and keying (true or false)
set CHECKPOINT "false" ;# Perform SQL Server checkpoint when complete (true or false)
set rampup 1;  # Rampup time in minutes before first Transaction Count is taken
set duration 3;  # Duration in minutes before second Transaction Count is taken
set mode "Local" ;# HammerDB operational mode
set authentication "windows";# Authentication Mode (WINDOWS or SQL)
set server {(local)\SQLDEVELOP1};# Microsoft SQL Server Database Server
set port "1433";# Microsoft SQL Server Port 
set odbc_driver {ODBC Driver 17 for SQL Server};# ODBC Driver
set uid "sa";#User ID for SQL Server Authentication
set pwd "admin";#Password for SQL Server Authentication
set tcp "false";#Specify TCP Protocol
set azure "false";#Azure Type Connection
set database "tpcc";# Database containing the TPC Schema
#EDITABLE OPTIONS##################################################
...</programlisting>
    </section>

    <section>
      <title>Configure Virtual Users</title>

      <para>With the schema built and the driver script loaded the next step
      in the workflow is to configure the Virtual Users.</para>

      <para>The print command can be used to show the number of Virtual Users
      currently created. As the Virtual Users were destroyed after the build
      it is reported that none are created.</para>

      <programlisting>hammerdb&gt;print vucreated
0 Virtual Users created</programlisting>

      <para>The vuset command is used to configure the Virtual User options
      either by setting an integer or the value vcpu which will set vu to the
      maximum the logical CPUs on the system, for example the number of
      Virtual Users to create.</para>

      <programlisting>hammerdb&gt;vuset vu 4</programlisting>

      <para>and to enable logging.</para>

      <programlisting>hammerdb&gt;vuset logtotemp 1</programlisting>

      <para>print vuconf confirms the configuration.</para>

      <programlisting>hammerdb&gt;print vuconf
Virtual Users = 4
User Delay(ms) = 500
Repeat Delay(ms) = 500
Iterations = 1
Show Output = 1
Log Output = 1
Unique Log Name = 0
No Log Buffer = 0
Log Timestamps = 0
</programlisting>

      <para>Then run vucreate to create the Virtual Users who will be created
      in an idle state not yet running. Note that when a timed test is
      selected a Monitor Virtual User is also created as is the case with the
      graphical interface.</para>

      <programlisting>hammerdb&gt;vucreate
Vuser 1 created MONITOR - WAIT IDLE
Vuser 2 created - WAIT IDLE
Vuser 3 created - WAIT IDLE
Vuser 4 created - WAIT IDLE
Vuser 5 created - WAIT IDLE
Logging activated
to C:/Users/Steve/AppData/Local/Temp/hammerdb.log
5 Virtual Users Created with Monitor VU</programlisting>

      <para>vustatus can confirm this status.</para>

      <programlisting>hammerdb&gt;vustatus
1 = WAIT IDLE
2 = WAIT IDLE
3 = WAIT IDLE
4 = WAIT IDLE
5 = WAIT IDLE</programlisting>
    </section>

    <section>
      <title>Run the workload</title>

      <para>To begin the workload type vurun.</para>

      <programlisting>hammerdb&gt;vurun
Vuser 1:RUNNING
Vuser 1:Beginning rampup time of 1 minutes
Vuser 2:RUNNING
Vuser 2:Processing 1000000 transactions with output suppressed...
Vuser 3:RUNNING
Vuser 3:Processing 1000000 transactions with output suppressed...
Vuser 4:RUNNING
Vuser 4:Processing 1000000 transactions with output suppressed...
Vuser 5:RUNNING
Vuser 5:Processing 1000000 transactions with output suppressed...</programlisting>

      <para>The vustatus command can confirm the change in status.</para>

      <programlisting>hammerdb&gt;vustatus

1 = RUNNING
2 = RUNNING
3 = RUNNING
4 = RUNNING
5 = RUNNING</programlisting>

      <para>The vucomplete command returns a boolean value to confirm whether
      an entire workload is still running or finished.</para>

      <para><programlisting>hammerdb&gt;vucomplete
false</programlisting>The test runs as per the configuration and reports the
      result at the end and the Virtual User status. Note that when complete
      the vucomplete command can confirm this.</para>

      <programlisting>hammerdb&gt;Vuser 1:Rampup 1 minutes complete ...
Vuser 1:Rampup complete, Taking start Transaction Count.
Vuser 1:Timing test period of 3 in minutes
Vuser 1:1 ...,
Vuser 1:2 ...,
Vuser 1:3 ...,
Vuser 1:Test complete, Taking end Transaction Count.
Vuser 1:4 Active Virtual Users configured
Vuser 1:TEST RESULT : System achieved 101005 NOPM from 232149 SQL Server TPM
Vuser 1:FINISHED SUCCESS
Vuser 5:FINISHED SUCCESS
Vuser 4:FINISHED SUCCESS
Vuser 3:FINISHED SUCCESS
Vuser 2:FINISHED SUCCESS
ALL VIRTUAL USERS COMPLETE

hammerdb&gt;vucomplete
true
hammerdb&gt;</programlisting>

      <para>To complete the test type vudestroy.</para>

      <programlisting>hammerdb&gt;vudestroy
Destroying Virtual Users
Virtual Users Destroyed
</programlisting>

      <para>and clear the script.</para>

      <programlisting>hammerdb&gt;clearscript
Script cleared</programlisting>
    </section>

    <section>
      <title>CLI Scripting in Tcl</title>

      <para>The CLI enables a powerful automated test environment through
      scripting in the TCL language. A recommended updated guide to TCL is
      "The Tcl Programming Language: A Comprehensive Guide by Ashok P.
      Nadkarni (ISBN: 9781548679644)"</para>

      <para>The following example shows an automated test script for a
      Microsoft SQL Server database that has previously been created. In this
      example the script runs a timed tests for a duration of a minute for 1,
      2 and 4 Virtual Users in a similar manner to autopilot functionality
      with a timer set to run for 2 minutes. Note that from HammerDB v4.6 the
      previous runtimer command has been deprecated.</para>

      <programlisting>#!/usr/bin/tclsh
puts "SETTING CONFIGURATION"
dbset db mssqls
diset tpcc mssqls_driver timed
diset tpcc mssqls_rampup 0
diset tpcc mssqls_duration 1
vuset logtotemp 1
loadscript
puts "SEQUENCE STARTED"
foreach z { 1 2 4 } {
puts "$z VU TEST"
vuset vu $z
vucreate
vurun
vudestroy
}
puts "TEST SEQUENCE COMPLETE"
</programlisting>

      <para>Run the hammerdbcli command and at the prompt type source and the
      name of the script. The following output is produced without further
      intervention whilst also writing the output to the logfile.</para>

      <programlisting>C:\Program Files\HammerDB-4.6&gt;hammerdbcli
HammerDB CLI v4.6
Copyright (C) 2003-2022 Steve Shaw
Type "help" for a list of commands
Initialized SQLite on-disk database C:/Users/Hdb/AppData/Local/Temp/hammer.DB using existing tables (36,864 KB)
hammerdb&gt;source cliexample.tcl
SETTING CONFIGURATION
Database set to MSSQLServer
Value timed for tpcc:mssqls_driver is the same as existing value timed, no change made
Value 0 for tpcc:mssqls_rampup is the same as existing value 0, no change made
Value 1 for tpcc:mssqls_duration is the same as existing value 1, no change made
Script loaded, Type "print script" to view
SEQUENCE STARTED
1 VU TEST
Vuser 1 created MONITOR - WAIT IDLE
Vuser 2 created - WAIT IDLE
Logging activated
to C:/Users/Hdb/AppData/Local/Temp/hammerdb.log
2 Virtual Users Created with Monitor VU
Vuser 1:RUNNING
Vuser 1:Beginning rampup time of 0 minutes
Vuser 1:Rampup complete, Taking start Transaction Count.
Vuser 1:Timing test period of 1 in minutes
Vuser 2:RUNNING
Vuser 2:Processing 10000000 transactions with output suppressed...
Vuser 1:1 ...,
Vuser 1:Test complete, Taking end Transaction Count.
Vuser 1:1 Active Virtual Users configured
Vuser 1:TEST RESULT : System achieved 44932 NOPM from 104216 SQL Server TPM
Vuser 1:FINISHED SUCCESS
Vuser 2:FINISHED SUCCESS
ALL VIRTUAL USERS COMPLETE
vudestroy success
2 VU TEST
Vuser 1 created MONITOR - WAIT IDLE
Vuser 2 created - WAIT IDLE
Vuser 3 created - WAIT IDLE
Logging activated
to C:/Users/Hdb/AppData/Local/Temp/hammerdb.log
3 Virtual Users Created with Monitor VU
Vuser 1:RUNNING
Vuser 1:Beginning rampup time of 0 minutes
Vuser 1:Rampup complete, Taking start Transaction Count.
Vuser 1:Timing test period of 1 in minutes
Vuser 2:RUNNING
Vuser 2:Processing 10000000 transactions with output suppressed...
Vuser 3:RUNNING
Vuser 3:Processing 10000000 transactions with output suppressed...
Vuser 1:1 ...,
Vuser 1:Test complete, Taking end Transaction Count.
Vuser 1:2 Active Virtual Users configured
Vuser 1:TEST RESULT : System achieved 68677 NOPM from 159282 SQL Server TPM
Vuser 1:FINISHED SUCCESS
Vuser 2:FINISHED SUCCESS
Vuser 3:FINISHED SUCCESS
ALL VIRTUAL USERS COMPLETE
vudestroy success
4 VU TEST
Vuser 1 created MONITOR - WAIT IDLE
Vuser 2 created - WAIT IDLE
Vuser 3 created - WAIT IDLE
Vuser 4 created - WAIT IDLE
Vuser 5 created - WAIT IDLE
Logging activated
to C:/Users/Hdb/AppData/Local/Temp/hammerdb.log
5 Virtual Users Created with Monitor VU
Vuser 1:RUNNING
Vuser 1:Beginning rampup time of 0 minutes
Vuser 1:Rampup complete, Taking start Transaction Count.
Vuser 1:Timing test period of 1 in minutes
Vuser 2:RUNNING
Vuser 2:Processing 10000000 transactions with output suppressed...
Vuser 3:RUNNING
Vuser 3:Processing 10000000 transactions with output suppressed...
Vuser 4:RUNNING
Vuser 4:Processing 10000000 transactions with output suppressed...
Vuser 5:RUNNING
Vuser 5:Processing 10000000 transactions with output suppressed...
Vuser 1:1 ...,
Vuser 1:Test complete, Taking end Transaction Count.
Vuser 1:4 Active Virtual Users configured
Vuser 1:TEST RESULT : System achieved 132905 NOPM from 309493 SQL Server TPM
Vuser 1:FINISHED SUCCESS
Vuser 4:FINISHED SUCCESS
Vuser 2:FINISHED SUCCESS
Vuser 5:FINISHED SUCCESS
Vuser 3:FINISHED SUCCESS
ALL VIRTUAL USERS COMPLETE
vudestroy success
TEST SEQUENCE COMPLETE

hammerdb&gt;</programlisting>

      <para>It is a common requirement to also want to drive HammerDB CLI
      scripts from an external scripting tool. The HammerDB CLI will accept
      the argument auto to run a specified script automatically by default
      expecting a filename with a .tcl extension and Tcl syntax.</para>

      <programlisting>hammerdbcli.bat auto autorunbuild.tcl</programlisting>
    </section>

    <section>
      <title>CLI Scripting in Python</title>

      <para>When run in Python mode with the py or python argument the
      HammerDB CLI will accept syntax and commands in Python format with the
      equivalent script as previous now written as follows:</para>

      <programlisting>print("SETTING CONFIGURATION")
dbset('db','mssqls')
diset('tpcc','mssqls_driver','timed')
diset('tpcc','mssqls_rampup',0)
diset('tpcc','mssqls_duration',1)
vuset('logtotemp',1)
loadscript()
print("SEQUENCE STARTED")
for z in [1,2,4]:
 print(z," VU TEST")
 vuset('vu',z)
 vucreate()
 vurun()
 vudestroy()

print("TEST SEQUENCE COMPLETE")

</programlisting>

      <para>and run with output as follows:</para>

      <programlisting>C:\Program Files\HammerDB-4.6&gt;hammerdbcli py
HammerDB CLI v4.6
Copyright (C) 2003-2022 Steve Shaw
Type "help()" for a list of commands
hammerdb&gt;&gt;&gt;source('cliexample.py')
hammerdb&gt;&gt;&gt;SETTING CONFIGURATION
Database set to MSSQLServer
Value timed for tpcc:mssqls_driver is the same as existing value timed, no change made
Value 0 for tpcc:mssqls_rampup is the same as existing value 0, no change made
Value 1 for tpcc:mssqls_duration is the same as existing value 1, no change made
Script loaded, Type "print script" to view
SEQUENCE STARTED
1  VU TEST
Vuser 1 created MONITOR - WAIT IDLE
Vuser 2 created - WAIT IDLE
Logging activated
to C:/Users/Hdb/AppData/Local/Temp/hammerdb.log
2 Virtual Users Created with Monitor VU
Vuser 1:RUNNING
Vuser 1:Beginning rampup time of 0 minutes
Vuser 1:Rampup complete, Taking start Transaction Count.
Vuser 1:Timing test period of 1 in minutes
Vuser 2:RUNNING
Vuser 2:Processing 10000000 transactions with output suppressed...
Vuser 1:1 ...,
Vuser 1:Test complete, Taking end Transaction Count.
Vuser 1:1 Active Virtual Users configured
Vuser 1:TEST RESULT : System achieved 46171 NOPM from 107258 SQL Server TPM
Vuser 1:FINISHED SUCCESS
Vuser 2:FINISHED SUCCESS
ALL VIRTUAL USERS COMPLETE
vudestroy success
2  VU TEST
Vuser 1 created MONITOR - WAIT IDLE
Vuser 2 created - WAIT IDLE
Vuser 3 created - WAIT IDLE
Logging activated
to C:/Users/Hdb/AppData/Local/Temp/hammerdb.log
3 Virtual Users Created with Monitor VU
Vuser 1:RUNNING
Vuser 1:Beginning rampup time of 0 minutes
Vuser 1:Rampup complete, Taking start Transaction Count.
Vuser 1:Timing test period of 1 in minutes
Vuser 2:RUNNING
Vuser 2:Processing 10000000 transactions with output suppressed...
Vuser 3:RUNNING
Vuser 3:Processing 10000000 transactions with output suppressed...
Vuser 1:1 ...,
Vuser 1:Test complete, Taking end Transaction Count.
Vuser 1:2 Active Virtual Users configured
Vuser 1:TEST RESULT : System achieved 77825 NOPM from 180859 SQL Server TPM
Vuser 1:FINISHED SUCCESS
Vuser 3:FINISHED SUCCESS
Vuser 2:FINISHED SUCCESS
ALL VIRTUAL USERS COMPLETE
vudestroy success
4  VU TEST
Vuser 1 created MONITOR - WAIT IDLE
Vuser 2 created - WAIT IDLE
Vuser 3 created - WAIT IDLE
Vuser 4 created - WAIT IDLE
Vuser 5 created - WAIT IDLE
Logging activated
to C:/Users/Hdb/AppData/Local/Temp/hammerdb.log
5 Virtual Users Created with Monitor VU
Vuser 1:RUNNING
Vuser 1:Beginning rampup time of 0 minutes
Vuser 1:Rampup complete, Taking start Transaction Count.
Vuser 1:Timing test period of 1 in minutes
Vuser 2:RUNNING
Vuser 2:Processing 10000000 transactions with output suppressed...
Vuser 3:RUNNING
Vuser 3:Processing 10000000 transactions with output suppressed...
Vuser 4:RUNNING
Vuser 4:Processing 10000000 transactions with output suppressed...
Vuser 5:RUNNING
Vuser 5:Processing 10000000 transactions with output suppressed...
Vuser 1:1 ...,
Vuser 1:Test complete, Taking end Transaction Count.
Vuser 1:4 Active Virtual Users configured
Vuser 1:TEST RESULT : System achieved 126528 NOPM from 293263 SQL Server TPM
Vuser 1:FINISHED SUCCESS
Vuser 2:FINISHED SUCCESS
Vuser 4:FINISHED SUCCESS
Vuser 5:FINISHED SUCCESS
Vuser 3:FINISHED SUCCESS
ALL VIRTUAL USERS COMPLETE
vudestroy success
TEST SEQUENCE COMPLETE
hammerdb&gt;&gt;&gt;</programlisting>

      <para>It is a common requirement to also want to drive HammerDB CLI
      scripts from an external scripting tool. The HammerDB CLI will accept
      the argument auto to run a specified script automatically in python mode
      expecting a python or py argument followed by a filename with a .py
      extension and Python syntax.</para>

      <programlisting>hammerdbcli.bat py auto autorunbuild.py</programlisting>
    </section>

    <section>
      <title>CLI Example Scripts driven by Bash and Powershell</title>

      <para>From HammerDB v4.7 the default HammerDB installation includes a
      scripts directory that contains example scripts for both TPROC-C and
      TPROC-H workloads for all databases in both Tcl and Python format to
      provide a template for updating and modifying scripts for an individual
      environment.</para>

      <para>At a minimum you should update the connection parameters in the
      scripts you wish to run for your environment. Additionally the default
      scripts will size the workload and run the number of Virtual Users
      according to the number of CPUs on the system where HammerDB is running
      and therefore if running the client on a separate system to the database
      then the number of virtual users and schema sizes should be modified
      accordingly.</para>

      <para>All target directories include driver scripts in powershell for
      Windows and bash for Linux to run an automated sequence of scripts to
      build, test, delete and query results for a workload. These driver
      scripts should be run with the full path to the script from the HammerDB
      home directory for example on Windows:</para>

      <programlisting>powershell scripts\python\mssqls\tproch\mssqls_tproch_py.ps1</programlisting>

      <para>and on Linux</para>

      <programlisting>./scripts/tcl/maria/tprocc/maria_tprocc.sh</programlisting>

      <para>Example output for MariaDB is shown as follows:</para>

      <programlisting>./scripts/tcl/maria/tprocc/maria_tprocc.sh
...
Vuser 4:Loading Orders for D=3 W=15
Vuser 5:...3000
Vuser 5:Orders Done
Vuser 5:Loading Orders for D=2 W=20
Vuser 3:...3000
Vuser 3:Orders Done
Vuser 3:Loading Orders for D=3 W=10
Vuser 2:...2000
Vuser 4:...1000
Vuser 2:...3000
Vuser 5:...1000
Vuser 2:Orders Done
Vuser 2:Loading Orders for D=4 W=5
Vuser 3:...1000
Vuser 4:...2000
Vuser 5:...2000
Vuser 2:...1000
Vuser 3:...2000
Vuser 4:...3000
Vuser 5:...3000
Vuser 4:Orders Done
Vuser 4:Loading Orders for D=4 W=15
Vuser 5:Orders Done
Vuser 5:Loading Orders for D=3 W=20
Vuser 3:...3000
Vuser 3:Orders Done
Vuser 2:...2000
Vuser 3:Loading Orders for D=4 W=10
Vuser 4:...1000
Vuser 2:...3000
Vuser 2:Orders Done
Vuser 2:Loading Orders for D=5 W=5
Vuser 5:...1000
Vuser 3:...1000
Vuser 2:...1000
Vuser 4:...2000
Vuser 5:...2000
Vuser 3:...2000
Vuser 2:...2000
Vuser 4:...3000
Vuser 4:Orders Done
Vuser 4:Loading Orders for D=5 W=15
Vuser 3:...3000
Vuser 5:...3000
Vuser 3:Orders Done
Vuser 5:Orders Done
Vuser 3:Loading Orders for D=5 W=10
Vuser 5:Loading Orders for D=4 W=20
Vuser 2:...3000
Vuser 2:Orders Done
Vuser 2:Loading Orders for D=6 W=5
Vuser 4:...1000
Vuser 5:...1000
Vuser 3:...1000
Vuser 2:...1000
Vuser 4:...2000
Vuser 3:...2000
Vuser 5:...2000
Vuser 4:...3000
Vuser 4:Orders Done
Vuser 4:Loading Orders for D=6 W=15
Vuser 2:...2000
Vuser 3:...3000
Vuser 5:...3000
Vuser 3:Orders Done
Vuser 5:Orders Done
Vuser 3:Loading Orders for D=6 W=10
Vuser 5:Loading Orders for D=5 W=20
Vuser 2:...3000
Vuser 2:Orders Done
Vuser 2:Loading Orders for D=7 W=5
Vuser 4:...1000
Vuser 5:...1000
Vuser 3:...1000
Vuser 2:...1000
Vuser 4:...2000
Vuser 5:...2000
Vuser 3:...2000
Vuser 4:...3000
Vuser 4:Orders Done
Vuser 4:Loading Orders for D=7 W=15
Vuser 2:...2000
Vuser 5:...3000
Vuser 5:Orders Done
Vuser 5:Loading Orders for D=6 W=20
Vuser 3:...3000
Vuser 3:Orders Done
Vuser 3:Loading Orders for D=7 W=10
Vuser 2:...3000
Vuser 4:...1000
Vuser 5:...1000
Vuser 2:Orders Done
Vuser 3:...1000
Vuser 2:Loading Orders for D=8 W=5
Vuser 4:...2000
Vuser 5:...2000
Vuser 4:...3000
Vuser 4:Orders Done
Vuser 4:Loading Orders for D=8 W=15
Vuser 2:...1000
Vuser 5:...3000
Vuser 3:...2000
Vuser 5:Orders Done
Vuser 5:Loading Orders for D=7 W=20
Vuser 3:...3000
Vuser 4:...1000
Vuser 3:Orders Done
Vuser 3:Loading Orders for D=8 W=10
Vuser 2:...2000
Vuser 5:...1000
Vuser 2:...3000
Vuser 2:Orders Done
Vuser 2:Loading Orders for D=9 W=5
Vuser 4:...2000
Vuser 3:...1000
Vuser 5:...2000
Vuser 4:...3000
Vuser 4:Orders Done
Vuser 4:Loading Orders for D=9 W=15
Vuser 5:...3000
Vuser 5:Orders Done
Vuser 5:Loading Orders for D=8 W=20
Vuser 3:...2000
Vuser 2:...1000
Vuser 4:...1000
Vuser 3:...3000
Vuser 5:...1000
Vuser 2:...2000
Vuser 3:Orders Done
Vuser 4:...2000
Vuser 3:Loading Orders for D=9 W=10
Vuser 5:...2000
Vuser 2:...3000
Vuser 2:Orders Done
Vuser 2:Loading Orders for D=10 W=5
Vuser 4:...3000
Vuser 4:Orders Done
Vuser 4:Loading Orders for D=10 W=15
Vuser 5:...3000
Vuser 3:...1000
Vuser 5:Orders Done
Vuser 5:Loading Orders for D=9 W=20
Vuser 2:...1000
Vuser 4:...1000
Vuser 3:...2000
Vuser 5:...1000
Vuser 2:...2000
Vuser 3:...3000
Vuser 4:...2000
Vuser 3:Orders Done
Vuser 3:Loading Orders for D=10 W=10
Vuser 5:...2000
Vuser 2:...3000
Vuser 2:Orders Done
Vuser 2:End:Fri Feb 10 15:36:44 GMT 2023
Vuser 2:FINISHED SUCCESS
Vuser 4:...3000
Vuser 4:Orders Done
Vuser 4:End:Fri Feb 10 15:36:44 GMT 2023
Vuser 4:FINISHED SUCCESS
Vuser 3:...1000
Vuser 5:...3000
Vuser 5:Orders Done
Vuser 5:Loading Orders for D=10 W=20
Vuser 3:...2000
Vuser 5:...1000
Vuser 3:...3000
Vuser 3:Orders Done
Vuser 3:End:Fri Feb 10 15:36:45 GMT 2023
Vuser 3:FINISHED SUCCESS
Vuser 5:...2000
Vuser 5:...3000
Vuser 5:Orders Done
Vuser 5:End:Fri Feb 10 15:36:46 GMT 2023
Vuser 5:FINISHED SUCCESS
Vuser 1:Workers: 0 Active 4 Done
Vuser 1:CREATING TPCC STORED PROCEDURES
Vuser 1:GATHERING SCHEMA STATISTICS
Vuser 1:TPCC SCHEMA COMPLETE
Vuser 1:FINISHED SUCCESS
ALL VIRTUAL USERS COMPLETE
SCHEMA BUILD COMPLETED
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-
RUN HAMMERDB TEST
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-
HammerDB CLI v4.6
Copyright (C) 2003-2022 Steve Shaw
Type "help" for a list of commands
Initialized SQLite on-disk database /opt/HammerDB-master/Build/BawtBuild/Linux/x64/Release/Distribution/HammerDB-4.6/TMP/hammer.DB using existing tables (802,816 KB)
SETTING CONFIGURATION
Database set to MariaDB
Benchmark set to TPC-C for MariaDB
Value localhost for connection:maria_host is the same as existing value localhost, no change made
Value 3306 for connection:maria_port is the same as existing value 3306, no change made
Value /tmp/mariadb.sock for connection:maria_socket is the same as existing value /tmp/mariadb.sock, no change made
Value root for tpcc:maria_user is the same as existing value root, no change made
Value maria for tpcc:maria_pass is the same as existing value maria, no change made
Value tpcc for tpcc:maria_dbase is the same as existing value tpcc, no change made
Value timed for tpcc:maria_driver is the same as existing value timed, no change made
Value 2 for tpcc:maria_rampup is the same as existing value 2, no change made
Value 5 for tpcc:maria_duration is the same as existing value 5, no change made
Value true for tpcc:maria_allwarehouse is the same as existing value true, no change made
Value true for tpcc:maria_timeprofile is the same as existing value true, no change made
Script loaded, Type "print script" to view
TEST STARTED
Vuser 1 created MONITOR - WAIT IDLE
Vuser 2 created - WAIT IDLE
Vuser 3 created - WAIT IDLE
Vuser 4 created - WAIT IDLE
Vuser 5 created - WAIT IDLE
5 Virtual Users Created with Monitor VU
Transaction Counter Started
Transaction Counter thread running with threadid:tid0x7f64a37fe700
Vuser 1:RUNNING
Vuser 1:Initializing xtprof time profiler
0 MariaDB tpm
Vuser 1:Ssl_cipher
Vuser 1:Beginning rampup time of 2 minutes
Vuser 2:RUNNING
Vuser 2:Initializing xtprof time profiler
Vuser 2:Ssl_cipher
Vuser 2:VU 2 : Assigning WID=1 based on VU count 4, Warehouses = 20 (1 out of 5)
Vuser 2:VU 2 : Assigning WID=5 based on VU count 4, Warehouses = 20 (2 out of 5)
Vuser 3:RUNNING
Vuser 3:Initializing xtprof time profiler
Vuser 2:VU 2 : Assigning WID=9 based on VU count 4, Warehouses = 20 (3 out of 5)
Vuser 3:Ssl_cipher
Vuser 2:VU 2 : Assigning WID=13 based on VU count 4, Warehouses = 20 (4 out of 5)
Vuser 3:VU 3 : Assigning WID=2 based on VU count 4, Warehouses = 20 (1 out of 5)
Vuser 2:VU 2 : Assigning WID=17 based on VU count 4, Warehouses = 20 (5 out of 5)
Vuser 3:VU 3 : Assigning WID=6 based on VU count 4, Warehouses = 20 (2 out of 5)
Vuser 2:Processing 10000000 transactions with output suppressed...
Vuser 3:VU 3 : Assigning WID=10 based on VU count 4, Warehouses = 20 (3 out of 5)
Vuser 3:VU 3 : Assigning WID=14 based on VU count 4, Warehouses = 20 (4 out of 5)
Vuser 4:RUNNING
Vuser 3:VU 3 : Assigning WID=18 based on VU count 4, Warehouses = 20 (5 out of 5)
Vuser 3:Processing 10000000 transactions with output suppressed...
Vuser 4:Initializing xtprof time profiler
Vuser 4:Ssl_cipher
Vuser 4:VU 4 : Assigning WID=3 based on VU count 4, Warehouses = 20 (1 out of 5)
Vuser 4:VU 4 : Assigning WID=7 based on VU count 4, Warehouses = 20 (2 out of 5)
Vuser 4:VU 4 : Assigning WID=11 based on VU count 4, Warehouses = 20 (3 out of 5)
Vuser 4:VU 4 : Assigning WID=15 based on VU count 4, Warehouses = 20 (4 out of 5)
Vuser 4:VU 4 : Assigning WID=19 based on VU count 4, Warehouses = 20 (5 out of 5)
Vuser 4:Processing 10000000 transactions with output suppressed...
Vuser 5:RUNNING
Vuser 5:Initializing xtprof time profiler
Vuser 5:Ssl_cipher
Vuser 5:VU 5 : Assigning WID=4 based on VU count 4, Warehouses = 20 (1 out of 5)
Vuser 5:VU 5 : Assigning WID=8 based on VU count 4, Warehouses = 20 (2 out of 5)
Vuser 5:VU 5 : Assigning WID=12 based on VU count 4, Warehouses = 20 (3 out of 5)
Vuser 5:VU 5 : Assigning WID=16 based on VU count 4, Warehouses = 20 (4 out of 5)
Vuser 5:VU 5 : Assigning WID=20 based on VU count 4, Warehouses = 20 (5 out of 5)
Vuser 5:Processing 10000000 transactions with output suppressed...
69528 MariaDB tpm
85680 MariaDB tpm
77940 MariaDB tpm
79722 MariaDB tpm
81180 MariaDB tpm
80652 MariaDB tpm
Vuser 1:Rampup 1 minutes complete ...
78762 MariaDB tpm
80508 MariaDB tpm
79014 MariaDB tpm
80208 MariaDB tpm
79278 MariaDB tpm
Vuser 1:Rampup 2 minutes complete ...
Vuser 1:Rampup complete, Taking start Transaction Count.
Vuser 1:Timing test period of 5 in minutes
78324 MariaDB tpm
79398 MariaDB tpm
78462 MariaDB tpm
82122 MariaDB tpm
71898 MariaDB tpm
53268 MariaDB tpm
Vuser 1:1 ...,
59340 MariaDB tpm
60990 MariaDB tpm
59760 MariaDB tpm
63852 MariaDB tpm
65172 MariaDB tpm
62694 MariaDB tpm
65286 MariaDB tpm
Vuser 1:2 ...,
63138 MariaDB tpm
64878 MariaDB tpm
63252 MariaDB tpm
61836 MariaDB tpm
62964 MariaDB tpm
61302 MariaDB tpm
Vuser 1:3 ...,
62778 MariaDB tpm
63174 MariaDB tpm
64452 MariaDB tpm
67230 MariaDB tpm
64524 MariaDB tpm
67542 MariaDB tpm
Vuser 1:4 ...,
67050 MariaDB tpm
69540 MariaDB tpm
69612 MariaDB tpm
67926 MariaDB tpm
67182 MariaDB tpm
68634 MariaDB tpm
Vuser 1:5 ...,
Vuser 1:Test complete, Taking end Transaction Count.
Vuser 1:4 Active Virtual Users configured
Vuser 1:TEST RESULT : System achieved 28531 NOPM from 66257 MariaDB TPM
Vuser 1:Gathering timing data from Active Virtual Users...
Vuser 3:FINISHED SUCCESS
Vuser 2:FINISHED SUCCESS
Vuser 4:FINISHED SUCCESS
Vuser 5:FINISHED SUCCESS
Vuser 1:Calculating timings...
Vuser 1:Writing timing data to /opt/HammerDB-master/Build/BawtBuild/Linux/x64/Release/Distribution/HammerDB-4.6/TMP/hdbxtprofile.log
Vuser 1:FINISHED SUCCESS
ALL VIRTUAL USERS COMPLETE
vudestroy success
Transaction Counter thread running with threadid:tid0x7f64a37fe700
Stopping Transaction Counter
TEST COMPLETE
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-
DROP HAMMERDB SCHEMA
HammerDB CLI v4.6
Copyright (C) 2003-2022 Steve Shaw
Type "help" for a list of commands
Initialized SQLite on-disk database /opt/HammerDB-master/Build/BawtBuild/Linux/x64/Release/Distribution/HammerDB-4.6/TMP/hammer.DB using existing tables (827,392 KB)
SETTING CONFIGURATION
Database set to MariaDB
Benchmark set to TPC-C for MariaDB
Value localhost for connection:maria_host is the same as existing value localhost, no change made
Value 3306 for connection:maria_port is the same as existing value 3306, no change made
Value /tmp/mariadb.sock for connection:maria_socket is the same as existing value /tmp/mariadb.sock, no change made
Value root for tpcc:maria_user is the same as existing value root, no change made
Value maria for tpcc:maria_pass is the same as existing value maria, no change made
Value tpcc for tpcc:maria_dbase is the same as existing value tpcc, no change made
 DROP SCHEMA STARTED
Script cleared
Deleting schema with 1 Virtual User
Do you want to delete the TPCC TPROC-C schema
 in host LOCALHOST:/TMP/MARIADB.SOCK under user ROOT?
Enter yes or no: replied yes
Vuser 1 created - WAIT IDLE
Vuser 1:RUNNING
Vuser 1:Ssl_cipher
DROP SCHEMA COMPLETED
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-
HAMMERDB RESULT
HammerDB CLI v4.6
Copyright (C) 2003-2022 Steve Shaw
Type "help" for a list of commands
Initialized SQLite on-disk database /opt/HammerDB-master/Build/BawtBuild/Linux/x64/Release/Distribution/HammerDB-4.6/TMP/hammer.DB using existing tables (827,392 KB)
TRANSACTION RESPONSE TIMES
{
  "NEWORD": {
    "elapsed_ms": "420772.0",
    "calls": "53589",
    "min_ms": "1.251",
    "avg_ms": "4.257",
    "max_ms": "126.83",
    "total_ms": "228145.129",
    "p99_ms": "14.536",
    "p95_ms": "7.009",
    "p50_ms": "3.849",
    "sd": "3230.18",
    "ratio_pct": "54.171"
  },
  "PAYMENT": {
    "elapsed_ms": "420772.0",
    "calls": "53271",
    "min_ms": "0.64",
    "avg_ms": "1.674",
    "max_ms": "151.637",
    "total_ms": "89174.518",
    "p99_ms": "6.25",
    "p95_ms": "3.016",
    "p50_ms": "1.454",
    "sd": "1637.493",
    "ratio_pct": "21.174"
  },
  "DELIVERY": {
    "elapsed_ms": "420772.0",
    "calls": "5315",
    "min_ms": "5.085",
    "avg_ms": "13.539",
    "max_ms": "206.927",
    "total_ms": "71960.928",
    "p99_ms": "96.888",
    "p95_ms": "31.328",
    "p50_ms": "9.982",
    "sd": "14895.051",
    "ratio_pct": "17.086"
  },
  "OSTAT": {
    "elapsed_ms": "420772.0",
    "calls": "5349",
    "min_ms": "0.484",
    "avg_ms": "2.556",
    "max_ms": "119.354",
    "total_ms": "13670.706",
    "p99_ms": "51.026",
    "p95_ms": "4.436",
    "p50_ms": "1.17",
    "sd": "7759.746",
    "ratio_pct": "3.246"
  },
  "SLEV": {
    "elapsed_ms": "420772.0",
    "calls": "5356",
    "min_ms": "0.82",
    "avg_ms": "1.96",
    "max_ms": "55.227",
    "total_ms": "10496.673",
    "p99_ms": "6.68",
    "p95_ms": "2.998",
    "p50_ms": "1.746",
    "sd": "1219.649",
    "ratio_pct": "2.492"
  }
}

TRANSACTION COUNT
{"MariaDB tpm": {
    "0": "2023-02-10 15:36:52",
    "69528": "2023-02-10 15:37:02",
    "85680": "2023-02-10 15:37:12",
    "77940": "2023-02-10 15:37:22",
    "79722": "2023-02-10 15:37:32",
    "81180": "2023-02-10 15:37:42",
    "80652": "2023-02-10 15:37:52",
    "78762": "2023-02-10 15:38:02",
    "80508": "2023-02-10 15:38:12",
    "79014": "2023-02-10 15:38:22",
    "80208": "2023-02-10 15:38:32",
    "79278": "2023-02-10 15:38:42",
    "78324": "2023-02-10 15:38:52",
    "79398": "2023-02-10 15:39:02",
    "78462": "2023-02-10 15:39:12",
    "82122": "2023-02-10 15:39:22",
    "71898": "2023-02-10 15:39:32",
    "53268": "2023-02-10 15:39:42",
    "59340": "2023-02-10 15:39:52",
    "60990": "2023-02-10 15:40:02",
    "59760": "2023-02-10 15:40:12",
    "63852": "2023-02-10 15:40:22",
    "65172": "2023-02-10 15:40:32",
    "62694": "2023-02-10 15:40:42",
    "65286": "2023-02-10 15:40:52",
    "63138": "2023-02-10 15:41:02",
    "64878": "2023-02-10 15:41:12",
    "63252": "2023-02-10 15:41:22",
    "61836": "2023-02-10 15:41:32",
    "62964": "2023-02-10 15:41:42",
    "61302": "2023-02-10 15:41:52",
    "62778": "2023-02-10 15:42:02",
    "63174": "2023-02-10 15:42:12",
    "64452": "2023-02-10 15:42:22",
    "67230": "2023-02-10 15:42:32",
    "64524": "2023-02-10 15:42:42",
    "67542": "2023-02-10 15:42:52",
    "67050": "2023-02-10 15:43:02",
    "69540": "2023-02-10 15:43:12",
    "69612": "2023-02-10 15:43:22",
    "67926": "2023-02-10 15:43:32",
    "67182": "2023-02-10 15:43:42",
    "68634": "2023-02-10 15:43:52"
  }}

HAMMERDB RESULT
[
  "63E664945F4503E233630353",
  "2023-02-10 15:36:52",
  "4 Active Virtual Users configured",
  "TEST RESULT : System achieved 28531 NOPM from 66257 MariaDB TPM"
]</programlisting>

      <para>Note that the performance results are gathered and reported at the
      end of the workload using the HammerDB Jobs interface and this Jobs
      interface is detailed in the following section.</para>
    </section>

    <section>
      <title>CLI Jobs Interface</title>

      <para>From v4.6 the HammerDB CLI includes a jobs interface that stores
      the configuration, output, transaction count and timing data for a
      HammerDB workload that can be queried at a later point in time. This
      data is stored in a SQLite database called hammer.DB. From v4.8 the GUI
      environment also uses this Jobs database. The location of this is set in
      the generic.xml configuration file in the commandline section as
      sqlite_db, by default this area will be in the Temp directory. A
      flexible way to set the location is to set the TMP environment variable
      to a preferred location. For example on Linux.</para>

      <programlisting>export TMP=`pwd`/TMP</programlisting>

      <para>and on Windows.</para>

      <programlisting>C:\Program Files\HammerDB-4.9&gt;set TEMP=C:\temp
C:\Program Files\HammerDB-4.9&gt;hammerdbcli.bat
HammerDB CLI v4.10
Copyright (C) 2003-2024 Steve Shaw
Type "help" for a list of commands
Initialized new Jobs on-disk database C:/temp/hammer.DB
hammerdb&gt;
</programlisting>

      <para>The Jobs database can be relocated to any alternative preferred
      location as desired.</para>

      <programlisting>  &lt;commandline&gt;
&lt;sqlite_db&gt;TMP&lt;/sqlite_db&gt; 
        ...
        &lt;jobsoutput&gt;JSON&lt;/jobsoutput&gt;
        &lt;jobs_disable&gt;0&lt;/jobs_disable&gt;
   &lt;/commandline&gt;</programlisting>

      <para>Additional configuration options are jobsoutput that can be set to
      "text" or "JSON" and jobs_disable that if set to 1 will disable the
      storage of jobs data.</para>

      <para>Also under the timeprofile section there is the option to
      individually disable the storage of the timeprofiler when timing data is
      activated by setting xt_job_storage to 0.</para>

      <programlisting>&lt;timeprofile&gt;
...
           &lt;xt_job_storage&gt;1&lt;/xt_job_storage&gt;
    &lt;/timeprofile&gt;
</programlisting>

      <para>If jobs are enabled and the database is successfully initialized
      then a message such as follows will be shown on CLI startup.</para>

      <programlisting>Initialized SQLite on-disk database C:/Users/Hdb/AppData/Local/Temp/hammer.DB using existing tables (36,864 KB)</programlisting>

      <para>If manually deleted HammerDB will recreate a new jobs
      database.</para>

      <para>If you do not want a jobs repository you can disable it as follows
      and then restart your database.</para>

      <programlisting>hammerdb&gt;jobs disable 1
Disabling jobs repository, restart HammerDB to take effect</programlisting>

      <para>Any further jobs commands will receive the following message until
      re-enabled.</para>

      <programlisting>hammerdb&gt;jobs
Error: Jobs Disabled: enable with command "jobs disable 0" and restart HammerDB

hammerdb&gt;</programlisting>

      <para>When jobs are running all output will be stored in the local
      SQLite database and therefore if for example running "test" workloads
      with large volumes of output or establishing a configuration you may not
      wish to store jobs output at this time.</para>

      <para>The jobs command has the following functionality:</para>

      <table>
        <title>Jobs command</title>

        <tgroup cols="2">
          <tbody>
            <row>
              <entry>jobs</entry>

              <entry>list all jobs</entry>
            </row>

            <row>
              <entry>jobs [jobid|joblist||result|timestamp]</entry>

              <entry>Query all jobs, list the VU output for the jobid, list
              the results for all jobs, list the timestamp for all
              jobs.</entry>
            </row>

            <row>
              <entry>jobs format [ text|JSON]</entry>

              <entry>Format job output as text or JSON.</entry>
            </row>

            <row>
              <entry>jobs disable [ 0 | 1 ]</entry>

              <entry>Disable or re-enable storage of job output after
              restart.</entry>
            </row>

            <row>
              <entry>jobs jobid
              [bm|db|delete|dict|result|status|tcount|timestamp|timing|vuid]</entry>

              <entry>Query an individual job, showing the full output,
              benchmark, database, delete the individual job, show the result,
              status, transaction count, timestamp, timing and limited output
              for a single vuid.</entry>
            </row>

            <row>
              <entry>jobs jobid timing - Usage: jobs jobid timing vuid</entry>

              <entry>Show the timing data for an individual vuid.</entry>
            </row>

            <row>
              <entry>jobs jobid getchart [result | timing | tcount]</entry>

              <entry>Generate html chart for TPROC-C/TPROC-H results, timing
              and transaction counts</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <para>As an example of jobs functionality if we run the following
      script.</para>

      <programlisting>dbset db mssqls
diset tpcc mssqls_driver timed
diset tpcc mssqls_rampup 2
diset tpcc mssqls_duration 5
diset tpcc mssqls_timeprofile true
loadscript
vuset vu vcpu
vucreate
tcstart
set jobid [ vurun ]
tcstop
vudestroy
puts "jobid is $jobid"</programlisting>

      <para>It produces output as follows, the vurun command returns the jobid
      of the job it ran.</para>

      <programlisting>hammerdb&gt;source clijob.tcl
Database set to MSSQLServer
Value timed for tpcc:mssqls_driver is the same as existing value timed, no change made
Value 2 for tpcc:mssqls_rampup is the same as existing value 2, no change made
Value 5 for tpcc:mssqls_duration is the same as existing value 5, no change made
Value true for tpcc:mssqls_timeprofile is the same as existing value true, no change made
Script loaded, Type "print script" to view
Vuser 1 created MONITOR - WAIT IDLE
Vuser 2 created - WAIT IDLE
Vuser 3 created - WAIT IDLE
Vuser 4 created - WAIT IDLE
Vuser 5 created - WAIT IDLE
Vuser 6 created - WAIT IDLE
Vuser 7 created - WAIT IDLE
Vuser 8 created - WAIT IDLE
Vuser 9 created - WAIT IDLE
9 Virtual Users Created with Monitor VU
Transaction Counter Started
Vuser 1:RUNNING
Vuser 1:Initializing xtprof time profiler
0 MSSQLServer tpm
Vuser 1:Beginning rampup time of 2 minutes
Vuser 2:RUNNING
Vuser 2:Initializing xtprof time profiler
Vuser 2:Processing 10000000 transactions with output suppressed...
Vuser 3:RUNNING
Vuser 3:Initializing xtprof time profiler
Vuser 3:Processing 10000000 transactions with output suppressed...
Vuser 4:RUNNING
Vuser 4:Initializing xtprof time profiler
Vuser 4:Processing 10000000 transactions with output suppressed...
Vuser 5:RUNNING
Vuser 5:Initializing xtprof time profiler
Vuser 5:Processing 10000000 transactions with output suppressed...
Vuser 6:RUNNING
Vuser 6:Initializing xtprof time profiler
Vuser 6:Processing 10000000 transactions with output suppressed...
Vuser 7:RUNNING
Vuser 7:Initializing xtprof time profiler
Vuser 7:Processing 10000000 transactions with output suppressed...
Vuser 8:RUNNING
Vuser 8:Initializing xtprof time profiler
Vuser 8:Processing 10000000 transactions with output suppressed...
Vuser 9:RUNNING
Vuser 9:Initializing xtprof time profiler
Vuser 9:Processing 10000000 transactions with output suppressed...
321276 MSSQLServer tpm
310152 MSSQLServer tpm
353814 MSSQLServer tpm
419544 MSSQLServer tpm
407388 MSSQLServer tpm
Vuser 1:Rampup 1 minutes complete ...
416802 MSSQLServer tpm
311670 MSSQLServer tpm
397320 MSSQLServer tpm
410778 MSSQLServer tpm
393156 MSSQLServer tpm
411288 MSSQLServer tpm
Vuser 1:Rampup 2 minutes complete ...
Vuser 1:Rampup complete, Taking start Transaction Count.
Vuser 1:Timing test period of 5 in minutes
361482 MSSQLServer tpm
388710 MSSQLServer tpm
381024 MSSQLServer tpm
309954 MSSQLServer tpm
385224 MSSQLServer tpm
394812 MSSQLServer tpm
Vuser 1:1 ...,
382122 MSSQLServer tpm
354288 MSSQLServer tpm
422178 MSSQLServer tpm
393714 MSSQLServer tpm
327702 MSSQLServer tpm
281616 MSSQLServer tpm
Vuser 1:2 ...,
405342 MSSQLServer tpm
403242 MSSQLServer tpm
385608 MSSQLServer tpm
399612 MSSQLServer tpm
422100 MSSQLServer tpm
385494 MSSQLServer tpm
Vuser 1:3 ...,
393402 MSSQLServer tpm
320376 MSSQLServer tpm
412254 MSSQLServer tpm
411258 MSSQLServer tpm
394920 MSSQLServer tpm
400608 MSSQLServer tpm
Vuser 1:4 ...,
372738 MSSQLServer tpm
370362 MSSQLServer tpm
387276 MSSQLServer tpm
379926 MSSQLServer tpm
350820 MSSQLServer tpm
370398 MSSQLServer tpm
Vuser 1:5 ...,
Vuser 1:Test complete, Taking end Transaction Count.
Vuser 1:8 Active Virtual Users configured
Vuser 1:TEST RESULT : System achieved 161688 NOPM from 376090 SQL Server TPM
Vuser 1:Gathering timing data from Active Virtual Users...
Vuser 8:FINISHED SUCCESS
Vuser 3:FINISHED SUCCESS
Vuser 9:FINISHED SUCCESS
Vuser 6:FINISHED SUCCESS
293592 MSSQLServer tpm
Vuser 4:FINISHED SUCCESS
Vuser 5:FINISHED SUCCESS
Vuser 2:FINISHED SUCCESS
Vuser 7:FINISHED SUCCESS
Vuser 1:Calculating timings...
Vuser 1:Writing timing data to C:/Users/Steve/AppData/Local/Temp/hdbxtprofile.log
Vuser 1:FINISHED SUCCESS
ALL VIRTUAL USERS COMPLETE
156 MSSQLServer tpm
Transaction Counter thread running with threadid:tid0000000000001308
Stopping Transaction Counter
vudestroy success
jobid is jobid=637CF9B73C8D03E243636303

hammerdb&gt;</programlisting>

      <para>Querying the job status will return all of the status messages
      from the Virtual Users showing that the job completed
      successfully.</para>

      <programlisting>hammerdb&gt;job 637CF9B73C8D03E243636303 status
[
  "0",
  "Vuser 1:RUNNING",
  "0",
  "Vuser 2:RUNNING",
  "0",
  "Vuser 3:RUNNING",
  "0",
  "Vuser 4:RUNNING",
  "0",
  "Vuser 5:RUNNING",
  "0",
  "Vuser 6:RUNNING",
  "0",
  "Vuser 7:RUNNING",
  "0",
  "Vuser 8:RUNNING",
  "0",
  "Vuser 9:RUNNING",
  "0",
  "Vuser 8:FINISHED SUCCESS",
  "0",
  "Vuser 3:FINISHED SUCCESS",
  "0",
  "Vuser 9:FINISHED SUCCESS",
  "0",
  "Vuser 6:FINISHED SUCCESS",
  "0",
  "Vuser 4:FINISHED SUCCESS",
  "0",
  "Vuser 5:FINISHED SUCCESS",
  "0",
  "Vuser 2:FINISHED SUCCESS",
  "0",
  "Vuser 7:FINISHED SUCCESS",
  "0",
  "Vuser 1:FINISHED SUCCESS",
  "0",
  "ALL VIRTUAL USERS COMPLETE"
]</programlisting>

      <para>We can also view the result of the job.</para>

      <programlisting>hammerdb&gt;job 637CF9B73C8D03E243636303 result
[
  "637CF9B73C8D03E243636303",
  "2022-11-22 16:32:55",
  "8 Active Virtual Users configured",
  "TEST RESULT : System achieved 161688 NOPM from 376090 SQL Server TPM"
]</programlisting>

      <para>Querying the job with no further arguments returns the output from
      all Virtual Users. If a vuid is specified it returns the output from a
      single VU.</para>

      <programlisting>hammerdb&gt;job 637CF9B73C8D03E243636303
[
  "0",
  "Vuser 1:RUNNING",
  "1",
  "Initializing xtprof time profiler",
  "1",
  "Beginning rampup time of 2 minutes",
  "0",
  "Vuser 2:RUNNING",
  "2",
  "Initializing xtprof time profiler",
  "2",
  "Processing 10000000 transactions with output suppressed...",
  "0",
  "Vuser 3:RUNNING",
  "3",
  "Initializing xtprof time profiler",
  "3",
  "Processing 10000000 transactions with output suppressed...",
  "0",
  "Vuser 4:RUNNING",
  "4",
  "Initializing xtprof time profiler",
  "4",
  "Processing 10000000 transactions with output suppressed...",
  "0",
  "Vuser 5:RUNNING",
  "5",
  "Initializing xtprof time profiler",
  "5",
  "Processing 10000000 transactions with output suppressed...",
  "0",
  "Vuser 6:RUNNING",
  "6",
  "Initializing xtprof time profiler",
  "6",
  "Processing 10000000 transactions with output suppressed...",
  "0",
  "Vuser 7:RUNNING",
  "7",
  "Initializing xtprof time profiler",
  "7",
  "Processing 10000000 transactions with output suppressed...",
  "0",
  "Vuser 8:RUNNING",
  "8",
  "Initializing xtprof time profiler",
  "8",
  "Processing 10000000 transactions with output suppressed...",
  "0",
  "Vuser 9:RUNNING",
  "9",
  "Initializing xtprof time profiler",
  "9",
  "Processing 10000000 transactions with output suppressed...",
  "1",
  "Rampup 1 minutes complete ...",
  "1",
  "Rampup 2 minutes complete ...",
  "1",
  "Rampup complete, Taking start Transaction Count.",
  "1",
  "Timing test period of 5 in minutes",
  "1",
  "1  ...,",
  "1",
  "2  ...,",
  "1",
  "3  ...,",
  "1",
  "4  ...,",
  "1",
  "5  ...,",
  "1",
  "Test complete, Taking end Transaction Count.",
  "1",
  "8 Active Virtual Users configured",
  "1",
  "TEST RESULT : System achieved 161688 NOPM from 376090 SQL Server TPM",
  "1",
  "Gathering timing data from Active Virtual Users...",
  "0",
  "Vuser 8:FINISHED SUCCESS",
  "0",
  "Vuser 3:FINISHED SUCCESS",
  "0",
  "Vuser 9:FINISHED SUCCESS",
  "0",
  "Vuser 6:FINISHED SUCCESS",
  "0",
  "Vuser 4:FINISHED SUCCESS",
  "0",
  "Vuser 5:FINISHED SUCCESS",
  "0",
  "Vuser 2:FINISHED SUCCESS",
  "0",
  "Vuser 7:FINISHED SUCCESS",
  "1",
  "Calculating timings...",
  "1",
  "Writing timing data to C:\/Users\/Steve\/AppData\/Local\/Temp\/hdbxtprofile.log",
  "0",
  "Vuser 1:FINISHED SUCCESS",
  "0",
  "ALL VIRTUAL USERS COMPLETE"
]</programlisting>

      <para>We also store the configuration so can query the dict of the job
      that was run.</para>

      <programlisting>hammerdb&gt;job 637CF9B73C8D03E243636303 dict
{
  "connection": {
    "mssqls_server": "(local)\\SQLDEVELOP",
    "mssqls_linux_server": "localhost",
    "mssqls_tcp": "false",
    "mssqls_port": "1433",
    "mssqls_azure": "false",
    "mssqls_authentication": "windows",
    "mssqls_linux_authent": "sql",
    "mssqls_odbc_driver": "ODBC Driver 18 for SQL Server",
    "mssqls_linux_odbc": "ODBC Driver 18 for SQL Server",
    "mssqls_uid": "sa",
    "mssqls_pass": "admin",
    "mssqls_encrypt_connection": "true",
    "mssqls_trust_server_cert": "true"
  },
  "tpcc": {
    "mssqls_count_ware": "10",
    "mssqls_num_vu": "8",
    "mssqls_dbase": "tpcc",
    "mssqls_imdb": "false",
    "mssqls_bucket": "1",
    "mssqls_durability": "SCHEMA_AND_DATA",
    "mssqls_total_iterations": "10000000",
    "mssqls_raiseerror": "false",
    "mssqls_keyandthink": "false",
    "mssqls_checkpoint": "false",
    "mssqls_driver": "timed",
    "mssqls_rampup": "2",
    "mssqls_duration": "5",
    "mssqls_allwarehouse": "false",
    "mssqls_timeprofile": "true",
    "mssqls_async_scale": "false",
    "mssqls_async_client": "10",
    "mssqls_async_verbose": "false",
    "mssqls_async_delay": "1000",
    "mssqls_connect_pool": "false"
  }
}</programlisting>

      <para>If we ran the transaction counter we can retrieve the transaction
      count for the duration of the job.</para>

      <programlisting>hammerdb&gt;job 637CF9B73C8D03E243636303 tcount
{"MSSQLServer tpm": {
    "0": "2022-11-22 16:32:56",
    "321276": "2022-11-22 16:33:06",
    "310152": "2022-11-22 16:33:16",
    "353814": "2022-11-22 16:33:26",
    "419544": "2022-11-22 16:33:36",
    "407388": "2022-11-22 16:33:46",
    "416802": "2022-11-22 16:33:56",
    "311670": "2022-11-22 16:34:06",
    "397320": "2022-11-22 16:34:16",
    "410778": "2022-11-22 16:34:26",
    "393156": "2022-11-22 16:34:36",
    "411288": "2022-11-22 16:34:46",
    "361482": "2022-11-22 16:34:56",
    "388710": "2022-11-22 16:35:07",
    "381024": "2022-11-22 16:35:17",
    "309954": "2022-11-22 16:35:27",
    "385224": "2022-11-22 16:35:37",
    "394812": "2022-11-22 16:35:47",
    "382122": "2022-11-22 16:35:57",
    "354288": "2022-11-22 16:36:07",
    "422178": "2022-11-22 16:36:17",
    "393714": "2022-11-22 16:36:27",
    "327702": "2022-11-22 16:36:37",
    "281616": "2022-11-22 16:36:47",
    "405342": "2022-11-22 16:36:57",
    "403242": "2022-11-22 16:37:07",
    "385608": "2022-11-22 16:37:18",
    "399612": "2022-11-22 16:37:28",
    "422100": "2022-11-22 16:37:38",
    "385494": "2022-11-22 16:37:48",
    "393402": "2022-11-22 16:37:58",
    "320376": "2022-11-22 16:38:08",
    "412254": "2022-11-22 16:38:18",
    "411258": "2022-11-22 16:38:28",
    "394920": "2022-11-22 16:38:38",
    "400608": "2022-11-22 16:38:48",
    "372738": "2022-11-22 16:38:58",
    "370362": "2022-11-22 16:39:09",
    "387276": "2022-11-22 16:39:19",
    "379926": "2022-11-22 16:39:29",
    "350820": "2022-11-22 16:39:39",
    "370398": "2022-11-22 16:39:49",
    "293592": "2022-11-22 16:39:59",
    "156": "2022-11-22 16:40:09"
  }}</programlisting>

      <para>and similarly with the timing data.</para>

      <programlisting>hammerdb&gt;job 637CF9B73C8D03E243636303 timing
{
  "NEWORD": {
    "elapsed_ms": "419545.5",
    "calls": "135219",
    "min_ms": "0.442",
    "avg_ms": "1.365",
    "max_ms": "5233.271",
    "total_ms": "184527.133",
    "p99_ms": "3.706",
    "p95_ms": "2.07",
    "p50_ms": "1.081",
    "sd": "155972.059",
    "ratio_pct": "44.232"
  },
  "PAYMENT": {
    "elapsed_ms": "419545.5",
    "calls": "135485",
    "min_ms": "0.371",
    "avg_ms": "1.009",
    "max_ms": "465.959",
    "total_ms": "136748.509",
    "p99_ms": "3.098",
    "p95_ms": "1.677",
    "p50_ms": "0.741",
    "sd": "36829.979",
    "ratio_pct": "32.779"
  },
  "DELIVERY": {
    "elapsed_ms": "419545.5",
    "calls": "13481",
    "min_ms": "0.922",
    "avg_ms": "2.882",
    "max_ms": "4545.935",
    "total_ms": "38847.119",
    "p99_ms": "5.296",
    "p95_ms": "3.074",
    "p50_ms": "1.879",
    "sd": "523063.177",
    "ratio_pct": "9.312"
  },
  "SLEV": {
    "elapsed_ms": "419545.5",
    "calls": "13435",
    "min_ms": "0.645",
    "avg_ms": "2.015",
    "max_ms": "1279.086",
    "total_ms": "27074.317",
    "p99_ms": "2.706",
    "p95_ms": "1.898",
    "p50_ms": "1.304",
    "sd": "220436.794",
    "ratio_pct": "6.49"
  },
  "OSTAT": {
    "elapsed_ms": "419545.5",
    "calls": "13548",
    "min_ms": "0.3",
    "avg_ms": "1.266",
    "max_ms": "2386.227",
    "total_ms": "17153.631",
    "p99_ms": "2.365",
    "p95_ms": "1.544",
    "p50_ms": "0.771",
    "sd": "235611.656",
    "ratio_pct": "4.112"
  }
}</programlisting>

      <para>By default the jobs output is returned in JSON list and dict
      format. If preferred this output can be set to text.</para>

      <programlisting>hammerdb&gt;job format text
Setting jobs output format to text

hammerdb&gt;jobs
6373B8BC5B2903E243637333
637CD47BAB9C03E203730313
637CD5C0CC7A03E203330323
637CD5FEF1A303E293839323
637CD63E175A03E203430383
637CD8FAB8D703E233032343
637CD9FD537D03E283432383
637CDA447DC703E293232393
637CDA82A24E03E233437353
637CDAC0C74B03E263631323
637CF5B8DA7503E293739383
637CF7CA167B03E213436333
637CF9B73C8D03E243636303

hammerdb&gt;job 637CF9B73C8D03E243636303 bm
TPC-C

hammerdb&gt;job 637CF9B73C8D03E243636303 timestamp
637CF9B73C8D03E243636303 {2022-11-22 16:32:55}
</programlisting>

      <para>The jobs functionality extends HammerDB functionality as an
      essential repository for your test configuration and results.</para>
    </section>
  </chapter>

  <chapter>
    <title>Viewing Jobs with the Web Service Interface (WS)</title>

    <para>From version 4.8 the HammerDB HTTP Web Service has been enhanced to
    provide a read-only interface to the SQLite Jobs repository database. The
    Web Service can be started, and stopped with the GUI, CLI and
    manually.</para>

    <section>
      <title>Web Service Configuration</title>

      <para>There are 2 configuration parameters for the web service in the
      file generic.xml in the config directory, ws_port and sqlite_db. ws_port
      defines the port on which the service will run and sqlite_db defines the
      location of the SQLite database file where job related data is stored.
      By default a temporary file location is used by specifying TMP. If
      :memory: is used an in-memory SQLite database will be used, however the
      data in this location will not be stored after the webservice stops and
      is incompatible with functionality such as time profiling, for this
      reason an on-disk location is recommended.</para>

      <programlisting>   &lt;sqlitedb&gt;
        &lt;sqlitedb_dir&gt;TMP&lt;/sqlitedb_dir&gt;
   &lt;/sqlitedb&gt;

  &lt;webservice&gt;
   &lt;ws_port&gt;8080&lt;/ws_port&gt; 
  &lt;/webservice&gt;</programlisting>

      <para>The webservice can be configured under the GUI under the new Jobs
      option.</para>

      <para><figure>
          <title>Jobs Option</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="../DocBook/docs/images/ch10ws-1.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>Under Options you can Enable/Disable the Jobs interface. If
      disabled no Job related information will be stored in the SQLite
      database. When enabled you have the option to set the port that the Web
      Service will listen on and to start and stop the Web Service.</para>

      <figure>
        <title>Jobs and Web Service Configuration</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="../DocBook/docs/images/ch10ws-1a.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>When running the Status button will show the Web Service
      environment.</para>

      <figure>
        <title>Web Service Status</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="../DocBook/docs/images/ch10ws-1b.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The same commands can also be run from the CLI to enable or
      disable Jobs and to start/stop the Web Service.</para>

      <programlisting>hammerdb&gt;jobs disable 0
Enabling jobs repository, restart HammerDB to take effect

hammerdb&gt;jobs disable 1
Disabling jobs repository, restart HammerDB to take effect

hammerdb&gt;wsport
Web Service Port set to 8080

hammerdb&gt;wsstart
HammerDB Web Service v4.8
Copyright (C) 2003-2023 Steve Shaw
Type "help" for a list of commands

hammerdb&gt;Starting HammerDB Web Service on port 8080

hammerdb&gt;wsstatus
Web Service running: Service Environment

BASE_URL = http://localhost:8080
DOCUMENT_ROOT = {C:/Program Files/HammerDB-4.8}
HTTP_ACCEPT_ENCODING = gzip,deflate,compress
HTTP_HOST = localhost:8080
HTTP_USER_AGENT = {Mozilla/5.0 (Windows; U; Windows NT 10.0) http/2.9.5 Tcl/8.6.12}
PATH_HEAD = env
PATH_INFO = /env
PATH_TAIL = {}
QUERY_STRING = {}
REMOTE_ADDR = ::1
REMOTE_PORT = 63830
REQUEST_METHOD = GET
REQUEST_URI = /env
SAME_ORIGIN = 0
SCRIPT_FILENAME = {C:/Program Files/HammerDB-4.8/wait}
SCRIPT_NAME = {}
SELF_URL = http://localhost:8080/env
WAPP_MODE = server
[pwd] = {C:/Program Files/HammerDB-4.8}

hammerdb&gt;wsstop
Stopping HammerDB Web Service on port 8080

</programlisting>

      <para>You can also start and stop the Web Service directly. With no
      arguments or the argument of wait the Web Service will wait in listening
      mode.</para>

      <programlisting>:\Program Files\HammerDB-4.8&gt;hammerdbws
HammerDB Web Service v4.8
Copyright (C) 2003-2023 Steve Shaw
Type "help" for a list of commands
Starting HammerDB Web Service on port 8080</programlisting>

      <para>With the argument of nowait the Web Service will return a prompt.
      This allows you to manually interact with the Web Service API and return
      Job related html to the prompt.</para>

      <programlisting>C:\Program Files\HammerDB-4.8&gt;hammerdbws nowait
HammerDB Web Service v4.8
Copyright (C) 2003-2023 Steve Shaw
Type "help" for a list of commands
Starting HammerDB Web Service on port 8080

hammerws&gt;help
HammerDB Web Service
Help:
GET jobs: Show the job ids, configuration, output, status, results and timings of jobs created by buildschema and vurun. Job output is equivalent to the output viewed in the graphical interface or command line.
get http://localhost:8080/jobs
get http://localhost:8080/jobs?jobid=TEXT
get http://localhost:8080/jobs?jobid=TEXT&amp;bm
get http://localhost:8080/jobs?jobid=TEXT&amp;db
get http://localhost:8080/jobs?jobid=TEXT&amp;delete
get http://localhost:8080/jobs?jobid=TEXT&amp;dict
get http://localhost:8080/jobs?jobid=TEXT&amp;index
get http://localhost:8080/jobs?jobid=TEXT&amp;result
get http://localhost:8080/jobs?jobid=TEXT&amp;resultdata
get http://localhost:8080/jobs?jobid=TEXT&amp;status
get http://localhost:8080/jobs?jobid=TEXT&amp;tcount
get http://localhost:8080/jobs?jobid=TEXT&amp;tcountdata
get http://localhost:8080/jobs?jobid=TEXT&amp;timestamp
get http://localhost:8080/jobs?jobid=TEXT&amp;timing
get http://localhost:8080/jobs?jobid=TEXT&amp;timingdata
get http://localhost:8080/jobs?jobid=TEXT&amp;timing&amp;vuid=INTEGER
get http://localhost:8080/jobs?jobid=TEXT&amp;vu=INTEGER

hammerws&gt;jobs
&lt;html&gt;
&lt;head&gt;
&lt;meta name="viewport" content="width=device-width, initial-scale=1.0"&gt;
&lt;meta http-equiv="content-type" content="text/html; charset=UTF-8"&gt;
&lt;link href="http://localhost:8080/style.css" rel="stylesheet"&gt;
&lt;title&gt;HammerDB Results&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;p&gt;&lt;img src='http://localhost:8080/logo.png' width='347' height='60'&gt;&lt;/p&gt;&lt;h3 class="title"&gt;Job Index:&lt;/h3&gt;&lt;div class='hammerdb' data-title='Jobs'&gt;&lt;div&gt;&lt;ol style='column-width: 20ex;'&gt;
&lt;br&gt;&lt;table&gt;</programlisting>
    </section>

    <section>
      <title>Viewing Jobs</title>

      <para>With Jobs enabling all schema builds and benchmark runs will be
      registered and recorded with a Jobid.</para>

      <para><figure>
          <title>Running Jobs</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="../DocBook/docs/images/ch10ws-1c.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>When running the Web Service directing a browser to the
      configured port will return a list of Jobs and their status of either
      successfully completed, failed or unknown. The browse option under Jobs
      in the menu will start the system default browser and direct it
      automatically to the correct port. The top job result for both TPROC-C
      and TPROC-H will be starred for quick identification. Note that if for
      example running jobs through the CLI the web service can be used to
      query from the same or separate system the status of currently running
      jobs. This means for example an automated scripted CLI workload can be
      monitored from a remote location and higher performance workloads
      identified.</para>

      <figure>
        <title>Browse Results</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="../DocBook/docs/images/ch10ws-1d.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Clicking on a particular job will return the job index. The
      entries for a job will depend on the data gathered for that job. For
      example if the transaction counter is not running then transaction count
      data will not be gathered. Similarly for response time data.</para>

      <figure>
        <title>Job Index</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="../DocBook/docs/images/ch10ws-1e.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Text based data will be reported in JSON format.</para>

      <figure>
        <title>JSON data</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="../DocBook/docs/images/ch10ws-1f.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>For both TPROC-C and TPROC-H result, transaction/query count and
      timing data will be reported in Chart form. The Charts use Apache
      e-charts and therefore access to the URL
      https://cdn.jsdelivr.net/npm/echarts@5.4.1/dist/echarts.min.js is
      required to access this functionality. The following example shows a
      TPROC-C result.</para>

      <figure>
        <title>Result Chart</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="../DocBook/docs/images/ch10ws-1g.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>A transaction count.</para>

      <figure>
        <title>Transaction Count</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="../DocBook/docs/images/ch10ws-1h.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>and response time measurements.</para>

      <figure>
        <title>Response Times</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="../DocBook/docs/images/ch10ws-1i.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>TPROC-H data will also be reported with Timing data for the first
      Virtual User.</para>

      <figure>
        <title>TPROC-H Timing</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="../DocBook/docs/images/ch10ws-1j.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>All charts are interactive and underlying data accessed through a
      link below the chart. Additional data for a job is also available
      through querying the job with the CLI interface.</para>
    </section>
  </chapter>

  <chapter>
    <title>Introduction to Analytic Testing (TPROC-H derived from TPC-H) and
    Cloud Queries</title>

    <para>Analytic workloads can also be interchangeably described as Decision
    Support, Data Warehousing or Business Intelligence, the basis of these
    workloads is the ability to process complex ad-hoc queries on large
    volumes of data. In contrast to a transactional workload the focus is upon
    reading as opposed to modifying data and therefore requires a distinct
    approach. The ability of a database to process transactions gives limited
    information towards the ability of a database to support query based
    workloads and vice-versa, therefore both TPROC-C and TPROC-H based
    workloads complement each other in investigating the capabilities of a
    particular database. When reading large volumes of data to satisfy query
    workloads it should be apparent that if multiple CPU cores are available
    reading with a single processing thread is going to leave a significant
    amount of resources underutilized. Consequently the most effective
    Analytic Systems employ a feature called Parallel Query to break down such
    queries into multiple sub tasks to complete the query more quickly.
    Additional features such as column orientation, compression and
    partitioning can also be used to improve parallel query performance.
    Advances in server technologies in particular large numbers of CPU cores
    available with large memory configurations have popularised both in-memory
    and column store technologies as a means to enhance Parallel Query
    performance. Examples of databases supported by HammerDB that support some
    or all of these enhanced query technologies are the Oracle Database, SQL
    Server, Db2, MariaDB and PostgreSQL, databases that do not support any of
    these technologies are single threaded query workloads and cannot be
    expected to complete these workloads as quickly. If you are unfamiliar
    with row-oriented and column-store technologies then it is beneficial to
    read one of the many guides explaining the differences and familiarising
    with the technologies available in the database that you have chosen to
    test. With commercial databases you should also ensure that your license
    includes the ability to run Parallel workloads as you may have a version
    of a database that supports single-threaded workloads only.</para>

    <section>
      <title>What is TPROC-H derived from TPC-H?</title>

      <para>To complement the OLTP type TPROC-C workload HammerDB also
      contains a Fair Use derivation of the decision support based TPC-H
      Benchmark Standard. The HammerDB TPROC-H workload is an open source
      workload derived from the TPC-H Benchmark Standard and as such is not
      comparable to published TPC-H results, as the results do not comply with
      the TPC-H Benchmark Standard. TPROC-H in simple terms can be thought of
      as complementing the workload implemented in TPROC-C related to the
      activities of a wholesale supplier. However, whereas TPROC-C simulates
      an online ordering system TPROC-H represents the typical workload of a
      retailer running analytical queries about their operations. To do this
      TPROC-H is represented by a set of business focused ad-hoc queries (in
      addition to concurrent data updates and deletes) and is measured upon
      the time it takes to complete these queries. In particular the focus is
      upon highly complex queries that require the processing of large volumes
      of data. Also in similarity to TPROC-C the schema size is not fixed and
      is dependent upon a Scale Factor and therefore your schema can also be
      as small or large as you wish with a larger schema requiring a more
      powerful computer system to process the increased data volume for
      queries. However, in contrast to TPROC-C it is not valid to compare the
      test results of query load tests taken at different Scale Factors shown
      as SF in the Schema diagram.</para>

      <figure>
        <title>TPROC-H Schema.</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch9-1.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The workload is represented by users executing a stream of 22
      ad-hocs queries against the database with an example query as
      follows:</para>

      <programlisting>-- using 647655760 as a seed to the RNG
 select
        l_returnflag,
        l_linestatus,
        sum(l_quantity) as sum_qty,
        sum(l_extendedprice) as sum_base_price,
        sum(l_extendedprice * (1 - l_discount)) as sum_disc_price,
        sum(l_extendedprice * (1 - l_discount) * (1 + l_tax)) as sum_charge,
        avg(l_quantity) as avg_qty,
        avg(l_extendedprice) as avg_price,
        avg(l_discount) as avg_disc,
        count(*) as count_order 
from
        lineitem 
where
        l_shipdate &lt;= date '1998-12-01' – interval '69' day (3)
group by
        l_returnflag,
        l_linestatus 
order by
        l_returnflag,
        l_linestatus;
</programlisting>

      <para>In measuring the results the key aspect is the time the queries
      take to complete and it is recommended to use the geometric mean of the
      query times for comparison. A typical performance profile is represented
      by the time it takes the system to process a query set from Q1 to Q22
      (run in a pre-determined random order).</para>

      <figure>
        <title>Power Query</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch9-2.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>

    <section>
      <title>Running the Power and Throughput Test and Calculating the
      Geometric Mean</title>

      <para>The audited metric for TPC-H workloads is called QphH, however
      because of the differences in how these workloads are run, in particular
      using bulk operations for data loads it is not recommended that the QphH
      be calculated for HammerDB workloads. Instead it is recommended to
      measure and compare the geometric mean of the power and throughput test
      query times. For the audited results the following 3 aspects of the
      capability of the system to process queries are considered:</para>

      <orderedlist>
        <listitem>
          <para>Database size.</para>
        </listitem>

        <listitem>
          <para>Query processing power of queries in a single stream.</para>
        </listitem>

        <listitem>
          <para>Total query throughput of queries from multiple concurrent
          users.</para>
        </listitem>
      </orderedlist>

      <para>For the multiple concurrent user tests the throughput test always
      follows the power test and the number of Virtual Users is based upon the
      following table where each Stream is processed by a Virtual User in
      HammerDB. This can also serve as a guide when running throughput tests
      with HammerDB taking the metric as the geomean of the query times of the
      slowest virtual user to complete the query set.</para>

      <table>
        <title>Query Streams and Scale Factors</title>

        <tgroup cols="2">
          <thead>
            <row>
              <entry align="center">SF ( Scale Factor )</entry>

              <entry align="center">S (Streams)</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>100000</entry>

              <entry>11</entry>
            </row>

            <row>
              <entry>30000</entry>

              <entry>10</entry>
            </row>

            <row>
              <entry>10000</entry>

              <entry>9</entry>
            </row>

            <row>
              <entry>3000</entry>

              <entry>8</entry>
            </row>

            <row>
              <entry>1000</entry>

              <entry>7</entry>
            </row>

            <row>
              <entry>300</entry>

              <entry>6</entry>
            </row>

            <row>
              <entry>100</entry>

              <entry>5</entry>
            </row>

            <row>
              <entry>30</entry>

              <entry>4</entry>
            </row>

            <row>
              <entry>10</entry>

              <entry>3</entry>
            </row>

            <row>
              <entry>1</entry>

              <entry>2</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <para>There is also the availability for a simultaneous data refresh
      set. HammerDB provides full capabilities to run this refresh set both
      automatically as part of a Power test and concurrently with a Throughput
      test. Note however that once a refresh set is run the schema is required
      to be refreshed and it is prudent to backup and restore a HammerDB
      TPROC-H based schema where running a refresh set is planned.</para>
    </section>

    <section>
      <title>Choosing a Database for running TPROC-H workloads</title>

      <para>TPROC-H workloads run complex queries scanning large volumes of
      data and therefore require the use of database features such as parallel
      query and in-memory column stores to maximise performance. With the
      available HammerDB TPROC-H based workloads the three databases that
      support these features are the Enterprise Editions of Oracle, SQL Server
      and Db2 and therefore these databases will deliver the best experience
      for building and running TPROC-H. Over time there has been improvement
      with open-source and open-source derived databases in the ability to run
      TPROC-H workloads. For example PostgreSQL supports Parallel Query and
      the PostgreSQL derived versions of Amazon Redshift and Greenplum offer
      further accelerated query solutions. MySQL does not support an analytic
      storage engine however the MariaDB column store storage is best suited
      for running analytic tests against MySQL. Nevertheless it is known that
      with some or all of the open source solutions a number of queries either
      fail or are extremely long running due to the limitations of the
      databases themselves (and not HammerDB) in optimizing the
      queries.</para>

      <section>
        <title>Oracle</title>

        <para>The Oracle database is fully featured for running TPROC-H based
        workloads and presents two options for configuring the database either
        row oriented parallel query or the In-Memory Column Store (IM column
        store). Both of these configurations are able to run a full TPROC-H
        workload and are configured on the database as opposed to configuring
        with HammerDB.</para>
      </section>

      <section>
        <title>Microsoft SQL Server</title>

        <para>SQL Server is able to support a full TPROC-H workload and offers
        row oriented parallel query as well as in-memory column store
        configured. The clustered columnstore build is selected through the
        HammerDB Build Options.</para>

        <figure>
          <title>Clustered Columnstore</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch9-9.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>

      <section>
        <title>Db2</title>

        <para>Db2 can support a full TPCH workload through row oriented
        parallel query and Db2 BLU in-memory column store. The column store is
        selected through the Db2 Organize by options.</para>

        <figure>
          <title>Db2 Organize By</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch9-10.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>

      <section>
        <title>PostgreSQL</title>

        <para>PostgreSQL supports standard row oriented parallel query. This
        offers significant performance improvement over single-threaded
        queries however not all queries at all schema sizes are expected to
        complete without database error and some run for a significant length
        of time. Options are also available to run the PostgreSQL queries
        against a Greenplum database. It is important to be aware that because
        of the Greenplum MPP architecture there is significant overhead in
        processing INSERT operations and therefore data should be loaded in
        bulk after generating with the HammerDB datagen operation.</para>

        <figure>
          <title>PostgreSQL TPROC-H</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch9-11.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>

      <section>
        <title>MySQL</title>

        <para>A standard install of MySQL with the innodb storage engine does
        not support row oriented parallel query or a column store
        configuration and therefore queries run against a MySQL database are
        expected to be long-running. Alternatively HammerDB supports running
        against a MySQL Heatwave column store which offers improved analytic
        performance.</para>

        <figure>
          <title>MySQL TPROC-H</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch9-12.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>

      <section>
        <title>MariaDB</title>

        <para>MariaDB supports a separate installation of a column store based
        database which offers significantly improved query times for most
        queries. This option is selected with the Data Warehouse Storage
        Engine Option set to "Columnstore".</para>

        <figure>
          <title>MariaDB TPROC-H</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="../DocBook/docs/images/ch9-12a.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>
    </section>

    <section>
      <title>Benchmarking Database Cloud Services</title>

      <para>In addition to the TPROC-H workload there are also a set of Cloud
      Analytic Queries made publicly available by Oracle for comparison of
      Cloud Analytic services. These queries run against a derived TPC-H
      schema and are included with HammerDB for running against Oracle, Amazon
      Aurora and Amazon Redshift with Amazon Aurora and Redshift being based
      upon and compatible with MySQL and PostgreSQL respectively. Note however
      that in similarity to MySQL Amazon Aurora does not have the features to
      support analytics such as parallel query or a column store option and
      therefore running the analytic tests against Aurora although possible is
      not likely to generate the best results. Amazon Redshift however is a
      column oriented database based on PostgreSQL and suitable for running
      analytic workloads.</para>

      <para>For the Cloud Analytic workload the Oracle specification requires
      a schema size of 10TB, it is recommended to create the schema with
      HammerDB using the Generating and Bulk Loading Data feature and this
      guide details how to do this for both Oracle and Redshift and this is
      particularly recommended when uploading data to the cloud.</para>

      <para>You are permitted to run both the in-built TPROC-H queries and the
      Cloud Analytic Queries against the same database. This new query set is
      enabled under the TPROC-H Driver Script Options dialog by selecting the
      Cloud Analytic Queries checkbox. This query set reports the geometric
      mean of the completed queries that returns rows for circumstances where
      the query set is run on a scale factor size of less than 10TB. Given the
      similarity of the Oracle implementation to the existing TPROC-H workload
      the following example illustrates running the workload against Amazon
      Redshift.</para>

      <section>
        <title>Redshift Cloud Analytic Workload</title>

        <para>Ensure that your Redshift cluster is active and note your
        Endpoint name given above the cluster properties.</para>

        <figure>
          <title>Redshift console</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch9-13.png"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>Also ensure that access is enabled to the cluster both by
        defining a user and a security group and allowing access through your
        firewall.</para>

        <figure>
          <title>Create Security Group</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch9-14.png"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>Create the TPROC-H schema within Redshift using the HammerDB
        Generating and Bulk Loading Data feature. Under PostgreSQL TPROC-H
        Driver Options use the Redshift Endpoint as your PostgreSQL host and
        5439 as your port. Set the user and password to the credentials you
        have set under the Amazon AWS console. To run the Cloud Analytic
        Workload with HammerDB refer to the following Chapter on How to run an
        Analytic Workload and select the Cloud Analytic Queries and Redshift
        Compatible Checkbox with the reported metric being the geometric mean
        of the query times that complete for the one Virtual User used. Note
        that when running the queries against data sets smaller than the
        specified 10TB this may result in some queries not returning rows,
        therefore for your calculations HammerDB calculates the geometric mean
        only of queries that returned rows.</para>
      </section>
    </section>
  </chapter>

  <chapter>
    <title>How to Run an Analytic Workload</title>

    <para>The basis of Analytic or Decision Support Systems is the ability to
    process complex ad-hoc queries on large volumes of data. Processing this
    amount of data within a single process or thread on traditional
    row-oriented database is time consuming. Consequently it is beneficial
    Parallel Query to break down such queries into multiple sub tasks to
    complex the query more quickly. Additional features such as compression
    and partitioning are also used with Parallel Query to improve performance.
    As a consequence when planning analytic workloads for optimal performance
    you should consider the database features for in-memory and parallel
    execution configuration. In similarity to the HammerDB OLTP workload,
    HammerDB implements a fair usage of a TPC workload however the results
    should not be compared to official published TPC-H results in any
    way.</para>

    <section>
      <title>SUT Database Server Configuration</title>

      <para>For query based workloads there is no requirement for a load
      testing client although you may use one if you wish. It is entirely
      acceptable to run HammerDB directly on the SUT (System Under Test)
      Database system if you wish, the client workload is minimal compared to
      an OLTP workload. In the analytic workload the client sends long running
      queries to the SUT and awaits a response therefore requiring minimal
      resources on the client side. As with an OLTP configuration however the
      database server architecture to be tested must meet the standard
      requirements for a database server system. Similarly the database can be
      installed on any supported operating system, there is no restriction on
      the version of that is required. Before running a HammerDB analytic test
      depending on your configuration you should focus on memory and I/O (disk
      performance). Also in turn the number and type of multi-core and
      multi-threaded processors installed will have a significant impact on
      parallel performance to drive the workload. When using in-memory column
      store features processors that support SIMD/AVX instructions sets are
      also required for the vectorisation of column scans. HammerDB by default
      provides TPROC-H schemas at Scale Factors 1,10,30,100,300 and 1000
      (larger can be configured if required). The Scale Factors correspond to
      the schema size in Gigabytes. As with the official TPROC-H tests the
      results at one schema size should not be compared with the results
      derived with another schema size. As the analytic workload utilizes
      parallel query where available it is possible for a single virtual user
      to use all of the CPU resources on the SUT at any schema size.
      Nevertheless there is still a relation with all of the hardware
      resources available including memory and I/O and a larger system will
      benefit from tests run a larger schema size. The actual sizing of
      hardware resources of hardware resources is beyond the scope of this
      document however at the basic level with traditional parallel execution
      and modern CPU capabilities I/O read performance is typically the
      constraining factor. Note that also in contrast to an OLTP workload high
      throughput transaction log write performance is not a requirement,
      however in similarity to the OLTP workload storage based on SSD disks
      will usually offer significant improvements in performance over standard
      hard disks although in this case it is the benefits of read bandwidth as
      opposed to the IOPs benefits of SSDs for OLTP. When using the in-memory
      column store memory capacity and bandwidth feature and if fully cached
      in memory storage performance is not directly a factor for query
      performance. Nevertheless data loads are an important consideration for
      in-memory data and therefore I/O and SSD read performance remain
      important for loading the data into memory to be available for
      scans.</para>
    </section>

    <section>
      <title>Installation and Configuration</title>

      <para>Before creating a test schema you should ensure that your database
      is configured to process analytic workloads instead of
      transactional.</para>

      <section>
        <title>Oracle</title>

        <para>When your database server is installed you should create a
        tablespace into which the test data will be installed allowing disk
        space according to the guide previously given in this document.</para>

        <programlisting>SQL&gt; create tablespace tpchtab datafile size 20g;

Tablespace created.
</programlisting>

        <para>When using parallel query ensure that the instance is configured
        for parallel execution, noting in particular the value for
        parallel_max_servers.</para>

        <programlisting>SQL&gt; show parameter parallel
NAME                      TYPE        VALUE
------------------------------------ -----------
parallel_max_servers      integer    160
…
parallel_min_servers      integer    16
…
parallel_servers_target   integer    64
parallel_threads_per_cpu  integer    2
</programlisting>

        <para>For testing purposes you can disable parallel execution in a
        particular environment by setting parallel_max_servers to a value of
        zero. An additional parameter that can provide significant benefit to
        the performance of parallel query workloads is
        optimizer_dynamic_sampling. By default this value is set to 2.
        Increasing this value to 4 has been shown to benefit query performance
        however testing the impact of changing this parameter should always be
        done during pre-testing as it may change between Oracle
        releases.</para>

        <programlisting>SQL&gt; alter system set optimizer_dynamic_sampling=4;

System altered.

SQL&gt; show parameter optimizer_dynamic

NAME                       TYPE       VALUE
------------------------------------ ----------- 
optimizer_dynamic_sampling integer    4


</programlisting>

        <para>If using the In-Memory option ensure that the parameter
        inmemory_size has been configured and the database restarted.</para>

        <programlisting>SQL&gt; show parameter inmemory

NAME                             TYPE        VALUE
-------------------------------------------- --------
inmemory_clause_defaultstring
inmemory_force                   string      DEFAULT
inmemory_max_populate_servers    integer     2
inmemory_query                   string      ENABLE
inmemory_size                    big integer 1500M
inmemory_trickle_repopulate      integer     1
_servers_percent    
optimizer_inmemory_aware         boolean     TRUE
</programlisting>

        <para>Then alter the new tablespace containing the schema to be
        in-memory.</para>

        <programlisting>SQL&gt; alter tablespace TPCHTAB default inmemory;
Tablespace altered.
</programlisting>

        <para>As shown the objects created within the tablespace will now be
        configured to be in-memory.</para>

        <para><programlisting>SQL&gt; select tablespace_name, def_inmemory from dba_tablespaces;

TABLESPACE_NAME           DEF_INME
------------------------- --------
SYSTEM                    DISABLED
SYSAUX                    DISABLED
TEMP                      DISABLED
USER                      DISABLED
TPCCTAB                   DISABLED
TPCHTAB                   ENABLED
</programlisting></para>

        <para>For larger schemas both partitioning and compression settings
        (both standard and in-memory) should be considered for query
        tuning.</para>
      </section>

      <section>
        <title>SQL Server</title>

        <para>For SQL Server ensure that the Max Degree of Parallelism is set
        to the maximum number of cores that you wish to process the
        queries.</para>

        <figure>
          <title>Max Degree of Parallelism</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch13-1.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>

      <section>
        <title>Db2</title>

        <para>If using DB2 BLU use db2set to set the parameter DB2_WORKLOAD to
        ANALYTICS.</para>

        <programlisting>[db2inst1 ~]$ db2set DB2_WORKLOAD=ANALYTICS
[db2inst1 ~]$ db2stop force
25/05/2016 10:08:22     0   0   SQL1064N  DB2STOP processing was successful.
SQL1064N  DB2STOP processing was successful.
[db2inst1 ~]$ db2start
05/25/2016 10:08:27     0   0   SQL1063N  DB2START processing was successful.
SQL1063N  DB2START processing was successful.
</programlisting>

        <para>In your db2cli.ini set the following parameter for each of the
        databases that you plan to create to test the TPROC-H workload, this
        will prevent failure due to SQLSTATE 01003 “Null values were
        eliminated from the argument of a column function” when running the
        workload thereby preventing the query set from completing.</para>

        <programlisting>[db2inst1 cfg]$ more db2cli.ini 
[TPCH]
IgnoreWarnList="'01003'"
[TPCH1]
IgnoreWarnList="'01003'"
</programlisting>

        <para>When your database server is installed you should create a
        database into which the test data will be installed, for TPROC-H
        workloads a large pagesize is recommended.</para>

        <programlisting>[db2inst1 ~]$ db2 create database tpch pagesize 32 k
DB20000I  The CREATE DATABASE command completed successfully.
</programlisting>
      </section>

      <section>
        <title>PostgreSQL</title>

        <para>The PostgreSQL Server should be at a minimum level of 9.6 that
        supports Parallel Query. The postgresql.conf file should include
        parallelism specific parameters such as follows:</para>

        <programlisting>work_mem = 1000MB                              
max_worker_processes = 16
max_parallel_workers_per_gather = 16
force_parallel_mode = on

</programlisting>
      </section>

      <section>
        <title>MySQL</title>

        <para>MySQL does not support analytic workloads and therefore there
        are no configuration parameters to optimize the database for such a
        workload.</para>
      </section>

      <section>
        <title>MariaDB</title>

        <para>To test analytic workloads MariaDB columnstore is the solution
        that should be used. Note that this is a separate installation from a
        standard MariaDB installation rather than a pluggable storage engine.
        As MariaDB columnstore is based on a columnstore solution called
        InfiniDB the relevant parameters start with infinidb, for
        example:</para>

        <programlisting># Enable compression by default on create, set to 0 to turn off
infinidb_compression_type=2
# Default for string table threshhold
infinidb_stringtable_threshold=20
# infinidb local query flag
# infinidb_local_query=1
infinidb_diskjoin_smallsidelimit=0
infinidb_diskjoin_largesidelimit=0
infinidb_diskjoin_bucketsize=100
infinidb_um_mem_limit=64000
infinidb_use_import_for_batchinsert=1
infinidb_import_for_batchinsert_delimiter=7</programlisting>
      </section>
    </section>

    <section>
      <title>Configuring Schema Build Options</title>

      <para>To create the analytic test schema based on the TPROC-H you will
      need to select which benchmark and database you wish to use by choosing
      select benchmark from under the Options menu or under the benchmark
      tree-view. The initial settings are determined by the values in your XML
      configuration files. The following example shows the selection of SQL
      Server however the process is the same for all databases.</para>

      <para><figure>
          <title>Benchmark Options</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch13-2.PNG"/>
            </imageobject>
          </mediaobject>
        </figure></para>

      <para>To create the TPROC-H schema select the TPROC-H schema options
      menu tab from the benchmark tree-view or the options menu. This menu
      will change dynamically according to your chosen database.</para>

      <para><figure>
          <title>TPROC-H Schema Build Options</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch13-3.PNG"/>
            </imageobject>
          </mediaobject>
        </figure></para>

      <para>If selected from the Options menu the schema options window is
      divided into two sections. The “Build Options” section details the
      general login information and where the schema will be built and the
      “Driver Options” for the Driver Script to run after the schema is built.
      If selected from the benchmark tree-view only the “Build Options” are
      shown and these are the only options of importance at this stage. Note
      that in any circumstance you do not have to rebuild the schema every
      time you change the “Driver Options”, once the schema has been built
      only the “Driver Options” may need to be modified. For the “Build
      Options” fill in the values according to the database where the schema
      will be built as follows.</para>

      <section>
        <title>Oracle Schema Build Options</title>

        <figure>
          <title>Oracle TPROC-H Build Options</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch13-4.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para><table>
            <title>Oracle Build Options</title>

            <tgroup cols="2">
              <thead>
                <row>
                  <entry align="center">Option</entry>

                  <entry align="center">Description</entry>
                </row>
              </thead>

              <tbody>
                <row>
                  <entry>Oracle Service Name</entry>

                  <entry>The Oracle Service Name is the service name that your
                  load generation server will use to connect to the database
                  running on the SUT database server.</entry>
                </row>

                <row>
                  <entry>System User</entry>

                  <entry>The “system” user or a user with system level
                  privileges</entry>
                </row>

                <row>
                  <entry>System User Password</entry>

                  <entry>The system user password is the password for the
                  “system” user you entered during database creation. The
                  system user already exists in all Oracle databases and has
                  the necessary permissions to create the TPROC-H
                  user.</entry>
                </row>

                <row>
                  <entry>TPROC-H User</entry>

                  <entry>The TPROC-H user is the name of a user to be created
                  that will own the TPROC-H schema. This user can have any
                  name you choose but must not already exist and adhere to the
                  standard rules for naming Oracle users. You may if you wish
                  run the schema creation multiple times and have multiple
                  TPROC-H schemas created with ownership under a different
                  user you create each time.</entry>
                </row>

                <row>
                  <entry>TPROC-H User Password</entry>

                  <entry>The TPROC-H user password is the password to be used
                  for the TPROC-H user you create and must adhere to the
                  standard rules for Oracle user password. You will need to
                  remember the TPROC-H user name and password for running the
                  TPROC-H driver script after the schema is built.</entry>
                </row>

                <row>
                  <entry>TPROC-H Default Tablespace</entry>

                  <entry>The TPROC-H default tablespace is the tablespace that
                  will be the default for the TPROC-H user and therefore the
                  tablespace to be used for the schema creation. The
                  tablespace must have sufficient free space for the schema to
                  be created.</entry>
                </row>

                <row>
                  <entry>TPROC-H Temporary Tablespace</entry>

                  <entry>The TPROC-H temporary tablespace is the temporary
                  tablespace that already exists in the database to be used by
                  the TPROC-H User.</entry>
                </row>

                <row>
                  <entry>TimesTen Database Compatible</entry>

                  <entry>When selected this option means that the Oracle
                  Service Name should be a TimesTen Data Source Name and will
                  grey out non-compatible options.</entry>
                </row>

                <row>
                  <entry>Scale Factor</entry>

                  <entry>The Scale Factor is selected by a radio button with a
                  choice of scale factors of 1,10,30,100,300 and 1000
                  corresponding to 1GB, 10GB, 30GB,100GB and 1000GB
                  respectively, larger schema sizes can also be created with
                  the datagen option. Note that the required space will be
                  larger than these values due to the indexes
                  required.</entry>
                </row>

                <row>
                  <entry>Virtual Users to Build Schema</entry>

                  <entry>The Virtual Users to Build Schema is the number of
                  Virtual Users to be created on the Load Generation Server
                  that will complete your multi-threaded schema build. You
                  should set this value to the number of cores on your Load
                  Generation Server or SUT if HammerDB is running
                  there.</entry>
                </row>
              </tbody>
            </tgroup>
          </table></para>
      </section>

      <section>
        <title>SQL Server Schema Build Options</title>

        <figure>
          <title>SQL Server Build Options</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch13-5.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para><table>
            <title>SQL Server Build Options</title>

            <tgroup cols="2">
              <thead>
                <row>
                  <entry align="center">Option</entry>

                  <entry align="center">Description</entry>
                </row>
              </thead>

              <tbody>
                <row>
                  <entry>SQL Server</entry>

                  <entry>The Microsoft SQL Server is the host name or host
                  name and instance where the TPROC-H database will be
                  created.</entry>
                </row>

                <row>
                  <entry>TCP</entry>

                  <entry>Use the TCP Protocol</entry>
                </row>

                <row>
                  <entry>SQL Server Port</entry>

                  <entry>When TCP is enabled, the SQL Server Port is the
                  network port that your load generation server will use to
                  connect to the database running on the SUT database server.
                  In most cases this will be the default port of 1433 and will
                  not need to be changed.</entry>
                </row>

                <row>
                  <entry>Azure</entry>

                  <entry>Include the Database name in the connect string
                  typical of Azure connections. To successfully build the
                  schema this database must be created and empty.</entry>
                </row>

                <row>
                  <entry>Encrypt Connection</entry>

                  <entry>Provides the same functionality as the "Encrypt
                  Connection" option in the SQL Server Management Studio
                  connect dialog. This option is the default with ODBC Driver
                  18.</entry>
                </row>

                <row>
                  <entry>Trust Server Certificate</entry>

                  <entry>Provides the same functionality as the "Trust Server
                  Certificate" option in the SQL Server Management Studio
                  connect dialog. This option is the default with ODBC Driver
                  18.</entry>
                </row>

                <row>
                  <entry>SQL Server ODBC Driver</entry>

                  <entry>The Microsoft SQL ODBC Driver is the ODBC driver you
                  will use to connect to the SQL Server database. To view
                  which drivers are available on Windows view the ODBC Data
                  Source Administrator Tool.</entry>
                </row>

                <row>
                  <entry>Authentication</entry>

                  <entry>When installing SQL Server on Windows you will have
                  configured SQL Server for Windows or Windows and SQL Server
                  Authentication. On Linux you will be using SQL Server
                  Authentication. If you specify Windows Authentication then
                  SQL Server will use a trusted connection to your SQL Server
                  using your Windows credentials without requiring a username
                  and password. If SQL Server Authentication is specified and
                  SQL Authentication is enabled on your SQL Server then you
                  will be able connect by specifying a username and password
                  that you have already configured on your SQL Server. From
                  v4.10 Microsoft Entra Authentication can also be used with a
                  minimum of ODBC Driver 18 and SQL Server 2022. Where Entra
                  is used an MSI Object ID can also be specified. If Entra is
                  chosen and the MSI Object ID is set to null then Interactive
                  Entra Authentication will be used prompting for login
                  credentials.</entry>
                </row>

                <row>
                  <entry>SQL Server User ID</entry>

                  <entry>The SQL Server User ID is the User ID of a user that
                  you have already created on your SQL Server.</entry>
                </row>

                <row>
                  <entry>SQL Server User Password</entry>

                  <entry>The SQL Server User Password is the Password
                  configured on the SQL Server for the User ID you have
                  specified. Note that when configuring the password on the
                  SQL Server there is a checkbox that when selected enforces
                  more complex rules for passwords or if unchecked enables a
                  simple password such as “admin”.</entry>
                </row>

                <row>
                  <entry>SQL Server TPCH Database</entry>

                  <entry>The SQL Server Database is the name of the Database
                  to be created on the SQL Server to contain the schema. If
                  this database does not already exist then HammerDB will
                  create it, if the database does already exist and the
                  database is empty then HammerDB will use this existing
                  database. Therefore if you wish to create a particular
                  layout or schema then pre-creating the database and using
                  this database is an advanced method to use this
                  configuration.</entry>
                </row>

                <row>
                  <entry>MAXDOP</entry>

                  <entry>The MAXDOP setting defines the maximum degree of
                  parallelism to be set as a default on the schema
                  objects.</entry>
                </row>

                <row>
                  <entry>Clustered Columnstore</entry>

                  <entry>This option selects the database to be created with
                  in-memory clustered columnstore indexes.</entry>
                </row>

                <row>
                  <entry>Scale Factor</entry>

                  <entry>The Scale Factor is selected by a radio button with a
                  choice of scale factors of 1,10,30,100,300 and 1000
                  corresponding to 1GB, 10GB, 30GB,100GB and 1000GB
                  respectively, larger schema sizes can also be created with
                  the datagen option. Note that the required space will be
                  larger than these values due to the indexes
                  required.</entry>
                </row>

                <row>
                  <entry>Virtual Users to Build Schema</entry>

                  <entry>The Virtual Users to Build Schema is the number of
                  Virtual Users to be created on the Load Generation Server
                  that will complete your multi-threaded schema build. You
                  should set this value to the number of cores on your Load
                  Generation Server or SUT if HammerDB is running
                  there.</entry>
                </row>

                <row>
                  <entry>Use BCP Option</entry>

                  <entry>This option uses the SQL Server BCP utility to bulk
                  load data. Temporary staging files will be created and
                  deleted in the location defined by the TEMP environment
                  variable. When unselected an insert based build is used. A
                  BCP build offers vastly improved performance over an insert
                  based build.</entry>
                </row>

                <row>
                  <entry>Partition Orders and Lineitems</entry>

                  <entry>This option allows the Order and Lineitem tables to
                  be partitioned. Each partition spans 1 week. Column Store
                  and Row store are allowed to be partitioned. When
                  paritioning is enabled certain primary and foreign key
                  definitions defined in HammerDB are no longer valid.
                  HammerDB will still create those keys/constraints when the
                  Orders and Lineitem tables are not partitioned. For
                  columnstore indices, the is an initial degredation of
                  performance, however if the user calls Alter Index ...
                  Reoganize or leaves the server running after the initial
                  load for approximately 48 hours, the performance of power
                  runs improves by appx 2x when compared to column store
                  indices with no partitioning.</entry>
                </row>

                <row>
                  <entry>Create Advanced Statistics</entry>

                  <entry>Perform a more advanced statistical analysis
                  benefiting performance at the expense of additional load
                  time.</entry>
                </row>
              </tbody>
            </tgroup>
          </table></para>
      </section>

      <section>
        <title>Db2 Schema Build Options</title>

        <figure>
          <title>Db2 Build Options</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch13-6.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para><table>
            <title>Db2 Build Options</title>

            <tgroup cols="2">
              <thead>
                <row>
                  <entry align="center">Option</entry>

                  <entry align="center">Description</entry>
                </row>
              </thead>

              <tbody>
                <row>
                  <entry>Db2 User</entry>

                  <entry>The name of the operating system user to connect to
                  the DB2 database for example db2inst1.</entry>
                </row>

                <row>
                  <entry>Db2 Password</entry>

                  <entry>The password for the operating system DB2 user by
                  default “ibmdb2”</entry>
                </row>

                <row>
                  <entry>Db2 Database</entry>

                  <entry>The name of the Db2 database that you have already
                  created, for example “tpcc”</entry>
                </row>

                <row>
                  <entry>Db2 Default Tablespace</entry>

                  <entry>The name of the existing tablespace where tables
                  should be located if a specific tablespace has not been
                  defined for that table in the tablespace list. The default
                  is “USERSPACE1”.</entry>
                </row>

                <row>
                  <entry>Db2 Organize By</entry>

                  <entry>The Organize by option is selected by a radio button
                  and determines an optional organize by clause to be
                  specified when creating the tables. The database version
                  must be able to accept the option chosen and therefore the
                  recommended choice is NONE to accept the defaults. When the
                  setting DB2_WORKLOAD is set to analytics for example the
                  default is configuration is for columnar storage. If for
                  example this parameter is set you can then choose ROW
                  configuration even when DB2_WORKLOAD is set to analytics to
                  create row organized tables. The DATE option is mutually
                  exclusive to the column store option however creates a ROW
                  organized table that is organized by date which can
                  accelerate some queries when row organized.</entry>
                </row>

                <row>
                  <entry>Scale Factor</entry>

                  <entry>The Scale Factor is selected by a radio button with a
                  choice of scale factors of 1,10,30,100,300 and 1000
                  corresponding to 1GB, 10GB, 30GB,100GB and 1000GB
                  respectively, larger schema sizes can also be created with
                  the datagen option. Note that the required space will be
                  larger than these values due to the indexes
                  required.</entry>
                </row>

                <row>
                  <entry>Virtual Users to Build Schema</entry>

                  <entry>The Virtual Users to Build Schema is the number of
                  Virtual Users to be created on the Load Generation Server
                  that will complete your multi-threaded schema build. You
                  should set this value to the number of cores on your Load
                  Generation Server or SUT if HammerDB is running
                  there.</entry>
                </row>
              </tbody>
            </tgroup>
          </table></para>
      </section>

      <section>
        <title>MySQL Schema Build Options</title>

        <para><figure>
            <title>MySQL Build Options</title>

            <mediaobject>
              <imageobject>
                <imagedata fileref="docs/images/ch13-8.PNG"/>
              </imageobject>
            </mediaobject>
          </figure></para>

        <para><table>
            <title>MySQL Build Options</title>

            <tgroup cols="2">
              <thead>
                <row>
                  <entry align="center">Option</entry>

                  <entry align="center">Description</entry>
                </row>
              </thead>

              <tbody>
                <row>
                  <entry>MySQL Host</entry>

                  <entry>The MySQL Host Name is the host name of the SUT
                  database server.</entry>
                </row>

                <row>
                  <entry>MySQL Port</entry>

                  <entry>The MySQL Port is the network port on the SUT
                  database server. In most cases this will be the default port
                  of 3306.</entry>
                </row>

                <row>
                  <entry>MySQL User</entry>

                  <entry>The MySQL User is the user which has permission to
                  create a database and you previously granted access to from
                  the load generation server. The root user already exists in
                  all MySQL databases and has the necessary permissions to
                  create the TPROC-H database.</entry>
                </row>

                <row>
                  <entry>MySQL User Password</entry>

                  <entry>The MySQL user password is the password for the user
                  defined as the MySQL User. You will need to remember the
                  MySQL user name and password for running the TPROC-H driver
                  script after the database is built.</entry>
                </row>

                <row>
                  <entry>MySQL Database</entry>

                  <entry>The MySQL Database is the database that will be
                  created containing the TPROC-H schema creation. There must
                  have sufficient free space for the database to be
                  created.</entry>
                </row>

                <row>
                  <entry>Data Warehouse Storage Engine</entry>

                  <entry>Use the "show engine" command to display available
                  storage engines and select a storage engine that supports
                  analytics. For MariaDB columnstore specify.
                  "Columnstore"</entry>
                </row>

                <row>
                  <entry>Scale Factor</entry>

                  <entry>The Scale Factor is selected by a radio button with a
                  choice of scale factors of 1,10,30,100,300 and 1000
                  corresponding to 1GB, 10GB, 30GB,100GB and 1000GB
                  respectively, larger schema sizes can also be created with
                  the datagen option. Note that the required space will be
                  larger than these values due to the indexes
                  required.</entry>
                </row>

                <row>
                  <entry>Virtual Users to Build Schema</entry>

                  <entry>The Virtual Users to Build Schema is the number of
                  Virtual Users to be created on the Load Generation Server
                  that will complete your multi-threaded schema build. You
                  should set this value to the number of cores on your Load
                  Generation Server or SUT if HammerDB is running
                  there.</entry>
                </row>
              </tbody>
            </tgroup>
          </table></para>
      </section>

      <section>
        <title>MariaDB Schema Build Options</title>

        <para><figure>
            <title>MariaDB Build Options</title>

            <mediaobject>
              <imageobject>
                <imagedata fileref="docs/images/ch13-8a.PNG"/>
              </imageobject>
            </mediaobject>
          </figure></para>

        <para><table>
            <title>MySQL Build Options</title>

            <tgroup cols="2">
              <thead>
                <row>
                  <entry align="center">Option</entry>

                  <entry align="center">Description</entry>
                </row>
              </thead>

              <tbody>
                <row>
                  <entry>MariaDB Host</entry>

                  <entry>The MariaDB Host Name is the host name of the SUT
                  database server.</entry>
                </row>

                <row>
                  <entry>MariaDB Port</entry>

                  <entry>The MariaDB Port is the network port on the SUT
                  database server. In most cases this will be the default port
                  of 3306.</entry>
                </row>

                <row>
                  <entry>MariaDB User</entry>

                  <entry>The MariaDB User is the user which has permission to
                  create a database and you previously granted access to from
                  the load generation server. The root user already exists in
                  all MariaDB databases and has the necessary permissions to
                  create the TPROC-H database.</entry>
                </row>

                <row>
                  <entry>MariaDB User Password</entry>

                  <entry>The MariaDB user password is the password for the
                  user defined as the MariaDB User. You will need to remember
                  the MariaDB user name and password for running the TPROC-H
                  driver script after the database is built.</entry>
                </row>

                <row>
                  <entry>MariaDB Database</entry>

                  <entry>The MariaDB Database is the database that will be
                  created containing the TPROC-H schema creation. There must
                  have sufficient free space for the database to be
                  created.</entry>
                </row>

                <row>
                  <entry>Data Warehouse Storage Engine</entry>

                  <entry>Use the "show engine" command to display available
                  storage engines and select a storage engine that supports
                  analytics. For MariaDB columnstore specify.
                  "Columnstore"</entry>
                </row>

                <row>
                  <entry>Scale Factor</entry>

                  <entry>The Scale Factor is selected by a radio button with a
                  choice of scale factors of 1,10,30,100,300 and 1000
                  corresponding to 1GB, 10GB, 30GB,100GB and 1000GB
                  respectively, larger schema sizes can also be created with
                  the datagen option. Note that the required space will be
                  larger than these values due to the indexes
                  required.</entry>
                </row>

                <row>
                  <entry>Virtual Users to Build Schema</entry>

                  <entry>The Virtual Users to Build Schema is the number of
                  Virtual Users to be created on the Load Generation Server
                  that will complete your multi-threaded schema build. You
                  should set this value to the number of cores on your Load
                  Generation Server or SUT if HammerDB is running
                  there.</entry>
                </row>
              </tbody>
            </tgroup>
          </table></para>
      </section>

      <section>
        <title>PostgreSQL Schema Build Options</title>

        <figure>
          <title>PostgreSQL Build Options</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch13-7.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para><table>
            <title>PostgreSQL Build Options</title>

            <tgroup cols="2">
              <thead>
                <row>
                  <entry align="center">Option</entry>

                  <entry align="center">Description</entry>
                </row>
              </thead>

              <tbody>
                <row>
                  <entry>PostgreSQL Host</entry>

                  <entry>The host name of the SUT running PostgreSQL.</entry>
                </row>

                <row>
                  <entry>PostgreSQL Port</entry>

                  <entry>The port of the PostgreSQL service. By default this
                  will be 5432 for a standard PostgreSQL installation or 5444
                  for EnterpriseDB.</entry>
                </row>

                <row>
                  <entry>PostgreSQL Superuser</entry>

                  <entry>The PostgreSQL Superuser is a user with sufficient
                  privileges to create both new users (roles) and databases to
                  enable the creation of the test schema.</entry>
                </row>

                <row>
                  <entry>PostgreSQL Superuser Password</entry>

                  <entry>The PostgreSQL Superuser Password is the password for
                  the PostgreSQL superuser which will have been defined during
                  installation. If you have forgotten the password it can be
                  reset from a psql prompt that has logged in from a trusted
                  connection therefore requiring no password using postgres=#
                  alter role postgres password ‘postgres’;</entry>
                </row>

                <row>
                  <entry>PostgreSQL Default Database</entry>

                  <entry>The PostgreSQL default databases is the database to
                  specify for the superuser connection. Typically this will be
                  postgres for a standard PostgreSQL installation or edb for
                  EnterpriseDB.</entry>
                </row>

                <row>
                  <entry>PostgreSQL User</entry>

                  <entry>The PostgreSQL User is the user (role) that will be
                  created that owns the database containing the TPROC-H
                  schema.</entry>
                </row>

                <row>
                  <entry>PostgreSQL User Password</entry>

                  <entry>The PostgreSQL User Password is the password that
                  will be specified for the PostgreSQL user when it is
                  created.</entry>
                </row>

                <row>
                  <entry>PostgreSQL Database</entry>

                  <entry>The PostgreSQL Database is the database that will be
                  created and owned by the PostgreSQL User that contains the
                  TPROC-H schema.</entry>
                </row>

                <row>
                  <entry>PostgreSQL Tablespace</entry>

                  <entry>The PostgreSQL Tablespace where the PostgreSQL
                  Database will be installed.</entry>
                </row>

                <row>
                  <entry>Prefer PostgreSQL SSL Mode</entry>

                  <entry>If both the PostgreSQL client and server have been
                  compiled to support SSL this option when selected enables
                  "prefer" SSL, when unselected it sets this option to
                  "disable" for connections. Other valid options such as
                  "allow" or "require" can be set directly in the build and
                  driver scripts if required.</entry>
                </row>

                <row>
                  <entry>Greenplum Database Compatible</entry>

                  <entry>Choosing Greenplum Database Compatible creates a
                  schema with Greenplum Database Options. Building the schema
                  by inserting into Greenplum is not recommended and instead a
                  bulk load of data created with the datagen option should be
                  used.</entry>
                </row>

                <row>
                  <entry>Greenplum Compressed Columns</entry>

                  <entry>Becomes active when Greenplum Database Compatible is
                  selected and configures the columns in a compressed
                  format.</entry>
                </row>

                <row>
                  <entry>Scale Factor</entry>

                  <entry>The Scale Factor is selected by a radio button with a
                  choice of scale factors of 1,10,30,100,300 and 1000
                  corresponding to 1GB, 10GB, 30GB,100GB and 1000GB
                  respectively, larger schema sizes can also be created with
                  the datagen option. Note that the required space will be
                  larger than these values due to the indexes
                  required.</entry>
                </row>

                <row>
                  <entry>Virtual Users to Build Schema</entry>

                  <entry>The Virtual Users to Build Schema is the number of
                  Virtual Users to be created on the Load Generation Server
                  that will complete your multi-threaded schema build. You
                  should set this value to the number of cores on your Load
                  Generation Server or SUT if HammerDB is running
                  there.</entry>
                </row>
              </tbody>
            </tgroup>
          </table></para>
      </section>
    </section>

    <section>
      <title>Creating the Schema</title>

      <para>To begin the schema creation select the Build Option from the
      tree-view.</para>

      <figure>
        <title>Build TPROC-H Schema</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch13-9.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>A dialog box is shown to confirm the options selected.</para>

      <figure>
        <title>Create Schema</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch13-10.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>When you click Yes HammerDB will login to your chosen service name
      with a monitor thread as the system user and create the user with the
      password you have chosen. It will then log out and log in again as your
      chosen user, create the tables and then load the region and nation table
      data before waiting and monitoring the other threads. The worker threads
      will wait for the monitor thread to complete its initial work.
      Subsequently the worker threads will create and insert the data for
      their assigned warehouses. There are no intermediate data files or
      manual builds required, HammerDB will both create and load your
      requested data dynamically. Data is inserted in a batch format for
      optimal performance, however for larger schemas doing a bulk load of
      data created with the datagen feature will be a faster way to create the
      schema as it bypasses the logging and consistency checks of the insert
      based load.</para>

      <figure>
        <title>Schema Build Start</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch13-11.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>When the workers are complete the monitor thread will create the
      indexes and gather the statistics. When complete Virtual User 1 will
      display the message TPCH SCHEMA COMPLETE and all virtual users will show
      that they completed their action successfully. Pressing the red button
      will destroy the Virtual Users.</para>

      <figure>
        <title>Schema Build Complete</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch13-12.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <section>
        <title>Verifying and Backing-Up the Oracle Schema</title>

        <para>The schema build is now complete with the following tables
        created and populated. Note that in example below the tables have
        inherited the tablespaces’s in-memory configuration without additional
        settings. If required the inmemory_priority can also be set at this
        point in time.</para>

        <programlisting>  1* select table_name, num_rows, inmemory, inmemory_priority from user_tables
SQL&gt; /

TABLE_NAME                       NUM_ROWS INMEMORY INMEMORY_PRIORITY
------------------------------ ---------- -------- -----------------
ORDERS                            1500000 ENABLED  NONE
PARTSUPP                           800000 ENABLED  NONE
CUSTOMER                           150000 ENABLED  NONE
PART                               200000 ENABLED  NONE
SUPPLIER                            10000 ENABLED  NONE
NATION                                 25 ENABLED  NONE
REGION5                                 5 ENABLED  NONE
LINEITEM                          6003632 ENABLED  NONE

</programlisting>

        <para>You can verify the contents with SQL*PLUS as the newly created
        user.</para>

        <programlisting>SQL&gt; select tname, tabtype from tab;

TNAME                          TABTYPE
------------------------------ -------
CUSTOMER                       TABLE
LINEITEM                       TABLE
NATION                         TABLE
ORDERS                         TABLE
PART                           TABLE
PARTSUPP                       TABLE
REGION                         TABLE
SUPPLIER                       TABLE

8 rows selected.

SQL&gt; select * from customer where rownum = 1;

 C_CUSTKEY C_MKTSEGME C_NATIONKEY C_NAME
---------- ---------- ----------- -------------------------
C_ADDRESS C_PHONE  C_ACCTBAL
---------------------------------------- --------------- ----------
C_COMMENT
--------------------------------------------------------------------------------
    112098 AUTOMOBILE       22 Customer#000112098
v,QXkbT8YhhyQYXjX4Ag3iFPQq0gbfZNo7 776-160-1375    5010.19
carefully pending instructions detect slyly-- pending deposits acco

SQL&gt; select index_name, index_type from ind;

INDEX_NAME       INDEX_TYPE
------------------------------
REGION_PK        NORMAL
NATION_PK        NORMAL
SUPPLIER_PK      NORMAL
PARTSUPP_PK      NORMAL
PART_PK          NORMAL
ORDERS_PK        NORMAL
LINEITEM_PK      NORMAL
CUSTOMER_PK      NORMAL

8 rows selected.
</programlisting>

        <para>At this point the data creation is complete and you are ready to
        start running a performance test. Before doing so note that as this is
        a query based workload you have the potential to run multiple tests
        and it will return the same results as the data is not modified during
        tests, however there is one exception, under the Driver Options the
        option to choose a Refresh Function. Further details will be given on
        the refresh function in the next section however at this point it is
        sufficient to note that the refresh function when enabled will modify
        data and no two same refresh functions can be run on the same data
        set. This means if you choose to include a refresh function and then
        attempt to re-run the test you will receive an error as the data has
        been modified. This means you have a number of options. Firstly (and
        recommended) you can use datapump to backup and restore your schema.
        To do this create a directory as follows to reference a file system
        folder you have already created (or use the pre-existing
        DATA_PUMP_DIR)</para>

        <programlisting>SQL&gt; create directory dump_dir1 as '/u02/app/oracle/dumpdir';

Directory created.

Then use datapump to export your schema to this directory before you have run any workloads with a refresh function:
[oracle@MERLIN oracle]$ expdp \"sys/oracle@pdb1 as sysdba\" schemas=tpch content=all directory=DUMP_DIR1 logfile=dp.log

Export: Release 12.1.0.2.0 - Production on Wed Sep 17 11:23:32 2014

Copyright (c) 1982, 2014, Oracle and/or its affiliates.  All rights reserved.

Connected to: Oracle Database 12c Enterprise Edition Release 12.1.0.2.0 - 64bit Production
With the Partitioning, OLAP, Advanced Analytics and Real Application Testing options
Starting "SYS"."SYS_EXPORT_SCHEMA_01":  "sys/********@pdb1 AS SYSDBA" schemas=tpch content=all directory=DUMP_DIR1 logfile=dp.log 
Estimate in progress using BLOCKS method...
Processing object type SCHEMA_EXPORT/TABLE/TABLE_DATA
Total estimation using BLOCKS method: 1.159 GB
Processing object type SCHEMA_EXPORT/USER
Processing object type SCHEMA_EXPORT/SYSTEM_GRANT
Processing object type SCHEMA_EXPORT/ROLE_GRANT
Processing object type SCHEMA_EXPORT/DEFAULT_ROLE
Processing object type SCHEMA_EXPORT/TABLESPACE_QUOTA
Processing object type SCHEMA_EXPORT/PRE_SCHEMA/PROCACT_SCHEMA
Processing object type SCHEMA_EXPORT/TABLE/TABLE
Processing object type SCHEMA_EXPORT/TABLE/INDEX/INDEX
Processing object type SCHEMA_EXPORT/TABLE/CONSTRAINT/CONSTRAINT
Processing object type SCHEMA_EXPORT/TABLE/INDEX/STATISTICS/INDEX_STATISTICS
Processing object type SCHEMA_EXPORT/TABLE/CONSTRAINT/REF_CONSTRAINT
Processing object type SCHEMA_EXPORT/TABLE/STATISTICS/TABLE_STATISTICS
Processing object type SCHEMA_EXPORT/STATISTICS/MARKER
. . exported "TPCH"."LINEITEM"                           746.2 MB 6003632 rows
. . exported "TPCH"."ORDERS"                             165.4 MB 1500000 rows
. . exported "TPCH"."PARTSUPP"                           112.7 MB  800000 rows
. . exported "TPCH"."PART"                               25.99 MB  200000 rows
. . exported "TPCH"."CUSTOMER"                           23.45 MB  150000 rows
. . exported "TPCH"."SUPPLIER"                           1.430 MB   10000 rows
. . exported "TPCH"."NATION"                             9.125 KB      25 rows
. . exported "TPCH"."REGION"                             6.476 KB       5 rows
Master table "SYS"."SYS_EXPORT_SCHEMA_01" successfully loaded/unloaded
******************************************************************************
Dump file set for SYS.SYS_EXPORT_SCHEMA_01 is:
  /u02/app/oracle/dumpdir/expdat.dmp
Job "SYS"."SYS_EXPORT_SCHEMA_01" successfully completed at Wed Sep 17 11:24:10 2014 elapsed 0 00:00:36
</programlisting>

        <para>After you have run a workload with a refresh function drop the
        TPROC-H user as follows:</para>

        <programlisting>SQL&gt; drop user tpch cascade;

User dropped.
</programlisting>

        <para>Then re-import the export file you took prior to running the
        refresh function:</para>

        <programlisting>[oracle@MERLIN oracle]$ impdp \"sys/oracle@pdb1 as sysdba\" schemas=tpch content=all directory=DUMP_DIR1 logfile=dp1.log

Import: Release 12.1.0.2.0 - Production on Wed Sep 17 11:37:54 2014

Copyright (c) 1982, 2014, Oracle and/or its affiliates.  All rights reserved.

Connected to: Oracle Database 12c Enterprise Edition Release 12.1.0.2.0 - 64bit Production
With the Partitioning, OLAP, Advanced Analytics and Real Application Testing options
Master table "SYS"."SYS_IMPORT_SCHEMA_04" successfully loaded/unloaded
Starting "SYS"."SYS_IMPORT_SCHEMA_04":  "sys/********@pdb1 AS SYSDBA" schemas=tpch content=all directory=DUMP_DIR1 logfile=dp1.log 
Processing object type SCHEMA_EXPORT/USER
Processing object type SCHEMA_EXPORT/SYSTEM_GRANT
Processing object type SCHEMA_EXPORT/ROLE_GRANT
Processing object type SCHEMA_EXPORT/DEFAULT_ROLE
Processing object type SCHEMA_EXPORT/TABLESPACE_QUOTA
Processing object type SCHEMA_EXPORT/PRE_SCHEMA/PROCACT_SCHEMA
Processing object type SCHEMA_EXPORT/TABLE/TABLE
Processing object type SCHEMA_EXPORT/TABLE/TABLE_DATA
. . imported "TPCH"."LINEITEM"                           746.2 MB 6003632 rows
. . imported "TPCH"."ORDERS"                             165.4 MB 1500000 rows
. . imported "TPCH"."PARTSUPP"                           112.7 MB  800000 rows
. . imported "TPCH"."PART"                               25.99 MB  200000 rows
. . imported "TPCH"."CUSTOMER"                           23.45 MB  150000 rows
. . imported "TPCH"."SUPPLIER"                           1.430 MB   10000 rows
. . imported "TPCH"."NATION"                             9.125 KB      25 rows
. . imported "TPCH"."REGION"                             6.476 KB       5 rows
Processing object type SCHEMA_EXPORT/TABLE/INDEX/INDEX
Processing object type SCHEMA_EXPORT/TABLE/CONSTRAINT/CONSTRAINT
Processing object type SCHEMA_EXPORT/TABLE/INDEX/STATISTICS/INDEX_STATISTICS
Processing object type SCHEMA_EXPORT/TABLE/CONSTRAINT/REF_CONSTRAINT
Processing object type SCHEMA_EXPORT/TABLE/STATISTICS/TABLE_STATISTICS
Processing object type SCHEMA_EXPORT/STATISTICS/MARKER
Job "SYS"."SYS_IMPORT_SCHEMA_04" successfully completed at Wed Sep 17 11:38:40 2014 elapsed 0 00:00:44
</programlisting>

        <para>You only need to export once and can then re-import as many
        times as you wish to run the successfully refresh function.</para>

        <para>Secondly another option you have is to use dbms_metadata to
        capture the table definitions and then use SQL*Loader to export and
        import the data using the datagen created data. Finally if you have
        the flashback table feature enabled you can note the time that you
        start running a test with a refresh function and then flashback the
        LINEITEM and ORDERS table to their previous state before the test, for
        example:</para>

        <programlisting>flashback table lineitem to timestamp TO_TIMESTAMP('17-SEP-14 11.41.00.00 AM')</programlisting>

        <para>Whichever method you use, ensure that if you wish to run the
        refresh function you are prepared to restore your schema to the
        previous state before running subsequent tests.</para>
      </section>

      <section>
        <title>Verifying and Backing Up the SQL Server Schema</title>

        <para>Once created you can verify the schema with SSMS or
        sqlcmd.</para>

        <programlisting>C:\Users&gt;sqlcmd -S (local)\SQLDEVELOP -E -Q "use tpch; select name from sys.tables"
Changed database context to 'tpch'.
name

--------------------------------------------------------------------------------
customer
lineitem
nation
part
partsupp
region
supplier
orders

(8 rows affected)

</programlisting>

        <para>At this point the data creation is complete and you are ready to
        start running a performance test. Before doing so note that as this is
        a query based workload you have the potential to run multiple tests
        and it will return the same results as the data is not modified during
        tests, however there is one exception, under the Driver Options the
        option to choose a Refresh Function. Further details will be given on
        the refresh function in the next section however at this point it is
        sufficient to note that the refresh function when enabled will modify
        data and no two same refresh functions can be run on the same data
        set. This means if you choose to include a refresh function and then
        attempt to re-run the test you will receive an error as the data has
        been modified. Therefore you should backup your schema before running
        a workload.</para>

        <figure>
          <title>Backup SQL Server</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch13-13.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>SQL Server will notify when the backup is successful.</para>

        <figure>
          <title>Backup successful</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch13-14.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>The restore can also be selected from the tasks option.</para>

        <figure>
          <title>Restore SQL Server</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch13-15.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>and notification given when the database is restored.</para>

        <figure>
          <title>Restore successful</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch13-16.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>

      <section>
        <title>Verifying and Backing up the Db2 Schema</title>

        <para>The schema build is now complete with the following tables
        created and populated with the example showing a scale factor 10
        schema.</para>

        <programlisting>db2 =&gt; select tabname, card from syscat.tables where tabschema = 'DB2INST1'

TABNAME                                            CARD  
---------------------------------- --------------------
CUSTOMER                                        1500000
LINEITEM                                       60001587
NATION                                               25
ORDERS                                         15000000
PART                                            2000000
PARTSUPP                                        8000000
REGION                                                5
SUPPLIER                                         100000

8 record(s) selected.

db2 =&gt; select * from customer fetch first row only

C_CUSTKEY   C_NAME                    C_ADDRESS     C_NATIONKEY C_PHONE         C_ACCTBAL             C_MKTSEGMENT C_COMMENT                                                                                                             
----------- ------------------------- ---------------------------------------- ----------- --------------- ------------------------ ------------ -------------------------------------------------------
    1128378 Customer#001128378        OzpJsusYMT              6 651-964-1273    +9.57891000000000E+003 HOUSEHOLD    ironic requests above the furiously special foxes wake                                                                

  1 record(s) selected.

db2 =&gt; 

</programlisting>

        <para>At this point the data creation is complete and you are ready to
        start running a performance test. Before doing so note that as this is
        a query based workload you have the potential to run multiple tests
        and it will return the same results as the data is not modified during
        tests, however there is one exception, under the Driver Options the
        option to choose a Refresh Function. Further details will be given on
        the refresh function in the next section however at this point it is
        sufficient to note that the refresh function when enabled will modify
        data and no two same refresh functions can be run on the same data
        set. This means if you choose to include a refresh function and then
        attempt to re-run the test you will receive an error as the data has
        been modified. This means that it is strongly recommended to backup or
        export your data before running a refresh function to ensure that if
        you wish to run the refresh function multiple times you are prepared
        to restore your schema to the previous state before running subsequent
        tests.</para>

        <programlisting>$ db2 backup db tpch to /opt/db2/backup

Backup successful. The timestamp for this backup image is : 20180417181222

$ db2 restore db tpch from /opt/db2/backup replace existing
SQL2539W  The specified name of the backup image to restore is the same as the
name of the target database.  Restoring to an existing database that is the
same as the backup image database will cause the current database to be
overwritten by the backup version.
DB20000I  The RESTORE DATABASE command completed successfully.

</programlisting>

        <para>It is useful to reiterate that before running a query test for
        all configurations you must make the following setting in the
        sb2cli.ini file.</para>

        <programlisting>[db2inst1 cfg]$ more db2cli.ini 
[TPCH]
IgnoreWarnList="'01003'"

</programlisting>

        <para>Failure to add this setting for each virtual user will result in
        the query set failing with the following error.</para>

        <programlisting>Error in Virtual User 3: [IBM][CLI Driver][DB2/NT64] SQLSTATE 01003: Null values were eliminated from the argument of a column function. </programlisting>
      </section>

      <section>
        <title>Verifying and Backing up the MySQL Schema</title>

        <para>MySQL does not directly support analytic workloads, however
        where used the same verification and backup methodology can be used as
        given for MariaDB.</para>
      </section>

      <section>
        <title>Verifying and Backing up the MariaDB Schema</title>

        <para>The schema build is now complete with the following tables
        created and populated with the example showing a scale factor 1
        schema.</para>

        <programlisting>MariaDB [(none)]&gt; show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| mysql              |
| performance_schema |
| tpch               |
+--------------------+
4 rows in set (0.00 sec)

MariaDB [(none)]&gt; use tpch;
Database changed
MariaDB [tpch]&gt; show tables;
+----------------+
| Tables_in_tpch |
+----------------+
| CUSTOMER       |
| LINEITEM       |
| NATION         |
| ORDERS         |
| PART           |
| PARTSUPP       |
| REGION         |
| SUPPLIER       |
+----------------+
8 rows in set (0.00 sec)</programlisting>

        <para>At this point the data creation is complete and you are ready to
        start running a performance test. Before doing so note that as this is
        a query based workload you have the potential to run multiple tests
        and it will return the same results as the data is not modified during
        tests, however there is one exception, under the Driver Options the
        option to choose a Refresh Function. Further details will be given on
        the refresh function in the next section however at this point it is
        sufficient to note that the refresh function when enabled will modify
        data and no two same refresh functions can be run on the same data
        set. This means if you choose to include a refresh function and then
        attempt to re-run the test you will receive an error as the data has
        been modified. This means that it is strongly recommended to backup or
        export your data before running a refresh function to ensure that if
        you wish to run the refresh function multiple times you are prepared
        to restore your schema to the previous state before running subsequent
        tests.</para>

        <programlisting>./client/mysqldump -u root -pmysql tpch &gt; backup-tpch.sql

./client/mysql -u root -pmysql
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 16
Server version: 10.1.25-MariaDB Source distribution

Copyright (c) 2000, 2017, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]&gt; create database tpch;
Query OK, 1 row affected (0.00 sec)

MariaDB [(none)]&gt; Bye
./client/mysql -u root -pmysql tpch &lt; backup-tpch.sql 

./client/mysql -u root -pmysql
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 18
Server version: 10.1.25-MariaDB Source distribution

Copyright (c) 2000, 2017, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]&gt; use tpch
Database changed
MariaDB [tpch]&gt; show tables;
+----------------+
| Tables_in_tpch |
+----------------+
| CUSTOMER       |
| LINEITEM       |
| NATION         |
| ORDERS         |
| PART           |
| PARTSUPP       |
| REGION         |
| SUPPLIER       |
+----------------+
8 rows in set (0.00 sec)

MariaDB [tpch]&gt; select count(*) from LINEITEM;
+----------+
| count(*) |
+----------+
|  6000385 |
+----------+
1 row in set (0.00 sec)</programlisting>
      </section>

      <section>
        <title>Verifying and Backing up the PostgreSQL Schema</title>

        <para>The schema build is now complete with the following tables
        created and populated with the example showing a scale factor 1
        schema.</para>

        <programlisting>$ ./bin/psql -d tpch
psql (10.1)
Type "help" for help.
tpch=# select relname, n_tup_ins - n_tup_del as rowcount from pg_stat_user_tables;
 relname  | rowcount 
----------+----------
 nation   |       25
 lineitem |  6000773
 orders   |  1497000
 customer |   150000
 region   |        5
 supplier |    10000
 part     |   200000
 partsupp |   800000
(8 rows)

tpch=# 
</programlisting>

        <para>At this point the data creation is complete and you are ready to
        start running a performance test. Before doing so note that as this is
        a query based workload you have the potential to run multiple tests
        and it will return the same results as the data is not modified during
        tests, however there is one exception, under the Driver Options the
        option to choose a Refresh Function. Further details will be given on
        the refresh function in the next section however at this point it is
        sufficient to note that the refresh function when enabled will modify
        data and no two same refresh functions can be run on the same data
        set. This means if you choose to include a refresh function and then
        attempt to re-run the test you will receive an error as the data has
        been modified. This means that it is strongly recommended to backup or
        export your data before running a refresh function to ensure that if
        you wish to run the refresh function multiple times you are prepared
        to restore your schema to the previous state before running subsequent
        tests.</para>

        <programlisting>pgsql$ ./bin/pg_dump tpch &gt; pgdumpfile

pgsql$ ./bin/psql 
psql (10.1)
Type "help" for help.

# drop database tpch;
DROP DATABASE
# drop role tpch;
DROP ROLE

# create user tpch password 'tpch';
CREATE ROLE
# create database tpch owner tpch;
CREATE DATABASE


pgsql$ cat pgdumpfile | ./bin/psql tpch
SET
SET
SET
SET
....</programlisting>
      </section>
    </section>

    <section>
      <title>Checking the Schema</title>

      <para>From v4.10 a schema and data check has been added that can be run
      after a schema build and also after running a workload with a refresh
      function to ensure that the data remains consistent. To run the check
      select the Check option from the treeview or the main menu. </para>

      <figure>
        <title>Check TPROC-H Schema</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="../DocBook/docs/images/ch13-16a.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>When completed a message will be shown if the check was
      successful. </para>

      <figure>
        <title>TPROC-H Schema Check Completed</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="../DocBook/docs/images/ch13-16aa.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The following checks will be run against the schema and
      data:</para>

      <orderedlist>
        <listitem>
          <para>Database Exists.</para>
        </listitem>

        <listitem>
          <para>Tables Exist.</para>
        </listitem>

        <listitem>
          <para>Scale Factor in schema is the same as the HammerDB
          configuration.</para>
        </listitem>

        <listitem>
          <para>Tables are indexed.</para>
        </listitem>

        <listitem>
          <para>Tables are populated.</para>
        </listitem>

        <listitem>
          <para>Consistency Check.</para>

          <orderedlist>
            <listitem>
              <para>A consistent state for the TPC-H database is defined to
              exist when: O_TOTALPRICE =
              SUM(L_EXTENDEDPRICE*(1-L_DISCOUNT)(1+L_TAX) for each ORDER and
              LINEITEM defined by (O_ORDERKEY=L_ORDERKEY) and can be checked
              by: SELECT DECIMAL(SUM(DECIMAL(INTEGER(INTEGER(DECIMAL
              (INTEGER(100DECIMAL(L_EXTENDEDPRICE,20,3)),20,3)*
              (1-L_DISCOUNT)) * (1+L_TAX)),20,3)/100.0),20,3) FROM LINEITEM
              WHERE L_ORDERKEY = okey SELECT DECIMAL(SUM(O_TOTALPRICE, 20, 3))
              from ORDERS WHERE O_ORDERKEY = okey</para>
            </listitem>
          </orderedlist>
        </listitem>
      </orderedlist>
    </section>

    <section>
      <title>Configuring Driver Script Options</title>

      <para>To select the driver script options select Options from under the
      Driver Script heading in the tree-view.</para>

      <figure>
        <title>Driver Script Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch13-17.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>This displays the Driver Script Options dialog. The connection
      options are common to the Schema Build Dialog in addition to new Driver
      Options.</para>

      <figure>
        <title>TPROC-H Driver Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch13-18.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <table>
        <title>Driver Script Options</title>

        <tgroup cols="2">
          <thead>
            <row>
              <entry align="center">Option</entry>

              <entry align="center">Description</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>Scale Factor</entry>

              <entry>Although not visible under the Driver Options the Scale
              Factor value is also inherited from the Build Options and must
              be the set to the same value for running the Driver Script as
              was used for the Build. This option is required in the Driver
              Script when the refresh function is run to ensure that the data
              is created to insert in the correction location without errors.
              The Scale Factor is also used as input to some queries. From
              v4.8 HammerDB will automatically detect and set the Scale Factor
              if it is set incorrectly.</entry>
            </row>

            <row>
              <entry>Total Query Sets per User</entry>

              <entry>A Query Set is a sequence of 22 queries. The Total number
              of query sets is the number of times after logging on that the
              virtual user completes an entire sequence of queries before
              logging off again. The difference between this and using
              iterations value in the Virtual User options is that the virtual
              user only logs on and off once and completes all of the query
              sets in between whereas with the iterations value the entire
              script is run multiple times.</entry>
            </row>

            <row>
              <entry>Exit on Database Error</entry>

              <entry>Exit on Database Error is shown as the parameter
              RAISEERROR in the Driver Script. RAISEERROR impacts the
              behaviour of an individual virtual user on detecting a database
              error. If set to TRUE on detecting an error the user will report
              the error into the HammerDB console and then terminate
              execution. If set to FALSE the virtual user will ignore the
              error and proceed with executing the next transaction. It is
              therefore important to be aware that if set to FALSE firstly if
              there has been a configuration error resulting in repeated
              errors then the workload might not be reported accurately and
              secondly you may not be aware of any occasional errors being
              reported as they are silently ignored.</entry>
            </row>

            <row>
              <entry>Verbose Output</entry>

              <entry>Verbose Output is shown as VERBOSE in the Driver Script.
              Setting this value to TRUE will print both the Queries and their
              results for each virtual user however will add to the Query time
              by the time required to print the results.</entry>
            </row>

            <row>
              <entry>Refresh Function</entry>

              <entry>The refresh function checkbox corresponds to refresh_on
              in the Driver Script. When this checkbox is enabled the first
              virtual user will run the refresh function as opposed to running
              a query set. Note that if you choose only one virtual user and
              select the refresh function checkbox then your virtual user will
              run a power test as detailed further in this document. The
              refresh function as the name implies inserts and deletes rows
              from the ORDERS and LINEITEM tables and the times of this
              function are required as input to calculating the QphH.</entry>
            </row>

            <row>
              <entry>Number of Update Sets/Trickle Refresh Delay(ms)/Refresh
              Verbose</entry>

              <entry>If you have enabled the refresh function then the values
              for Number of Update Sets/Trickle Refresh Delay(ms)/Refresh
              Verbose become active and these correspond to update_sets
              trickle_refresh and REFRESH_VERBOSE in the driver script
              respectively. The update sets determines how many times the
              virtual users will cycle through the refresh functions whilst
              noting that the function always starts at 1 and therefore cannot
              be restarted against the same schema until the schema has been
              refreshed. The Trickle Refresh Delay value sets the delay
              between each insert and delete with a default of 1 second
              ensuring that the refresh function does not place a significant
              load on the system, The Refresh Verbose value means that the
              virtual user running the refresh function reports on its
              activities.</entry>
            </row>

            <row>
              <entry>Cloud Analytic Queries (Oracle MySQL PostgreSQL
              Only)</entry>

              <entry>When selected this option loads a driver script that runs
              the sequence of 13 Oracle Cloud Analytic Queries.</entry>
            </row>

            <row>
              <entry>Degree of Parallelism (Oracle, Db2 and PostgreSQL
              Only)</entry>

              <entry>For Oracle Degree of Parallelism defines the number of
              Parallel Execution Server processes that the Queries will be
              executed with. The Degree of Parallelism is defined as the
              degree of parallel in the driver script. You should consult a
              good reference on Parallel Execution as the actual execution
              environment is more complex including both Producer and Consumer
              Parallel Execution Servers. This value will be determined by
              your available hardware resources and may be different for both
              the Power and Throughput tests. HammerDB will ensure that the
              test will run at your chosen degree of parallelism (also
              dependant on your settings of parallel_min and parallel_max
              servers). For Db2 The Degree of Parallelism is the value used
              for the command “SET CURRENT DEGREE” in the driver script and
              determines the level of parallelism used in executing the
              queries. For PostgreSQL the Degree of Parallelism sets the
              max_parallel_workers_per_gather parameter for the sessions
              executing the queries.</entry>
            </row>

            <row>
              <entry>MAXDOP (SQL Server Only)</entry>

              <entry>The MAXDOP setting defines the Maximum degree of
              parallelism to be set as a default on the schema
              objects.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>

    <section>
      <title>Loading the Driver Script</title>

      <para>After selected the Driver Script Options the Driver Script is
      loaded. The configured options can be seen in the Driver Script window
      and also modified directly there. The Load option can also be used to
      refresh the script to the configured Options. Pay particular attention
      to the Scale Factor value shown as "scale_factor" if different from the
      schema that you have loaded. From v4.8 HammerDB will automatically
      detect and set the Scale Factor if it is set incorrectly.</para>

      <figure>
        <title>Driver Script Loaded</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch13-19.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>

    <section>
      <title>Configure Virtual Users</title>

      <para>Select Virtual User Options from the tree-view.</para>

      <figure>
        <title>Virtual User Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch13-20.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>This displays the Virtual User Options dialog. For Analytic
      workloads it is recommended that only one Virtual User is selected for
      initial testing. If you wish to see the times for individual queries
      rather than just the query set you will also need to write to the
      log.</para>

      <figure>
        <title>Virtual User Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch13-21.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The values have the following meaning.</para>

      <para><table>
          <title>Virtual User Options</title>

          <tgroup cols="2">
            <thead>
              <row>
                <entry align="center">Option</entry>

                <entry align="center">Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>Virtual Users</entry>

                <entry>The number of Virtual Users to create. Note that when
                running a Timed Workload HammerDB will automatically create an
                additional Virtual User to monitor the workload.</entry>
              </row>

              <row>
                <entry>User Delay(ms)</entry>

                <entry>User Delay(ms) defines the time to wait a Virtual User
                will wait behind the previous Virtual User before starting its
                test, this is to prevent a login storm with all Virtual Users
                attempting to login at the same time.</entry>
              </row>

              <row>
                <entry>Repeat Delay(ms)</entry>

                <entry>Repeat Delay(ms) is the time that each Virtual User
                will wait before running its next Iteration of the Driver
                Script. For TPROC-H this is an external loop before running
                another query set, however should not be more than 1 when the
                refresh function is enabled.</entry>
              </row>

              <row>
                <entry>Iterations</entry>

                <entry>Iterations is the number of times that the Driver
                Script is run in its entirety.</entry>
              </row>

              <row>
                <entry>Show Output</entry>

                <entry>Show Output will report Virtual User Output to the
                Virtual User Output Window, For TPROC-H tests this should be
                enabled.</entry>
              </row>

              <row>
                <entry>Log Output to Temp</entry>

                <entry>When enabled this appends all Virtual User Output to a
                text file in an available temp directory named
                hammerdb.log</entry>
              </row>

              <row>
                <entry>Use Unique Log Name</entry>

                <entry>Use a unique identifier for the Log Name.</entry>
              </row>

              <row>
                <entry>No Log Buffer</entry>

                <entry>By default text log output is buffered in memory before
                being written, this option writes the log output
                immediately.</entry>
              </row>

              <row>
                <entry>Log Timestamps</entry>

                <entry>Add an additional line of output with a timestamp every
                time that the log is written to.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>Select the Virtual User options, Press OK.</para>
    </section>

    <section>
      <title>Run a Single Virtual User Test</title>

      <para>Check that your scale_factor in the Driver Script is the same as
      the schema you are running the test against. You can also set the Degree
      of Parallelism/MAXDOP directly in the script.</para>

      <figure>
        <title>Modified Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch13-24.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Double-click on create Virtual User followed by Run. This will
      proceed to run a single Virtual User with one Query Set.</para>

      <figure>
        <title>Run a single Virtual User Test</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch13-23.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>When complete the Virtual User will show the query set time as
      well as the geometric mean of queries that returned rows including a
      count of those queries that returned rows.</para>

      <figure>
        <title>Single Virtual User Complete</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch13-25.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>And the log will show the Query times. Note how the queries are
      run in a pre-determined random order.</para>

      <programlisting>Hammerdb Log @ Fri Oct 23 15:31:59 BST 2020
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-
Vuser 1:Executing Query 14 (1 of 22)
Vuser 1:query 14 completed in 7.301 seconds
Vuser 1:Executing Query 2 (2 of 22)
Vuser 1:query 2 completed in 0.952 seconds
Vuser 1:Executing Query 9 (3 of 22)
Vuser 1:query 9 completed in 24.457 seconds
Vuser 1:Executing Query 20 (4 of 22)
Vuser 1:query 20 completed in 1.249 seconds
Vuser 1:Executing Query 6 (5 of 22)
Vuser 1:query 6 completed in 1.978 seconds
Vuser 1:Executing Query 17 (6 of 22)
Vuser 1:query 17 completed in 1.079 seconds
Vuser 1:Executing Query 18 (7 of 22)
Vuser 1:query 18 completed in 19.45 seconds
Vuser 1:Executing Query 8 (8 of 22)
Vuser 1:query 8 completed in 11.962 seconds
Vuser 1:Executing Query 21 (9 of 22)
Vuser 1:query 21 completed in 58.399 seconds
Vuser 1:Executing Query 13 (10 of 22)
Vuser 1:query 13 completed in 17.475 seconds
Vuser 1:Executing Query 3 (11 of 22)
Vuser 1:query 3 completed in 4.463 seconds
Vuser 1:Executing Query 22 (12 of 22)
Vuser 1:query 22 completed in 2.39 seconds
Vuser 1:Executing Query 16 (13 of 22)
Vuser 1:query 16 completed in 1.152 seconds
Vuser 1:Executing Query 4 (14 of 22)
Vuser 1:query 4 completed in 19.246 seconds
Vuser 1:Executing Query 11 (15 of 22)
Vuser 1:query 11 completed in 2.593 seconds
Vuser 1:Executing Query 15 (16 of 22)
Vuser 1:query 15 completed in 2.253 seconds
Vuser 1:Executing Query 1 (17 of 22)
Vuser 1:query 1 completed in 19.213 seconds
Vuser 1:Executing Query 10 (18 of 22)
Vuser 1:query 10 completed in 21.596 seconds
Vuser 1:Executing Query 19 (19 of 22)
Vuser 1:query 19 completed in 20.239 seconds
Vuser 1:Executing Query 5 (20 of 22)
Vuser 1:query 5 completed in 19.305 seconds
Vuser 1:Executing Query 7 (21 of 22)
Vuser 1:query 7 completed in 6.117 seconds
Vuser 1:Executing Query 12 (22 of 22)
Vuser 1:query 12 completed in 15.223 seconds
Vuser 1:Completed 1 query set(s) in 278 seconds
Vuser 1:Geometric mean of query times returning rows (22) is 6.82555
</programlisting>

      <section>
        <title>Changing the Query Order</title>

        <para>For a single virtual User test you may wish to change the query
        order. This query order is predetermined in the common modules.
        However you can redefine this function by copying and pasting the
        ordered_set function and modifying the order. The following example is
        sufficient for the single Virtual User</para>

        <programlisting>rename ordered_set ordered_set_orig
proc ordered_set { myposition } {
if { $myposition &gt; 40 } { set myposition [ expr $myposition % 40 ] }
        set o_s(0)  { 14 2 9 20 6 17 18 8 21 13 3 22 16 4 11 15 1 10 19 5 7 12 }
        return $o_s($myposition)
}</programlisting>

        <para>and then you can change the query order as follows:</para>

        <programlisting>set o_s(0)  { 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 }</programlisting>

        <para>If your database has issues with particular queries being long
        running you can also remove queries this way that you do not wish to
        run.</para>
      </section>
    </section>

    <section>
      <title>Run a Power Test</title>

      <para>Many test environments are sufficient with running single Virtual
      User tests. With available parallel and column store configurations this
      test is sufficient to stress an entire system. Nevertheless a component
      of the TPROC-H test is the refresh function and the refresh function
      should be run either side of the Power Test. To enable this
      functionality HammerDB has a special power test mode, whereby if
      refresh_on is set to true as shown and only one virtual user is
      configured then HammerDB will run a Power Test. Note that once you
      selected refresh_on for a single Virtual User in Power Test Mode the
      value of update_sets will be set to 1 and the value of trickle_refresh
      set to 0 and the value of REFRESH_VERBOSE set to false, all these values
      will be set automatically to ensure optimal running of the Power
      Test.</para>

      <figure>
        <title>Power Test Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch13-26.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>When loaded note that the refresh_on option is set in the script.
      You should also ensure that the scale factor setting matches the setting
      for your schema. From v4.8 HammerDB will automatically detect and set
      the Scale Factor if it is set incorrectly.</para>

      <figure>
        <title>TPROC-H refresh on</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch13-28.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>With these settings run the Virtual User and it will run a New
      Sales Refresh, single Virtual User Query Set and Old Sales Refresh in
      order as required by a Power Test.</para>

      <figure>
        <title>Power Test</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch13-27.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>HammerDB will report when the Power Test is complete.</para>

      <figure>
        <title>Power Test Complete</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch13-29.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>and you can collect the refresh and query times from the
      log.</para>

      <programlisting>Hammerdb Log @ Fri Oct 23 15:41:39 BST 2020
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-
Vuser 1:New Sales refresh
Vuser 1:New Sales refresh complete in 54.15 seconds
Vuser 1:Completed 1 update set(s)
Vuser 1:Executing Query 14 (1 of 22)
Vuser 1:query 14 completed in 7.868 seconds
Vuser 1:Executing Query 2 (2 of 22)
Vuser 1:query 2 completed in 0.334 seconds
Vuser 1:Executing Query 9 (3 of 22)
Vuser 1:query 9 completed in 21.87 seconds
Vuser 1:Executing Query 20 (4 of 22)
Vuser 1:query 20 completed in 0.816 seconds
Vuser 1:Executing Query 6 (5 of 22)
Vuser 1:query 6 completed in 0.926 seconds
Vuser 1:Executing Query 17 (6 of 22)
Vuser 1:query 17 completed in 1.299 seconds
Vuser 1:Executing Query 18 (7 of 22)
Vuser 1:query 18 completed in 19.289 seconds
Vuser 1:Executing Query 8 (8 of 22)
Vuser 1:query 8 completed in 4.232 seconds
Vuser 1:Executing Query 21 (9 of 22)
Vuser 1:query 21 completed in 59.815 seconds
Vuser 1:Executing Query 13 (10 of 22)
Vuser 1:query 13 completed in 13.889 seconds
Vuser 1:Executing Query 3 (11 of 22)
Vuser 1:query 3 completed in 5.773 seconds
Vuser 1:Executing Query 22 (12 of 22)
Vuser 1:query 22 completed in 0.928 seconds
Vuser 1:Executing Query 16 (13 of 22)
Vuser 1:query 16 completed in 0.792 seconds
Vuser 1:Executing Query 4 (14 of 22)
Vuser 1:query 4 completed in 19.258 seconds
Vuser 1:Executing Query 11 (15 of 22)
Vuser 1:query 11 completed in 0.497 seconds
Vuser 1:Executing Query 15 (16 of 22)
Vuser 1:query 15 completed in 9.436 seconds
Vuser 1:Executing Query 1 (17 of 22)
Vuser 1:query 1 completed in 16.067 seconds
Vuser 1:Executing Query 10 (18 of 22)
Vuser 1:query 10 completed in 22.284 seconds
Vuser 1:Executing Query 19 (19 of 22)
Vuser 1:query 19 completed in 19.648 seconds
Vuser 1:Executing Query 5 (20 of 22)
Vuser 1:query 5 completed in 18.98 seconds
Vuser 1:Executing Query 7 (21 of 22)
Vuser 1:query 7 completed in 6.089 seconds
Vuser 1:Executing Query 12 (22 of 22)
Vuser 1:query 12 completed in 12.512 seconds
Vuser 1:Completed 1 query set(s) in 263 seconds
Vuser 1:Geometric mean of query times returning rows (22) is 5.43452
Vuser 1:Old Sales refresh
Vuser 1:Old Sales refresh complete in 16.016 seconds
Vuser 1:Completed 1 update set(s)</programlisting>

      <para>Be aware that some databases are considerably better at running
      the refresh functions than others and also that once the power test has
      been run it is necessary to restore the database from backup before
      running the refresh function again. If you fail to do so you will
      receive a constraint violation error. This is expected behaviour.</para>

      <programlisting>Error in Virtual User 1: 23000 2627 {[Microsoft][ODBC Driver 13 for SQL Server][SQL Server]
Violation of PRIMARY KEY constraint 'orders_pk'. 
Cannot insert duplicate key in object 'dbo.orders'. The duplicate key value is (9).}</programlisting>
    </section>

    <section>
      <title>Run a Throughput Test</title>

      <para>After the power test you should run the throughput test (if the
      refresh function has been run it is necessary to refresh the schema).
      For the throughput test you need to also run the refresh function
      however this time the aim is to trickle the refresh function slowly
      while multiple query streams are run. Configure the options as for the
      Power Test and enable the refresh function, this time the update sets,
      trickle refresh and REFRESH_VERBOSE options will also be enabled when
      refresh_on is set to true. Configure the correct number of Virtual Users
      to enable the first Virtual User to run the Refresh Functions and
      additional Virtual Users to run the Query Streams as defined in the
      specification for the test. For the example below at Scale Factor 10
      there are 3 Virtual Users to run the queries and 1 to run the refresh.
      Note that the Refresh Function will run more slowly as expected and all
      of the Virtual Users run the queries in a different order.</para>

      <figure>
        <title>Throughput Test</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch13-30.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <section>
        <title>SQL Server Snapshot Isolation</title>

        <para>Note that before running a long running query at the same time
        as the inserts of the refresh function you should enable snapshot
        isolation on the database. Failure to do so will mean the Query
        streams will hang under a shared lock (LCK_M_S) whilst the refresh
        function is running.</para>

        <figure>
          <title>Enable Snapshot Isolation</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch13-31.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>Once set the refresh and queries will run as expected. If you
        observe foreign key or constraint violation errors after having
        restored a schema verify the scale factor in the driver script is set
        to the same value as the scale factor of the schema you have restored.
        From v4.8 HammerDB will automatically detect and set the Scale Factor
        if it is set incorrectly.</para>

        <figure>
          <title>SQL Server with Snapshot Isolation</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch13-32.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>When the Virtual Users running the Query sets have completed the
        throughput tests, note the longest (not the shortest) time taken for a
        full query set to complete. You do not need to wait for the trickled
        refresh function to complete, however must have configured enough
        update sets to ensure that the refresh function remains running whilst
        the throughput test completes.</para>

        <figure>
          <title>Throughput test complete</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch13-33.PNG"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>
    </section>

    <section>
      <title>Calculate the Geometric Mean</title>

      <para>For comparison of HammerDB TPROC-H workloads between systems it is
      recommended to use the geometric mean of the query times. This can be a
      straightforward comparison between power tests. When comparing
      throughput tests it is recommended to compare the geomean of the slowest
      virtual user when comparing throughput tests of an equal number of
      virtual users across systems.</para>

      <programlisting>0.943
0.332
21.281
1.163
1.93
0.504
13.358
11.419
55.767
20.412
2.435
1.011
2.08
18.064
5.83
2.003
13.503
5.548
1.525
9.765
3.69
3.226
SUM 195.789
GEOMEAN 4.011822724
</programlisting>
    </section>
  </chapter>

  <chapter>
    <title>Generating and Loading Bulk Datasets</title>

    <para>For all workloads HammerDB can create the schema and generate and
    load the data without requiring a staging area, in many circumstances this
    is the preferred method of loading especially for OLTP workloads.
    Nevertheless in some circumstances it is preferable to create the data
    externally as flat files and then use a special database vendor provided
    bulk loading command to load the data into pre-created tables. This option
    may be preferred for example where the target database to load is located
    in the cloud or where the target database has a column structure meaning
    that load performance using batch inserts is poor. Additionally bulk
    loading can enable more flexibility to modify the schema according to
    preference and reload during testing. This chapter details how to generate
    and load large data sets with HammerDB. From version 4.2 the limit for
    generating the TPROC-C schema has increased from 30,000 to 100,000
    warehouses. Note that this is an interface limit to prevent
    over-provisioning (100,000 warehouses may generate up to 10TB of data),
    however it is straightforward to exceed this capacity by manually
    modifying the generated datageb build script to increase the value.</para>

    <section>
      <title>Generate the Dataset</title>

      <para>This example for generating the dataset uses SQL Server on
      Windows, however the process is identical when creating data on Linux.
      Firstly create an empty directory that is writable for the user running
      HammerDB and does not contain any existing generated files. HammerDB
      will not overwrite existing generated files.</para>

      <para><figure>
          <title>Data Directory</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch10-1.png"/>
            </imageobject>
          </mediaobject>
        </figure>This point in time is a good stage to validate how much free
      space is in the directory, HammerDB will not check before generating
      data that enough free space exists in the file system before proceeding.
      From the benchmark menu select your chosen database and workload. Note
      that it is particularly important that you select the correct database
      and workload for your environment before generating the data. The data
      generated is different between different databases and workloads. For
      example for optimization purposes the columns may be ordered differently
      between different databases. The data is generated in column order for
      the way that HammerDB generates the schema and data such as time and
      date formats may be different. Errors will result from loading the data
      generated for one database in another without modification.</para>

      <figure>
        <title>Benchmark Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch10-2.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Under the benchmark or Options menus select Datagen and
      Options</para>

      <figure>
        <title>Datagen Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch10-3.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Depending upon whether you have selected a TPROC-C or TPROC-H
      benchmark under the benchmark options the dialog will be different. For
      the TPROC-H options select the Scale Factor, the directory that you have
      pre-created and the number of Virtual Users to generate the
      schema.</para>

      <figure>
        <title>Data Generation Options</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch10-4.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The data generation offers linear scalability with the key limit
      on performance being the number of CPU cores/threads on the system. Each
      virtual user will use 100% of an available core/thread and therefore it
      should be clear that creating more virtual users than available
      cores/threads is non-productive. Additionally creating the same number
      of virtual users as cores/threads will drive CPU utilisation to 100% -
      therefore select the number of Virtual Users equivalent to the available
      capacity on the system. Similarly it should also be clear that the time
      to create a data set is dependent on the number of available
      cores/threads – a 4 socket or above server with hundreds of
      cores/threads will be able to generate data considerably more quickly
      than a single socket PC or laptop. Finally bear in mind that each
      virtual user will generate a subsection of the data for tables. For
      example selecting 10 virtual users will generate 10 separate files to
      load each table. This approach enables both flexibility and scalability
      in both generating the data but also uploading generated files to the
      cloud and loading data in parallel.</para>

      <figure>
        <title>Multiple files</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch10-5.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>When the chosen values have been selected choose and click the
      Generate button or Generate menu option.</para>

      <figure>
        <title>Generate</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch10-6.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>This displays the Generate Data confirmation. Click Yes.</para>

      <figure>
        <title>Generate Data Confirmation</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch10-7.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>HammerDB will begin to generate the chosen schema in
      parallel.</para>

      <figure>
        <title>Generating Data</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch10-8.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Observe that the CPU utilisation level is in accordance with your
      Virtual User settings.</para>

      <figure>
        <title>CPU Utilisation 100%</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch10-9.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>HammerDB requires no further intervention to generate the data for
      the required schema.</para>

      <figure>
        <title>Schema Generated</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch10-10.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>You may also view the data with a text editor to see that the
      generated data is delimited by a pip character ie “|” and intended NULL
      values are represented by blank or empty data.</para>

      <figure>
        <title>Pipe Delimited Data</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch10-11.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Data is now generated and you are ready to proceed to creating a
      schema and loading the data. It should be clear that once you have
      created a chosen data set then you are able to reuse and reload that
      dataset multiple times and therefore HammerDB enables a quick way to
      generate all of the test data that you require. It is also important to
      note that in some cases databases will support compressing data with
      tools such as zip or gzip prior to loading and therefore if you have a
      large dataset to upload then investigating whether this is an option for
      your database is a worthwhile task.</para>
    </section>

    <section>
      <title>Generate the Dataset with the CLI</title>

      <para>Data Generation can also be run from the command line. As shown
      the dbset command is used to specify the database and benchmark to
      generate data for.</para>

      <programlisting>./hammerdbcli 
HammerDB CLI v4.1
Copyright (C) 2003-2021 Steve Shaw
Type "help" for a list of commands
The xml is well-formed, applying configuration
hammerdb&gt;print datagen
Data Generation set to build a TPC-C schema for Oracle with 1 warehouses with 1 virtual users in /tmp

hammerdb&gt;dbset bm TPROC-H
Benchmark set to TPROC-H for Oracle

hammerdb&gt;print datagen
Data Generation set to build a TPC-H schema for Oracle with 1 scale factor with 1 virtual users in /tmp

hammerdb&gt;dbset bm TPROC-C
Benchmark set to TPROC-C for Oracle

hammerdb&gt;print datagen
Data Generation set to build a TPC-C schema for Oracle with 1 warehouses with 1 virtual users in /tmp</programlisting>

      <para>and the Data Generation options set with the command dgset:</para>

      <programlisting>hammerdb&gt;dgset warehouse 10

hammerdb&gt;dgset vu 8
Set virtual users to 8 for data generation

hammerdb&gt;dgset directory "/tmp/TPCCDATA"

hammerdb&gt;print datagen
Data Generation set to build a TPC-C schema for Oracle with 10 warehouses with 8 virtual users in /tmp/TPCCDATA</programlisting>

      <para>The data is then created using the command datagenrun.</para>

      <programlisting>hammerdb&gt;datagenrun
Ready to generate the data for a 10 Warehouse Oracle TPC-C schema
in directory /tmp/TPCCDATA ?
Enter yes or no: replied yes
Vuser 1 created - WAIT IDLE
Vuser 2 created - WAIT IDLE
Vuser 3 created - WAIT IDLE
Vuser 4 created - WAIT IDLE
Vuser 5 created - WAIT IDLE
Vuser 6 created - WAIT IDLE
Vuser 7 created - WAIT IDLE
Vuser 8 created - WAIT IDLE
Vuser 9 created - WAIT IDLE
RUNNING - TPC-C generation
Vuser 1:RUNNING
Vuser 1:Monitor Thread
Vuser 1:Opened File /tmp/TPCCDATA/item_1.tbl
Vuser 1:Generating Item
Vuser 2:RUNNING
Vuser 2:Worker Thread
Vuser 2:Waiting for Monitor Thread...
Vuser 2:Generating 2 Warehouses start:1 end:2
Vuser 2:Start:Mon Apr 09 16:21:36 BST 2018
Vuser 2:Opened File /tmp/TPCCDATA/warehouse_1.tbl
Vuser 2:Opened File /tmp/TPCCDATA/stock_1.tbl
Vuser 2:Opened File /tmp/TPCCDATA/district_1.tbl
Vuser 2:Opened File /tmp/TPCCDATA/customer_1.tbl
Vuser 2:Opened File /tmp/TPCCDATA/history_1.tbl
Vuser 2:Opened File /tmp/TPCCDATA/orders_1.tbl
Vuser 2:Opened File /tmp/TPCCDATA/new_order_1.tbl
Vuser 2:Opened File /tmp/TPCCDATA/order_line_1.tbl
Vuser 2:Generating Warehouse
Vuser 2:Generating Stock Wid=1

...</programlisting>
    </section>

    <section>
      <title>Generate the template database</title>

      <para>Generating a template database is exceptionally straightforward.
      From the HammerDB documentation follow the steps for Build a Schema and
      create the smallest size database such as Scale Factor 1 for TPROC-H.
      This database can then be used as a template to capture the DDL. Note
      that if you stop the database creation after the tables are created but
      before all of the data is loaded objects such as indexes will not have
      been created and will not therefore be included in generated DDL, this
      may or may not be an issue for the type of schema you are intending to
      build, for example for a column store such as Amazon Redshift, indexes
      are not a requirement.</para>

      <section>
        <title>Capture and run the table creation DDL</title>

        <para>All of the mainstream databases supported with HammerDB enable
        DDL capture. This can be done as follows for each database. Note that
        at this stage you have the option to modify the DDL for your needs
        such as for partitioning or column orientation.</para>

        <section>
          <title>Oracle</title>

          <para>As the user owning the template database at a sqlplus prompt
          run a GET_DDL SQL statement as follows, noting that you need to set
          the long and pagesize values correctly to view all of the
          output.</para>

          <programlisting>SQL&gt;select DBMS_METADATA.GET_DDL('TABLE','ORDERS') from dual;</programlisting>

          <para>This produces a Create Table statement such as follows:</para>

          <programlisting>CREATE TABLE "TPCH"."ORDERS"                                                 
("O_ORDERDATE" DATE,                                         
"O_ORDERKEY" NUMBER NOT NULL ENABLE,                            
"O_CUSTKEY" NUMBER NOT NULL ENABLE,                            
"O_ORDERPRIORITY" CHAR(15),                                   
"O_SHIPPRIORITY" NUMBER,                                      
"O_CLERK" CHAR(15),                                           
"O_ORDERSTATUS" CHAR(1),                                       
"O_TOTALPRICE" NUMBER,              
"O_COMMENT" VARCHAR2(79),
 CONSTRAINT "ORDERS_PK" PRIMARY KEY ("O_ORDERKEY")
);
</programlisting>

          <para>Joining these files together can then be run against the
          database to create the schema of empty tables:</para>

          <programlisting>sqlplus tpch/tpch
SQL*Plus: Release 12.1.0.2.0 Production
Copyright (c) 1982, 2014, Oracle.  All rights reserved.
Connected to:
Oracle Database 12c Enterprise Edition Release 12.1.0.2.0
SQL&gt; @tpch_tables.sql
Table created.
Table created.
Table created.
Table created.
Table created.
Table created.
Table created.
Table created.
SQL&gt;
</programlisting>
        </section>

        <section>
          <title>SQL Server</title>

          <para>Within SQL Server Management Studio right click your chosen
          table, select “Script Table as” followed by “CREATE To” and choose
          your destination for the DDL. This produces DDL to create your table
          such as follows. Create a single file containing all of your DDL
          statements and click on Execute (F5) under SQL Server Management
          Studio.</para>

          <figure>
            <title>SQL Server Create Table</title>

            <mediaobject>
              <imageobject>
                <imagedata fileref="docs/images/ch10-12.png"/>
              </imageobject>
            </mediaobject>
          </figure>
        </section>

        <section>
          <title>Db2</title>

          <para>For Db2 use the db2look command, this can generate the DDL for
          all objects within a schema with one command.</para>

          <programlisting>db2look -d TPCH -a -e -x -o tpchcreate.sql
-- Generate statistics for all creators 
-- Creating DDL for table(s)
-- Output is sent to file: tpchcreate.sql
-- Binding package automatically ... 
-- Bind is successful
-- Binding package automatically ... 
-- Bind is successful
------------------------------------------------
-- DDL Statements for Table "DB2INST1"."ORDERS"
------------------------------------------------
</programlisting>

          <para>The output file will contain output as follows:</para>

          <programlisting>
CREATE TABLE "DB2INST1"."ORDERS"  (
  "O_ORDERKEY" INTEGER NOT NULL , 
  "O_CUSTKEY" INTEGER NOT NULL , 
  "O_ORDERSTATUS" CHAR(1 OCTETS) NOT NULL , 
  "O_TOTALPRICE" DOUBLE NOT NULL , 
  "O_ORDERDATE" DATE NOT NULL , 
  "O_ORDERPRIORITY" CHAR(15 OCTETS) NOT NULL , 
  "O_CLERK" CHAR(15 OCTETS) NOT NULL , 
  "O_SHIPPRIORITY" INTEGER , 
  "O_COMMENT" VARCHAR(79 OCTETS) NOT NULL )   
 IN "USERSPACE1"  
 ORGANIZE BY ROW; 
</programlisting>

          <para>Run the file as follows:</para>

          <programlisting>db2 -tvf tpchcreate.sql
CONNECT TO TPCH

   Database Connection Information

 Database server        = DB2/LINUXX8664 11.1.0
 SQL authorization ID   = DB2INST1
 Local database alias   = TPCH2

CREATE SCHEMA "DB2INST1"
DB20000I  The SQL command completed successfully.

CREATE TABLE "DB2INST1"."ORDERS"  ( "O_ORDERKEY" INTEGER NOT NULL , "O_CUSTKEY" INTEGER NOT NULL , "O_ORDERSTATUS" CHAR(1 OCTETS) NOT NULL , "O_TOTALPRICE" DOUBLE NOT NULL , "O_ORDERDATE" DATE NOT NULL , "O_ORDERPRIORITY" CHAR(15 OCTETS) NOT NULL , "O_CLERK" CHAR(15 OCTETS) NOT NULL , "O_SHIPPRIORITY" INTEGER , "O_COMMENT" VARCHAR(79 OCTETS) NOT NULL ) IN "USERSPACE1" ORGANIZE BY ROW
DB20000I  The SQL command completed successfully.
</programlisting>
        </section>

        <section>
          <title>MySQL</title>

          <para>For MySQL use the show create table command. Be aware that if
          foreign keys are defined at this stage they will significantly
          impact load performance.</para>

          <programlisting>mysql&gt; use tpch;
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Database changed
mysql&gt; show tables;
+----------------+
| Tables_in_tpch |
+----------------+
| CUSTOMER       |
| LINEITEM       |
| NATION         |
| ORDERS         |
| PART           |
| PARTSUPP       |
| REGION         |
| SUPPLIER       |
+----------------+
8 rows in set (0.00 sec)

mysql&gt; show create table SUPPLIER;

CREATE TABLE `SUPPLIER` (
  `S_SUPPKEY` int(11) NOT NULL,
  `S_NATIONKEY` int(11) DEFAULT NULL,
  `S_COMMENT` varchar(102) CHARACTER SET latin1 COLLATE latin1_bin DEFAULT NULL,
  `S_NAME` char(25) CHARACTER SET latin1 COLLATE latin1_bin DEFAULT NULL,
  `S_ADDRESS` varchar(40) CHARACTER SET latin1 COLLATE latin1_bin DEFAULT NULL,
  `S_PHONE` char(15) CHARACTER SET latin1 COLLATE latin1_bin DEFAULT NULL,
  `S_ACCTBAL` decimal(10,2) DEFAULT NULL,
  PRIMARY KEY (`S_SUPPKEY`),
  KEY `SUPPLIER_FK1` (`S_NATIONKEY`)
) ENGINE=MyISAM DEFAULT CHARSET=latin1;
</programlisting>

          <para>Create a file containing all of the table creation statements
          and run as follows:</para>

          <programlisting>sql&gt; use tpch
Database changed
mysql&gt; source /home/mysql/TPCHDATA/createtpch.sql
Query OK, 0 rows affected (0.08 sec)
Query OK, 0 rows affected (0.07 sec)
Query OK, 0 rows affected (0.05 sec)
Query OK, 0 rows affected (0.05 sec)
Query OK, 0 rows affected (0.05 sec)
Query OK, 0 rows affected (0.06 sec)
Query OK, 0 rows affected (0.05 sec)
Query OK, 0 rows affected (0.05 sec)
mysql&gt;
</programlisting>
        </section>

        <section>
          <title>PostgreSQL/Amazon Redshift</title>

          <para>To create the DLL for PostgreSQL or Amazon Redshift (note you
          can create a template local PostgreSQL database and the DDL is 100%
          compatible to create a database in Redshift) use the pg_dump command
          as follows:</para>

          <programlisting>pg_dump -U postgres -h localhost tpch -t table_name --schema-only -f table.sql</programlisting>

          <para>On Linux systems you can use the bash shell to generate the
          DDL for all tables with one command, for example:</para>

          <programlisting>for sys in customer lineitem nation orders part partsupp region supplier; do pg_dump -U postgres -h localhost tpch -t $sys --schema-only -f $sys.sql; done</programlisting>

          <para>This generates a series of files containing the required DDL
          as follows:</para>

          <programlisting>CREATE TABLE customer (
    c_custkey numeric NOT NULL,
    c_mktsegment character(10),
    c_nationkey numeric,
    c_name character varying(25),
    c_address character varying(40),
    c_phone character(15),
    c_acctbal numeric,
    c_comment character varying(118)
);
</programlisting>

          <para>Run the files as follows under PostgreSQL or Redshift to
          create the desired tables. The following example create the schema
          on PostgreSQL</para>

          <programlisting>psql -d tpch -f pgtpchtables.sql</programlisting>

          <para>and the following on Redshift</para>

          <programlisting>bash-4.2$ psql -h tpch-instance.xxxxxxxxxxxx.eu-west-1.redshift.amazonaws.com -U postgres -d tpch -p 5439 -f pgtpchtables.sql
Password for user postgres: 
CREATE TABLE
CREATE TABLE
CREATE TABLE
CREATE TABLE
CREATE TABLE
CREATE TABLE
CREATE TABLE
CREATE TABLE
-bash-4.2$ psql -h tpch-instance.xxxxxxxxxxxx.eu-west-1.redshift.amazonaws.com -U postgres -d tpch -p 5439
Password for user postgres: 
psql (9.2.15, server 8.0.2)
WARNING: psql version 9.2, server version 8.0.
         Some psql features might not work.
SSL connection (cipher: ECDHE-RSA-AES256-GCM-SHA384, bits: 256)
Type "help" for help.

tpch=# \d
          List of relations
 schema |   name   | type  |  owner   
--------+----------+-------+----------
 public | customer | table | postgres
 public | lineitem | table | postgres
 public | nation   | table | postgres
 public | orders   | table | postgres
 public | part     | table | postgres
 public | partsupp | table | postgres
 public | region   | table | postgres
 public | supplier | table | postgres
(8 rows)
</programlisting>
        </section>
      </section>
    </section>

    <section>
      <title>Run the bulk data load</title>

      <para>With the schemas created you can proceed to bulk load the data you
      have created without the overhead of features such as logging associated
      with inserts. This section details example methods by which data can be
      bulk loaded. Note that some of these databases support multiple
      different methods to bulk load data and this section gives examples
      using either the most straightforward or widely available tools,
      therefore additional methods may exists to load your data.</para>

      <section>
        <title>Oracle</title>

        <para>SQL*Loader is the default method for loading Oracle with
        external data. SQL*Loader has the advantage of flexibility in being
        adaptable to loading data in many different formats. To use SQL*Loader
        firstly create a control file. The following example shows a control
        file for the ORDERS table from the TPCH schema. Firstly note that the
        control can accept wildcard characters and therefore multiple files
        can be loaded with one command. Also note how the DATE format has been
        specified.</para>

        <programlisting>more sqlldr-orders.ctl
load data
infile '/home/oracle/TPCCDATA/orders_*.tbl'
into table orders
fields terminated by "|"
(O_ID,O_W_ID,O_D_ID,O_C_ID,O_CARRIER_ID,O_OL_CNT,O_ALL_LOCAL,O_ENTRY_D DATE "YYY
YMMDDHH24MISS")
</programlisting>

        <para>A further different date formatting example can be seen for the
        LINEITEM table in the TPCH schema.</para>

        <programlisting>more sqlldr-lineitem.ctl
load data
infile '/home/oracle/TPCHDATA/lineitem_*.tbl'
into table lineitem
fields terminated by "|"
(L_SHIPDATE DATE "yyyy-mon-dd",L_ORDERKEY,L_DISCOUNT ,L_EXTENDEDPRICE,L_SUPPKEY,L_QUANTITY,L_RETURNFLAG,L_PARTKEY,L_LINESTATUS,L_TAX,L_COMMITDATE DATE "yyyy-mon-dd", L_RECEIPTDATE DATE "yyyy-mon-dd",L_SHIPMODE, L_LINENUMBER, L_SHIPINSTRUCT, L_COMMENT)
</programlisting>

        <para>Now run SQL*Loader specifying the control file and username and
        password.</para>

        <programlisting>sqlldr tpch/tpch control=/home/oracle/sqlldr-orders.ctl direct=true</programlisting>
      </section>

      <section>
        <title>SQL Server</title>

        <para>For SQL Server use a bulk insert state as follows. SQL Server
        does not recognize wildcard characters for bulk insert however is
        adaptable in recognizing both NULLS and various date formats by
        default.</para>

        <programlisting>BULK INSERT customer FROM 'C:\TEMP\TPCHDATA\customer_1.tbl' WITH (TABLOCK, DATAFILETYPE='char', CODEPAGE='raw', FIELDTERMINATOR = '|')</programlisting>

        <para>Run the bulk insert commands via SQL Server Management
        studio.</para>

        <figure>
          <title>SQL Server Bulk Insert</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch10-13.png"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>

      <section>
        <title>Db2</title>

        <para>For DB2 firstly connect to your database and then use the db2
        load command. The delimiter is specified by 0x7c as the ASCII
        character for vertical bar as the delimiter used.</para>

        <programlisting>$ db2 connect to tpch

Database Connection Information

Database server        = DB2/LINUXX8664 11.1.0
SQL authorization ID   = DB2INST1
Local database alias   = TPCH


db2 load from /home/db2inst1/TPCHDATA/nation_1.tbl of DEL MODIFIED BY DELPRIORITYCHAR COLDEL0x7c insert INTO nation
</programlisting>

        <para>This command is easy to script in Linux environments to load all
        available files:</para>

        <programlisting>$ for sys in `ls -1 customer_*.tbl`; do db2 load from /home/db2inst1/TPCHDATA/$sys of DEL MODIFIED BY DELPRIORITYCHAR COLDEL0x7c insert INTO customer; done

</programlisting>

        <para>Where a date is specified the dateformat must be given as
        follows:</para>

        <programlisting>db2 load from /home/db2inst1/TPCHDATA/lineitem_1.tbl of DEL MODIFIED BY DELPRIORITYCHAR COLDEL0x7c DATEFORMAT=\"YYYY-MMM-DD\" insert INTO lineitem</programlisting>

        <para>As with Oracle and SQL Server Db2 automatically recognises NULL
        values in the data.</para>
      </section>

      <section>
        <title>MySQL</title>

        <para>Bulk loading is done in MySQL using the load data infile
        command. For the MySQL TPROC-C schema NULLS are not automatically
        recognised and a SET command is required as follows for the ORDER_LINE
        and ORDERS table:</para>

        <programlisting>load data infile '/home/mysql/TPCCDATA/order_line_1.tbl' INTO table order_line fields terminated by '|'
(ol_w_id, ol_d_id, ol_o_id, ol_number, ol_i_id, @dt1, ol_amount, ol_supply_w_id, ol_quantity, ol_dist_info)
set ol_delivery_d = nullif(@dt1,'');

load data infile '/home/mysql/TPCCDATA/orders_1.tbl' INTO table orders fields terminated by '|'
(o_id, o_w_id, o_d_id, o_c_id, @id1, o_ol_cnt, o_all_local, o_entry_d)
set o_carrier_id = nullif(@id1,'');
</programlisting>
      </section>

      <section>
        <title>MariaDB</title>

        <para>As with MySQL bulk loading is done in MariaDB using the load
        data infile command.</para>

        <programlisting>mysql&gt; load data infile '/home/mysql/TPCHDATA/supplier_1.tbl' INTO table SUPPLIER fields terminated by '|';</programlisting>

        <para>Load data infile does not offer enable a method by which
        different date formats can be specified however this can be achieved
        by specifying an additional SET command as shown for the LINEITEM
        table:</para>

        <programlisting>load data infile '/home/mysql/TPCHDATA/lineitem_1.tbl' INTO table LINEITEM fields terminated by '|'
(@dt1, L_ORDERKEY, L_DISCOUNT, L_EXTENDEDPRICE, L_SUPPKEY, L_QUANTITY, L_RETURNFLAG, L_PARTKEY, L_LINESTATUS, L_TAX, @dt2, @dt3, L_SHIPMODE, L_LINENUMBER, L_SHIPINSTRUCT, L_COMMENT)
set L_SHIPDATE = STR_TO_DATE(@dt1, '%Y-%b-%d'),L_COMMITDATE = STR_TO_DATE(@dt2, '%Y-%b-%d'),L_RECEIPTDATE = STR_TO_DATE(@dt3, '%Y-%b-%d');
</programlisting>

        <para>and a SET command as follows shown for the ORDERS table:</para>

        <programlisting>load data infile '/home/mysql/TPCHDATA/orders_1.tbl' INTO table ORDERS fields terminated by '|'
(@dt1, O_ORDERKEY, O_CUSTKEY, O_ORDERPRIORITY, O_SHIPPRIORITY, O_CLERK, O_ORDERSTATUS, O_TOTALPRICE, O_COMMENT)
set O_ORDERDATE = STR_TO_DATE(@dt1, '%Y-%b-%d');
</programlisting>

        <para>Add these commands to a file:</para>

        <programlisting>load data infile '/home/mysql/TPCHDATA/customer_1.tbl' INTO table CUSTOMER fields terminated by '|';
load data infile '/home/mysql/TPCHDATA/customer_2.tbl' INTO table CUSTOMER fields terminated by '|';
load data infile '/home/mysql/TPCHDATA/customer_3.tbl' INTO table CUSTOMER fields terminated by '|';
load data infile '/home/mysql/TPCHDATA/customer_4.tbl' INTO table CUSTOMER fields terminated by '|';
load data infile '/home/mysql/TPCHDATA/customer_5.tbl' INTO table CUSTOMER fields terminated by '|';
load data infile '/home/mysql/TPCHDATA/customer_6.tbl' INTO table CUSTOMER fields terminated by '|';
</programlisting>

        <para>and run as follows:</para>

        <para><programlisting>mysql&gt; source /home/mysql/TPCHDATA/loadfiles.sql
Query OK, 150000 rows affected (0.74 sec)
Records: 150000  Deleted: 0  Skipped: 0  Warnings: 0

Query OK, 150000 rows affected (1.56 sec)
Records: 150000  Deleted: 0  Skipped: 0  Warnings: 0</programlisting></para>
      </section>

      <section>
        <title>PostgreSQL/Amazon Redshift</title>

        <para>Both PostgreSQL and Amazon Redshift use the copy command to bulk
        load data, however Redshift has additional requirements to load the
        data into the cloud. For PostgreSQL make a file with the copy commands
        for all tables for example:</para>

        <programlisting>\copy customer from '/home/postgres/TPCHDATA/customer_1.tbl' WITH DELIMITER AS '|';
\copy customer from '/home/postgres/TPCHDATA/customer_2.tbl' WITH DELIMITER AS '|';
\copy customer from '/home/postgres/TPCHDATA/customer_3.tbl' WITH DELIMITER AS '|';
\copy customer from '/home/postgres/TPCHDATA/customer_4.tbl' WITH DELIMITER AS '|';
\copy customer from '/home/postgres/TPCHDATA/customer_5.tbl' WITH DELIMITER AS '|';
\copy customer from '/home/postgres/TPCHDATA/customer_6.tbl' WITH DELIMITER AS '|';
\copy customer from '/home/postgres/TPCHDATA/customer_7.tbl' WITH DELIMITER AS '|';
\copy customer from '/home/postgres/TPCHDATA/customer_8.tbl' WITH DELIMITER AS '|';

</programlisting>

        <para>And run the script to copy the files</para>

        <programlisting>psql -U postgres -d tpch -f TPCHCOPY.sql</programlisting>

        <para>With PostgreSQL additional lines are required to handle NULL
        value for the TPROC-C schema as follows:</para>

        <programlisting>\copy order_line from '/home/postgres/TPCCDATA/order_line_1.tbl' WITH NULL AS '' DELIMITER AS '|';
\copy orders from '/home/postgres/TPCCDATA/orders_1.tbl' WITH NULL AS '' DELIMITER AS '|';
</programlisting>

        <para>For Amazon Redshift firstly upload the generated files to an
        Amazon S3 bucket. As noted previously Amazon S3 is one of the
        databases that supports loading from a compressed file and therefore
        you may wish to convert the files to a compressed format such as gzip
        before uploading.</para>

        <figure>
          <title>Upload to S3</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch10-14.png"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>Under the AWS IAM Console create a user for uploading and under
        security credentials create and download an access key. Note that the
        access keys have been removed from the image.</para>

        <figure>
          <title>Postgres User Access Keys</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch10-15.png"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>Finally give the postgres user permission to access Amazon
        S3.</para>

        <figure>
          <title>S3 Permissions</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch10-16.png"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>Now connect to Redshift using the PostgreSQL command line tool
        and run the copy command specifying the location of the S3 bucket and
        the CREDENTIALs option where the XXX characters are replaced by the
        access key and secure access key you created previously.</para>

        <programlisting>-bash-4.2$ psql -h tpch-instance.xxxxxxxxxxxx.eu-west-1.redshift.amazonaws.com -U postgres -d tpch -p 5439
Password for user postgres: 
psql (9.2.15, server 8.0.2)
WARNING: psql version 9.2, server version 8.0.
         Some psql features might not work.
SSL connection (cipher: ECDHE-RSA-AES256-GCM-SHA384, bits: 256)
Type "help" for help.

tpch=# copy region from 's3://s3bucket/load/region_1.tbl' CREDENTIALS 'aws_access_key_id=XXXXXXXXXXXXXXXXXXXX;aws_secret_access_key=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX' delimiter '|';
INFO:  Load into table 'region' completed, 5 record(s) loaded successfully.
COPY
</programlisting>

        <para>Note that if you specify part of the filename Redshift will
        upload all of the files with the same prefix.</para>

        <programlisting>copy customer from 's3://s3bucket/load/customer_' CREDENTIALS 'aws_access_key_id=XXXXXXXXXXXXXXXXXXXX;aws_secret_access_key=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX' delimiter '|';
INFO:  Load into table 'customer' completed, 150000 record(s) loaded successfully.
</programlisting>

        <para>For NULL values and date and time formats you can specify the
        formats for load as follows:</para>

        <programlisting>tpch=# copy orders from 's3://s3bucket/load/orders_' CREDENTIALS 'aws_access_key_id=XXXXXXXXXXXXXXXXXXXX;aws_secret_access_key=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX' emptyasnull blanksasnull delimiter '|' timeformat 'YYYY-MON-DD HH:MI:SS';
INFO:  Load into table 'orders' completed, 1500000 record(s) loaded successfully.
COPY
</programlisting>

        <para>Note that as a column store Redshift does not require the
        additional indexes or constraints of a traditional row store
        format.</para>
      </section>
    </section>
  </chapter>

  <chapter>
    <title>Oracle Trace File Replay</title>

    <para>Oracle Trace Files can be converted and replayed against an Oracle
    database. Within HammerDB this trace file replay functionality is only
    available with Oracle as the other supported databases do not provide an
    interface to generating a detailed trace of workloads.</para>

    <sect1>
      <title>Generating Trace Files</title>

      <para>To begin converting Oracle trace file workloads with HammerDB the
      first step is to load an Oracle trace file under the File:Open menu
      option. Before doing this therefore an Oracle trace file must be
      generated using Oracle Event 10046. (There are numerous methods to
      generate Oracle trace files of which only event 10046, level 4 to
      capture bind variables is covered here). As an example the simplest
      method to start and stop the trace of a session interactively is as
      follows:</para>

      <programlisting>SQL&gt; connect / as sysdba
Connected.
SQL&gt; alter session set events '10046 trace name context forever, level 4'; 

Session altered.

SQL&gt; select sysdate from dual;

SYSDATE
---------
10-APR-18

SQL&gt; alter session set events '10046 trace name context off';

Session altered.

SQL&gt; 
</programlisting>

      <para>For more advanced use the creation of a logon trigger is
      recommended. This trigger can then be enabled or disabled to capture the
      trace information for a particular use. The example uses the user TPCC
      created for an Oracle HammerDB OLTP test.</para>

      <programlisting>create or replace trigger logon_trigger
after logon on database
begin
if (user = 'TPCC') then
execute immediate
'alter session set events ''10046 trace name context forever, level 4''';
end if;
end;</programlisting>

      <para>This will then create a tracefile automatically at logon.</para>

      <programlisting>SQL&gt; connect tpcc/tpcc@RVDB1
Connected.
SQL&gt; select sysdate from dual;

SYSDATE
---------
10-APR-18
</programlisting>

      <para>The trigger must be created as SYS with SYSDBA privileges, if
      created by system the trigger will create successfully but fail on the
      user login. This event will produce a trace file in the diagnostic area
      specified for the database server. By default the file will be
      identifiable by ora_SPID.trc, however there are also methods that can be
      used to set the name of the trace file. Note that in a Shared Server
      environment (previously MTS) one users’ session may be distributed
      across numerous trace files as the user processes share multiple server
      processes. Therefore in this environment it is necessary to reassemble
      the trace file data before converting with the Oracle utility ‘trcsess’.
      A a raw trace file is shown below:</para>

      <programlisting>race file /home/oracle/app/oracle/diag/rdbms/rvdb1/RVDB1/trace/RVDB1_ora_13783.trc
Oracle Database 12c Enterprise Edition Release 12.1.0.2.0 - 64bit Production
With the Partitioning, OLAP, Advanced Analytics and Real Application Testing options
ORACLE_HOME = /home/oracle/app/oracle/product/12.1.0/dbhome_1
System name:Linux
Node name:raven
Release:4.1.12-61.1.23.el7uek.x86_64
Version:#2 SMP Tue Dec 20 16:59:23 PST 2016
Machine:x86_64
Instance name: RVDB1
Redo thread mounted by this instance: 1
Oracle process number: 72
Unix process pid: 13783, image: oracle@raven (TNS V1-V3)


*** 2018-04-10 14:35:39.821
*** SESSION ID:(4.48241) 2018-04-10 14:35:39.821
*** CLIENT ID:() 2018-04-10 14:35:39.821
*** SERVICE NAME:(SYS$USERS) 2018-04-10 14:35:39.821
*** MODULE NAME:(sqlplus@raven (TNS V1-V3)) 2018-04-10 14:35:39.821
*** CLIENT DRIVER:(SQL*PLUS) 2018-04-10 14:35:39.821
*** ACTION NAME:() 2018-04-10 14:35:39.821
 
CLOSE #140169830640712:c=0,e=6,dep=0,type=1,tim=530154237
=====================
PARSING IN CURSOR #140169830722080 len=24 dep=0 uid=0 oct=3 lid=0 tim=530194337 hv=2343063137 ad='97b9c9e8' sqlid='7h35uxf5uhmm1'
select sysdate from dual
END OF STMT
PARSE #140169830722080:c=3149,e=39623,p=0,cr=0,cu=0,mis=1,r=0,dep=0,og=1,plh=1388734953,tim=530194336
EXEC #140169830722080:c=0,e=21,p=0,cr=0,cu=0,mis=0,r=0,dep=0,og=1,plh=1388734953,tim=530194497
FETCH #140169830722080:c=0,e=16,p=0,cr=0,cu=0,mis=0,r=1,dep=0,og=1,plh=1388734953,tim=530194568
STAT #140169830722080 id=1 cnt=1 pid=0 pos=1 obj=0 op='FAST DUAL  (cr=0 pr=0 pw=0 time=0 us cost=2 size=0 card=1)'
FETCH #140169830722080:c=0,e=1,p=0,cr=0,cu=0,mis=0,r=0,dep=0,og=0,plh=1388734953,tim=530213687

*** 2018-04-10 14:36:07.973
CLOSE #140169830722080:c=539,e=34,dep=0,type=0,tim=558307025
=====================
PARSING IN CURSOR #140169830722080 len=55 dep=0 uid=0 oct=42 lid=0 tim=558307224 hv=2217940283 ad='0' sqlid='06nvwn223659v'
alter session set events '10046 trace name context off'
END OF STMT
PARSE #140169830722080:c=0,e=112,p=0,cr=0,cu=0,mis=0,r=0,dep=0,og=0,plh=0,tim=558307223
EXEC #140169830722080:c=0,e=214,p=0,cr=0,cu=0,mis=0,r=0,dep=0,og=0,plh=0,tim=558307510</programlisting>

      <para>For more information on the trace file format the document
      Note:39817.1 Subject “Interpreting Raw SQL_TRACE output “ available from
      My Oracle Support, however this knowledge is not essential as HammerDB
      can convert this raw format into a form that can be replayed.</para>

      <figure>
        <title>Doc 39817.1</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch12-1.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>
    </sect1>

    <sect1>
      <title>Converting Oracle Trace Files</title>

      <para>It is important to note that to convert Oracle trace files you
      must have selected Oracle from the treeview. If another database is
      selected the button to convert Oracle trace files is disabled Copy the
      trace file to the client machine or location where HammerDB is running
      and use the File:Open menu option or the “Open an existing file button”
      under the Edit Menu to display the Open File dialogue</para>

      <figure>
        <title>Open File</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch12-2.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The Open File dialog allows the specifying of the Directory and a
      filter for the file type, by default this is *.tcl. Change the file
      extension to ‘trc’ and change directory to the location of your files,
      select the trace file you previously generated and select OK.</para>

      <figure>
        <title>Trace Loaded</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch12-3.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Select the Trace Conversion button at the bottom of the Edit
      menu</para>

      <figure>
        <title>Convert Trace</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch12-4.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>and the trace file is converted into a format that can replayed
      against the database.</para>

      <figure>
        <title>Trace Converted</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch12-5.PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Note that the trace file never records a users’ authentication
      details. Therefore the connect string must always be modified manually
      after conversion. Remove the comment before the “set connect” line on
      line 4 and enter the correct username and password. The SID will be set
      by default to the SID that the trace file was taken from and therefore
      if using a pluggable database then the correct container must also be
      set to the correct identifier. Once the connect string is updated the
      generated script is ready for running against the database and contains
      the original statements that were traced. The save menu option or “Save
      current file” button can be used to save the generated script for
      reloading at a later point in time.</para>
    </sect1>

    <sect1>
      <title>Replaying Oracle Trace Files</title>

      <para>Next to the “Convert” button there is a “Test current code”
      button. Click on this to test the code in an individual Virtual User
      environment. Once tested the window can be closed manually or by
      clicking the same button now containing a stop image.</para>

      <para><figure>
          <title>Run Trace</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="docs/images/ch12-8.png"/>
            </imageobject>
          </mediaobject>
        </figure></para>

      <para>Once the script has been tested and is ready for running increase
      the number of Virtual Users and run as for an OLTP test.</para>

      <figure>
        <title>Multiuser Test</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="docs/images/ch12-6.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The tracefile has now been replayed with multiple Virtual Users
      illustrating the basic building blocks for creating a bespoke Oracle
      workload from a captured and converted trace file.</para>
    </sect1>

    <sect1>
      <title>Capturing Errors from Trace File Workloads</title>

      <para>It must be remembered that although the previous workloads shown
      are suitable for directly replaying, with some applications replaying
      data from a previous transaction may result in constraint violations or
      updates of data that no longer exists that results in errors during
      replay. One of the most common error messages received is is the
      standard PL/SQL failure reported by Oratcl oraplexec: SQL execution
      failed. To get the actual Oracle error message underlying this error you
      can use the TCL "catch" command to capture the error and print it out
      inline. For example if you wanted to see the error in the neword
      procedure of the TPROC-C script change the oraplexec line to look like
      the following :</para>

      <programlisting>if {[ catch {oraplexec $curn1 $sql5 :no_w_id $no_w_id :no_max_w_id $w_id_input :no_d_id $no_d_id :no_c_id $no_c_id :no_o_ol_cnt $ol_cnt :no_c_discount {NULL} :no_c_last {NULL} :no_c_credit {NULL} :no_d_tax {NULL} :no_w_tax {NULL} :no_d_next_o_id {0} :timestamp $date} message]} {
puts $message
puts [ oramsg $curn1 all ]
}
</programlisting>

      <para>So you are adding the statements as below to the oraplexec
      statement</para>

      <programlisting>if {[ catch { ... } message] } {
puts $message
puts [ oramsg $curn1 all ]
}
</programlisting>

      <para>Note that the cursor variable $curn1 used in oramsg is the same
      variable used in the previous oraplexec. You can then run the script by
      pressing the test current code button ( or running one user thread with
      an output window ) The $message variable will contain and print out (
      using the "puts" command ) any TCL error and the [ oramsg $curn1 all ]
      command will then print out the oracle error from the cursor $curn1. An
      example is if the procedure neword has not yet been created: This then
      gives the full output from Oracle as shown here instead of just the
      standard message :</para>

      <programlisting>new order
{6550 {ORA-06550: line 1, column 7:
PLS-00201: identifier 'NEWORD' must be declared
ORA-06550: line 1, column 7:
PL/SQL: Statement ignored} 0 0 4 8}
</programlisting>
    </sect1>
  </chapter>

  <chapter>
    <title>HTTP and HTTPS Testing</title>

    <para>HammerDB can also be used for HTTP and HTTPS testing by using
    bespoke scripts. HammerDB uses the TLS package for HTTPS
    encryption.</para>

    <sect1>
      <title>HTTPS Script</title>

      <para>An example HTTPS script is shown. Modify the URL to your own URL.
      The example will fetch the URL in a loop for 10 iterations and report
      the time taken.</para>

      <para><programlisting>#!./bin/tclsh8.6
      package require http
      package require tls
      #package require twapi_crypto

proc http::Log {args} {
puts $args
}

proc cb {token}  {
if { [ http::status $token ] != "ok" } { error "Response not OK [ http::error $token]" } else {
http::cleanup $token
   }
set ::done 1
}


<emphasis role="bold">set url "https://MYURL"</emphasis>
#http::register https 443 [list ::twapi::tls_socket -async ]
http::register https 443 [list ::tls::socket -async -tls1 1 -ssl3 false -ssl2 false -autoservername true ]

set response [ http::geturl $url -keepalive true ]
    puts [ http::status $response ]
    set so [ lsearch -inline [ chan names ] sock* ]

chan configure $so -buffering none -encoding binary -blocking 1 -translation crlf -eofchar {{} {}}
      ::tls::handshake $so
     puts [ ::tls::status $so ]

chan configure $so -buffering full -buffersize 8192 -encoding binary -blocking 0 -translation crlf -eofchar {{} {}}
        set test_count 10
        set response_count 0
        set total_time 0
        set start_time [clock milliseconds]
        for {set i 0} {$i &lt; $test_count} {incr i} {
            set start_time [clock milliseconds]
          set ::done 0
set response [ http::geturl $url -binary true -blocksize 8192 -timeout 10000 -keepalive true -command cb ]        
vwait ::done
            set end_time [clock milliseconds]
            set elapsed [expr {$end_time - $start_time}]
            incr total_time $elapsed
            puts "Call $i took $elapsed"
            }
            set end_time [clock milliseconds]
puts "$test_count iterations in $total_time at an average of [expr {$total_time / $test_count}]"

</programlisting></para>
    </sect1>

    <sect1>
      <title>HTTPS Output</title>

      <para>The following shows the output of the script run against
      www.hammerdb.com.</para>

      <para><programlisting>Vuser 1:^A1 URL https://www.hammerdb.com - token ::http::1
Vuser 1:{Using sock55f342ae0870 for www.hammerdb.com:443 - token ::http::1} keepalive
Vuser 1:^B1 begin sending request - token ::http::1
Vuser 1:^C1 end sending request - token ::http::1
Vuser 1:^D1 begin receiving response - token ::http::1
Vuser 1:^E1 end of response headers - token ::http::1
Vuser 1:^F1 end of response body (unchunked) - token ::http::1
Vuser 1:ok
Vuser 1:sha1_hash 1C27C238029A8E34DBC1C7DBDE500DCD42C38446 subject CN=hammerdb.com issuer CN=Encryption Everywhere DV TLS CA - G1,OU=www.digicert.com,O=DigiCert Inc,C=US notBefore Sep  9 00:00:00 2020 GMT notAfter Sep 19 12:00:00 2021 GMT serial 0D9F3C403C1001E7A648E621BECF67F1 certificate 
-----BEGIN CERTIFICATE-----
...
-----END CERTIFICATE-----
 sbits 256 cipher ECDHE-RSA-AES256-GCM-SHA384
Vuser 1:^A2 URL https://www.hammerdb.com - token ::http::2
Vuser 1:{reusing socket sock55f342ae0870 for www.hammerdb.com:443 - token ::http::2}
Vuser 1:{Using sock55f342ae0870 for www.hammerdb.com:443 - token ::http::2} keepalive
Vuser 1:^B2 begin sending request - token ::http::2
Vuser 1:^C2 end sending request - token ::http::2
Vuser 1:^D2 begin receiving response - token ::http::2
Vuser 1:^E2 end of response headers - token ::http::2
Vuser 1:^F2 end of response body (unchunked) - token ::http::2
Vuser 1:Call 0 took 175
Vuser 1:^A3 URL https://www.hammerdb.com - token ::http::3
Vuser 1:{reusing socket sock55f342ae0870 for www.hammerdb.com:443 - token ::http::3}
Vuser 1:{Using sock55f342ae0870 for www.hammerdb.com:443 - token ::http::3} keepalive
Vuser 1:^B3 begin sending request - token ::http::3
Vuser 1:^C3 end sending request - token ::http::3
Vuser 1:^D3 begin receiving response - token ::http::3
Vuser 1:^E3 end of response headers - token ::http::3
Vuser 1:^F3 end of response body (unchunked) - token ::http::3
Vuser 1:Call 1 took 62
Vuser 1:^A4 URL https://www.hammerdb.com - token ::http::4
Vuser 1:{reusing socket sock55f342ae0870 for www.hammerdb.com:443 - token ::http::4}
Vuser 1:{Using sock55f342ae0870 for www.hammerdb.com:443 - token ::http::4} keepalive
Vuser 1:^B4 begin sending request - token ::http::4
Vuser 1:^C4 end sending request - token ::http::4
Vuser 1:^D4 begin receiving response - token ::http::4
Vuser 1:^E4 end of response headers - token ::http::4
Vuser 1:^F4 end of response body (unchunked) - token ::http::4
Vuser 1:Call 2 took 56
Vuser 1:^A5 URL https://www.hammerdb.com - token ::http::5
Vuser 1:{reusing socket sock55f342ae0870 for www.hammerdb.com:443 - token ::http::5}
Vuser 1:{Using sock55f342ae0870 for www.hammerdb.com:443 - token ::http::5} keepalive
Vuser 1:^B5 begin sending request - token ::http::5
Vuser 1:^C5 end sending request - token ::http::5
Vuser 1:^D5 begin receiving response - token ::http::5
Vuser 1:^E5 end of response headers - token ::http::5
Vuser 1:^F5 end of response body (unchunked) - token ::http::5
Vuser 1:Call 3 took 65
Vuser 1:^A6 URL https://www.hammerdb.com - token ::http::6
Vuser 1:{reusing socket sock55f342ae0870 for www.hammerdb.com:443 - token ::http::6}
Vuser 1:{Using sock55f342ae0870 for www.hammerdb.com:443 - token ::http::6} keepalive
Vuser 1:^B6 begin sending request - token ::http::6
Vuser 1:^C6 end sending request - token ::http::6
Vuser 1:^D6 begin receiving response - token ::http::6
Vuser 1:^E6 end of response headers - token ::http::6
Vuser 1:^F6 end of response body (unchunked) - token ::http::6
Vuser 1:Call 4 took 73
Vuser 1:^A7 URL https://www.hammerdb.com - token ::http::7
Vuser 1:{reusing socket sock55f342ae0870 for www.hammerdb.com:443 - token ::http::7}
Vuser 1:{Using sock55f342ae0870 for www.hammerdb.com:443 - token ::http::7} keepalive
Vuser 1:^B7 begin sending request - token ::http::7
Vuser 1:^C7 end sending request - token ::http::7
Vuser 1:^D7 begin receiving response - token ::http::7
Vuser 1:^E7 end of response headers - token ::http::7
Vuser 1:^F7 end of response body (unchunked) - token ::http::7
Vuser 1:Call 5 took 62
Vuser 1:^A8 URL https://www.hammerdb.com - token ::http::8
Vuser 1:{reusing socket sock55f342ae0870 for www.hammerdb.com:443 - token ::http::8}
Vuser 1:{Using sock55f342ae0870 for www.hammerdb.com:443 - token ::http::8} keepalive
Vuser 1:^B8 begin sending request - token ::http::8
Vuser 1:^C8 end sending request - token ::http::8
Vuser 1:^D8 begin receiving response - token ::http::8
Vuser 1:^E8 end of response headers - token ::http::8
Vuser 1:^F8 end of response body (unchunked) - token ::http::8
Vuser 1:Call 6 took 77
Vuser 1:^A9 URL https://www.hammerdb.com - token ::http::9
Vuser 1:{reusing socket sock55f342ae0870 for www.hammerdb.com:443 - token ::http::9}
Vuser 1:{Using sock55f342ae0870 for www.hammerdb.com:443 - token ::http::9} keepalive
Vuser 1:^B9 begin sending request - token ::http::9
Vuser 1:^C9 end sending request - token ::http::9
Vuser 1:^D9 begin receiving response - token ::http::9
Vuser 1:^E9 end of response headers - token ::http::9
Vuser 1:^F9 end of response body (unchunked) - token ::http::9
Vuser 1:Call 7 took 52
Vuser 1:^A10 URL https://www.hammerdb.com - token ::http::10
Vuser 1:{reusing socket sock55f342ae0870 for www.hammerdb.com:443 - token ::http::10}
Vuser 1:{Using sock55f342ae0870 for www.hammerdb.com:443 - token ::http::10} keepalive
Vuser 1:^B10 begin sending request - token ::http::10
Vuser 1:^C10 end sending request - token ::http::10
Vuser 1:^D10 begin receiving response - token ::http::10
Vuser 1:^E10 end of response headers - token ::http::10
Vuser 1:^F10 end of response body (unchunked) - token ::http::10
Vuser 1:Call 8 took 52
Vuser 1:^A11 URL https://www.hammerdb.com - token ::http::11
Vuser 1:{reusing socket sock55f342ae0870 for www.hammerdb.com:443 - token ::http::11}
Vuser 1:{Using sock55f342ae0870 for www.hammerdb.com:443 - token ::http::11} keepalive
Vuser 1:^B11 begin sending request - token ::http::11
Vuser 1:^C11 end sending request - token ::http::11
Vuser 1:^D11 begin receiving response - token ::http::11
Vuser 1:^E11 end of response headers - token ::http::11
Vuser 1:^F11 end of response body (unchunked) - token ::http::11
Vuser 1:Call 9 took 53
Vuser 1:10 iterations in 727 at an average of 72
</programlisting></para>
    </sect1>
  </chapter>

  <chapter>
    <title>GNU Free Documentation License</title>

    <para>## GNU Free Documentation License</para>

    <para>Version 1.3, 3 November 2008</para>

    <para>Copyright (C) 2000, 2001, 2002, 2007, 2008 Free Software Foundation,
    Inc. &lt;https://fsf.org/&gt;</para>

    <para>Everyone is permitted to copy and distribute verbatim copies of this
    license document, but changing it is not allowed.</para>

    <para>#### 0. PREAMBLE The purpose of this License is to make a manual,
    textbook, or other functional and useful document "free" in the sense of
    freedom: to assure everyone the effective freedom to copy and redistribute
    it, with or without modifying it, either commercially or noncommercially.
    Secondarily, this License preserves for the author and publisher a way to
    get credit for their work, while not being considered responsible for
    modifications made by others.</para>

    <para>This License is a kind of "copyleft", which means that derivative
    works of the document must themselves be free in the same sense. It
    complements the GNU General Public License, which is a copyleft license
    designed for free software.</para>

    <para>We have designed this License in order to use it for manuals for
    free software, because free software needs free documentation: a free
    program should come with manuals providing the same freedoms that the
    software does. But this License is not limited to software manuals; it can
    be used for any textual work, regardless of subject matter or whether it
    is published as a printed book. We recommend this License principally for
    works whose purpose is instruction or reference.</para>

    <para>#### 1. APPLICABILITY AND DEFINITIONS</para>

    <para>This License applies to any manual or other work, in any medium,
    that contains a notice placed by the copyright holder saying it can be
    distributed under the terms of this License. Such a notice grants a
    world-wide, royalty-free license, unlimited in duration, to use that work
    under the conditions stated herein. The "Document", below, refers to any
    such manual or work. Any member of the public is a licensee, and is
    addressed as "you". You accept the license if you copy, modify or
    distribute the work in a way requiring permission under copyright
    law.</para>

    <para>A "Modified Version" of the Document means any work containing the
    Document or a portion of it, either copied verbatim, or with modifications
    and/or translated into another language.</para>

    <para>A "Secondary Section" is a named appendix or a front-matter section
    of the Document that deals exclusively with the relationship of the
    publishers or authors of the Document to the Document's overall subject
    (or to related matters) and contains nothing that could fall directly
    within that overall subject. (Thus, if the Document is in part a textbook
    of mathematics, a Secondary Section may not explain any mathematics.) The
    relationship could be a matter of historical connection with the subject
    or with related matters, or of legal, commercial, philosophical, ethical
    or political position regarding them.</para>

    <para>The "Invariant Sections" are certain Secondary Sections whose titles
    are designated, as being those of Invariant Sections, in the notice that
    says that the Document is released under this License. If a section does
    not fit the above definition of Secondary then it is not allowed to be
    designated as Invariant. The Document may contain zero Invariant Sections.
    If the Document does not identify any Invariant Sections then there are
    none.</para>

    <para>The "Cover Texts" are certain short passages of text that are
    listed, as Front-Cover Texts or Back-Cover Texts, in the notice that says
    that the Document is released under this License. A Front-Cover Text may
    be at most 5 words, and a Back-Cover Text may be at most 25 words.</para>

    <para>A "Transparent" copy of the Document means a machine-readable copy,
    represented in a format whose specification is available to the general
    public, that is suitable for revising the document straightforwardly with
    generic text editors or (for images composed of pixels) generic paint
    programs or (for drawings) some widely available drawing editor, and that
    is suitable for input to text formatters or for automatic translation to a
    variety of formats suitable for input to text formatters. A copy made in
    an otherwise Transparent file format whose markup, or absence of markup,
    has been arranged to thwart or discourage subsequent modification by
    readers is not Transparent. An image format is not Transparent if used for
    any substantial amount of text. A copy that is not "Transparent" is called
    "Opaque".</para>

    <para>Examples of suitable formats for Transparent copies include plain
    ASCII without markup, Texinfo input format, LaTeX input format, SGML or
    XML using a publicly available DTD, and standard-conforming simple HTML,
    PostScript or PDF designed for human modification. Examples of transparent
    image formats include PNG, XCF and JPG. Opaque formats include proprietary
    formats that can be read and edited only by proprietary word processors,
    SGML or XML for which the DTD and/or processing tools are not generally
    available, and the machine-generated HTML, PostScript or PDF produced by
    some word processors for output purposes only.</para>

    <para>The "Title Page" means, for a printed book, the title page itself,
    plus such following pages as are needed to hold, legibly, the material
    this License requires to appear in the title page. For works in formats
    which do not have any title page as such, "Title Page" means the text near
    the most prominent appearance of the work's title, preceding the beginning
    of the body of the text.</para>

    <para>The "publisher" means any person or entity that distributes copies
    of the Document to the public.</para>

    <para>A section "Entitled XYZ" means a named subunit of the Document whose
    title either is precisely XYZ or contains XYZ in parentheses following
    text that translates XYZ in another language. (Here XYZ stands for a
    specific section name mentioned below, such as "Acknowledgements",
    "Dedications", "Endorsements", or "History".) To "Preserve the Title" of
    such a section when you modify the Document means that it remains a
    section "Entitled XYZ" according to this definition.</para>

    <para>The Document may include Warranty Disclaimers next to the notice
    which states that this License applies to the Document. These Warranty
    Disclaimers are considered to be included by reference in this License,
    but only as regards disclaiming warranties: any other implication that
    these Warranty Disclaimers may have is void and has no effect on the
    meaning of this License.</para>

    <para>#### 2. VERBATIM COPYING</para>

    <para>You may copy and distribute the Document in any medium, either
    commercially or noncommercially, provided that this License, the copyright
    notices, and the license notice saying this License applies to the
    Document are reproduced in all copies, and that you add no other
    conditions whatsoever to those of this License. You may not use technical
    measures to obstruct or control the reading or further copying of the
    copies you make or distribute. However, you may accept compensation in
    exchange for copies. If you distribute a large enough number of copies you
    must also follow the conditions in section 3.</para>

    <para>You may also lend copies, under the same conditions stated above,
    and you may publicly display copies.</para>

    <para>#### 3. COPYING IN QUANTITY</para>

    <para>If you publish printed copies (or copies in media that commonly have
    printed covers) of the Document, numbering more than 100, and the
    Document's license notice requires Cover Texts, you must enclose the
    copies in covers that carry, clearly and legibly, all these Cover Texts:
    Front-Cover Texts on the front cover, and Back-Cover Texts on the back
    cover. Both covers must also clearly and legibly identify you as the
    publisher of these copies. The front cover must present the full title
    with all words of the title equally prominent and visible. You may add
    other material on the covers in addition. Copying with changes limited to
    the covers, as long as they preserve the title of the Document and satisfy
    these conditions, can be treated as verbatim copying in other
    respects.</para>

    <para>If the required texts for either cover are too voluminous to fit
    legibly, you should put the first ones listed (as many as fit reasonably)
    on the actual cover, and continue the rest onto adjacent pages.</para>

    <para>If you publish or distribute Opaque copies of the Document numbering
    more than 100, you must either include a machine-readable Transparent copy
    along with each Opaque copy, or state in or with each Opaque copy a
    computer-network location from which the general network-using public has
    access to download using public-standard network protocols a complete
    Transparent copy of the Document, free of added material. If you use the
    latter option, you must take reasonably prudent steps, when you begin
    distribution of Opaque copies in quantity, to ensure that this Transparent
    copy will remain thus accessible at the stated location until at least one
    year after the last time you distribute an Opaque copy (directly or
    through your agents or retailers) of that edition to the public.</para>

    <para>It is requested, but not required, that you contact the authors of
    the Document well before redistributing any large number of copies, to
    give them a chance to provide you with an updated version of the
    Document.</para>

    <para>#### 4. MODIFICATIONS</para>

    <para>You may copy and distribute a Modified Version of the Document under
    the conditions of sections 2 and 3 above, provided that you release the
    Modified Version under precisely this License, with the Modified Version
    filling the role of the Document, thus licensing distribution and
    modification of the Modified Version to whoever possesses a copy of it. In
    addition, you must do these things in the Modified Version:</para>

    <para>- A. Use in the Title Page (and on the covers, if any) a title
    distinct from that of the Document, and from those of previous versions
    (which should, if there were any, be listed in the History section of the
    Document). You may use the same title as a previous version if the
    original publisher of that version gives permission.</para>

    <para>- B. List on the Title Page, as authors, one or more persons or
    entities responsible for authorship of the modifications in the Modified
    Version, together with at least five of the principal authors of the
    Document (all of its principal authors, if it has fewer than five), unless
    they release you from this requirement.</para>

    <para>- C. State on the Title page the name of the publisher of the
    Modified Version, as the publisher.</para>

    <para>- D. Preserve all the copyright notices of the Document.</para>

    <para>- E. Add an appropriate copyright notice for your modifications
    adjacent to the other copyright notices.</para>

    <para>- F. Include, immediately after the copyright notices, a license
    notice giving the public permission to use the Modified Version under the
    terms of this License, in the form shown in the Addendum below.</para>

    <para>- G. Preserve in that license notice the full lists of Invariant
    Sections and required Cover Texts given in the Document's license
    notice.</para>

    <para>- H. Include an unaltered copy of this License.</para>

    <para>- I. Preserve the section Entitled "History", Preserve its Title,
    and add to it an item stating at least the title, year, new authors, and
    publisher of the Modified Version as given on the Title Page. If there is
    no section Entitled "History" in the Document, create one stating the
    title, year, authors, and publisher of the Document as given on its Title
    Page, then add an item describing the Modified Version as stated in the
    previous sentence.</para>

    <para>- J. Preserve the network location, if any, given in the Document
    for public access to a Transparent copy of the Document, and likewise the
    network locations given in the Document for previous versions it was based
    on. These may be placed in the "History" section. You may omit a network
    location for a work that was published at least four years before the
    Document itself, or if the original publisher of the version it refers to
    gives permission.</para>

    <para>- K. For any section Entitled "Acknowledgements" or "Dedications",
    Preserve the Title of the section, and preserve in the section all the
    substance and tone of each of the contributor acknowledgements and/or
    dedications given therein.</para>

    <para>- L. Preserve all the Invariant Sections of the Document, unaltered
    in their text and in their titles. Section numbers or the equivalent are
    not considered part of the section titles.</para>

    <para>- M. Delete any section Entitled "Endorsements". Such a section may
    not be included in the Modified Version.</para>

    <para>- N. Do not retitle any existing section to be Entitled
    "Endorsements" or to conflict in title with any Invariant Section.</para>

    <para>- O. Preserve any Warranty Disclaimers.</para>

    <para>If the Modified Version includes new front-matter sections or
    appendices that qualify as Secondary Sections and contain no material
    copied from the Document, you may at your option designate some or all of
    these sections as invariant. To do this, add their titles to the list of
    Invariant Sections in the Modified Version's license notice. These titles
    must be distinct from any other section titles.</para>

    <para>You may add a section Entitled "Endorsements", provided it contains
    nothing but endorsements of your Modified Version by various parties for
    example, statements of peer review or that the text has been approved by
    an organization as the authoritative definition of a standard.</para>

    <para>You may add a passage of up to five words as a Front-Cover Text, and
    a passage of up to 25 words as a Back-Cover Text, to the end of the list
    of Cover Texts in the Modified Version. Only one passage of Front-Cover
    Text and one of Back-Cover Text may be added by (or through arrangements
    made by) any one entity. If the Document already includes a cover text for
    the same cover, previously added by you or by arrangement made by the same
    entity you are acting on behalf of, you may not add another; but you may
    replace the old one, on explicit permission from the previous publisher
    that added the old one.</para>

    <para>The author(s) and publisher(s) of the Document do not by this
    License give permission to use their names for publicity for or to assert
    or imply endorsement of any Modified Version.</para>

    <para>#### 5. COMBINING DOCUMENTS</para>

    <para>You may combine the Document with other documents released under
    this License, under the terms defined in section 4 above for modified
    versions, provided that you include in the combination all of the
    Invariant Sections of all of the original documents, unmodified, and list
    them all as Invariant Sections of your combined work in its license
    notice, and that you preserve all their Warranty Disclaimers.</para>

    <para>The combined work need only contain one copy of this License, and
    multiple identical Invariant Sections may be replaced with a single copy.
    If there are multiple Invariant Sections with the same name but different
    contents, make the title of each such section unique by adding at the end
    of it, in parentheses, the name of the original author or publisher of
    that section if known, or else a unique number. Make the same adjustment
    to the section titles in the list of Invariant Sections in the license
    notice of the combined work.</para>

    <para>In the combination, you must combine any sections Entitled "History"
    in the various original documents, forming one section Entitled "History";
    likewise combine any sections Entitled "Acknowledgements", and any
    sections Entitled "Dedications". You must delete all sections Entitled
    "Endorsements".</para>

    <para>#### 6. COLLECTIONS OF DOCUMENTS</para>

    <para>You may make a collection consisting of the Document and other
    documents released under this License, and replace the individual copies
    of this License in the various documents with a single copy that is
    included in the collection, provided that you follow the rules of this
    License for verbatim copying of each of the documents in all other
    respects.</para>

    <para>You may extract a single document from such a collection, and
    distribute it individually under this License, provided you insert a copy
    of this License into the extracted document, and follow this License in
    all other respects regarding verbatim copying of that document.</para>

    <para>#### 7. AGGREGATION WITH INDEPENDENT WORKS</para>

    <para>A compilation of the Document or its derivatives with other separate
    and independent documents or works, in or on a volume of a storage or
    distribution medium, is called an "aggregate" if the copyright resulting
    from the compilation is not used to limit the legal rights of the
    compilation's users beyond what the individual works permit. When the
    Document is included in an aggregate, this License does not apply to the
    other works in the aggregate which are not themselves derivative works of
    the Document.</para>

    <para>If the Cover Text requirement of section 3 is applicable to these
    copies of the Document, then if the Document is less than one half of the
    entire aggregate, the Document's Cover Texts may be placed on covers that
    bracket the Document within the aggregate, or the electronic equivalent of
    covers if the Document is in electronic form. Otherwise they must appear
    on printed covers that bracket the whole aggregate.</para>

    <para>#### 8. TRANSLATION</para>

    <para>Translation is considered a kind of modification, so you may
    distribute translations of the Document under the terms of section 4.
    Replacing Invariant Sections with translations requires special permission
    from their copyright holders, but you may include translations of some or
    all Invariant Sections in addition to the original versions of these
    Invariant Sections. You may include a translation of this License, and all
    the license notices in the Document, and any Warranty Disclaimers,
    provided that you also include the original English version of this
    License and the original versions of those notices and disclaimers. In
    case of a disagreement between the translation and the original version of
    this License or a notice or disclaimer, the original version will
    prevail.</para>

    <para>If a section in the Document is Entitled "Acknowledgements",
    "Dedications", or "History", the requirement (section 4) to Preserve its
    Title (section 1) will typically require changing the actual title.</para>

    <para>#### 9. TERMINATION</para>

    <para>You may not copy, modify, sublicense, or distribute the Document
    except as expressly provided under this License. Any attempt otherwise to
    copy, modify, sublicense, or distribute it is void, and will automatically
    terminate your rights under this License.</para>

    <para>However, if you cease all violation of this License, then your
    license from a particular copyright holder is reinstated (a)
    provisionally, unless and until the copyright holder explicitly and
    finally terminates your license, and (b) permanently, if the copyright
    holder fails to notify you of the violation by some reasonable means prior
    to 60 days after the cessation.</para>

    <para>Moreover, your license from a particular copyright holder is
    reinstated permanently if the copyright holder notifies you of the
    violation by some reasonable means, this is the first time you have
    received notice of violation of this License (for any work) from that
    copyright holder, and you cure the violation prior to 30 days after your
    receipt of the notice.</para>

    <para>Termination of your rights under this section does not terminate the
    licenses of parties who have received copies or rights from you under this
    License. If your rights have been terminated and not permanently
    reinstated, receipt of a copy of some or all of the same material does not
    give you any rights to use it.</para>

    <para>#### 10. FUTURE REVISIONS OF THIS LICENSE</para>

    <para>The Free Software Foundation may publish new, revised versions of
    the GNU Free Documentation License from time to time. Such new versions
    will be similar in spirit to the present version, but may differ in detail
    to address new problems or concerns. See
    &lt;https://www.gnu.org/licenses/&gt;.</para>

    <para>Each version of the License is given a distinguishing version
    number. If the Document specifies that a particular numbered version of
    this License "or any later version" applies to it, you have the option of
    following the terms and conditions either of that specified version or of
    any later version that has been published (not as a draft) by the Free
    Software Foundation. If the Document does not specify a version number of
    this License, you may choose any version ever published (not as a draft)
    by the Free Software Foundation. If the Document specifies that a proxy
    can decide which future versions of this License can be used, that proxy's
    public statement of acceptance of a version permanently authorizes you to
    choose that version for the Document.</para>

    <para>#### 11. RELICENSING</para>

    <para>"Massive Multiauthor Collaboration Site" (or "MMC Site") means any
    World Wide Web server that publishes copyrightable works and also provides
    prominent facilities for anybody to edit those works. A public wiki that
    anybody can edit is an example of such a server. A "Massive Multiauthor
    Collaboration" (or "MMC") contained in the site means any set of
    copyrightable works thus published on the MMC site.</para>

    <para>"CC-BY-SA" means the Creative Commons Attribution-Share Alike 3.0
    license published by Creative Commons Corporation, a not-for-profit
    corporation with a principal place of business in San Francisco,
    California, as well as future copyleft versions of that license published
    by that same organization.</para>

    <para>"Incorporate" means to publish or republish a Document, in whole or
    in part, as part of another Document.</para>

    <para>An MMC is "eligible for relicensing" if it is licensed under this
    License, and if all works that were first published under this License
    somewhere other than this MMC, and subsequently incorporated in whole or
    in part into the MMC, (1) had no cover texts or invariant sections, and
    (2) were thus incorporated prior to November 1, 2008.</para>

    <para>The operator of an MMC Site may republish an MMC contained in the
    site under CC-BY-SA on the same site at any time before August 1, 2009,
    provided the MMC is eligible for relicensing.</para>
  </chapter>
</book>
